{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter recovery\n",
    "\n",
    "We simulated data for a set of design and various ground truth parameters. Now we will try to estimate those parameters from the simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in/Generic Imports\n",
    "import os,sys\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# Libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(\"pymc\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_bhm(subj_id=[],design_df=[],choices=[],type='single'):\n",
    "\n",
    "    delay_amt = design_df['cdd_delay_amt'].values\n",
    "    delay_wait = design_df['cdd_delay_wait'].values\n",
    "    immed_amt = design_df['cdd_immed_amt'].values\n",
    "    immed_wait = design_df['cdd_immed_wait'].values\n",
    "    \n",
    "    # We will fit a model for each subject\n",
    "    with pm.Model() as model_simple:\n",
    "\n",
    "        # Hyperparameters for kappa and gamma\n",
    "        # estimated from MLE approximations : np.exp(-3.60) = 0.0273, np.sqrt(1.71)=1.308\n",
    "        mu_kappa_hyper = pm.Beta('mu_kappa_hyper',mu=np.exp(-3.60),sigma=0.01)\n",
    "        sd_kappa_hyper = pm.Normal('sd_kappa_hyper',mu=np.sqrt(1.71),sigma=0.1)\n",
    "        # estimated from MLE approximations : np.sqrt(2.30) = 1.517\n",
    "        sd_gamma_hyper = pm.Normal('sd_hyper',mu=np.sqrt(2.30),sigma=0.1)\n",
    "\n",
    "        kappa = pm.LogNormal('kappa',mu=mu_kappa_hyper,sigma=sd_kappa_hyper,shape=np.size(np.unique(subj_id)))\n",
    "        gamma = pm.HalfNormal('gamma',sigma=sd_gamma_hyper,shape=np.size(np.unique(subj_id)))\n",
    "        \n",
    "        prob = pm.Deterministic('prob', 1 / (1 + pm.math.exp(-gamma[subj_id] * ( delay_amt/(1+(kappa[subj_id]*delay_wait)) \n",
    "                                                                                - immed_amt/(1+(kappa[subj_id]*immed_wait)) ))))\n",
    "\n",
    "        y_1 = pm.Bernoulli('y_1',p=prob,observed=choices)\n",
    "\n",
    "        trace_prior = pm.sample(10000, tune=1000, cores=5,target_accept=0.99,progressbar=False)\n",
    "\n",
    "    # This is how you get a nice array. Note that this returns a pandas DataFrame, not a numpy array. Indexing is totally different.\n",
    "    summary= az.summary(trace_prior,round_to=10)\n",
    "    if type=='single':\n",
    "        kappa_hat = summary['mean'].loc['kappa[{}]'.format(0)]\n",
    "        gamma_hat = summary['mean'].loc['gamma[{}]'.format(0)]\n",
    "    elif type=='aggregate':\n",
    "        kappa_hat = [summary['mean'].loc['kappa[{}]'.format(x)] for x in set(subj_id)]\n",
    "        gamma_hat = [summary['mean'].loc['gamma[{}]'.format(x)] for x in set(subj_id)]\n",
    "    return kappa_hat,gamma_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simul/split/p0000/cdd/p0000_cdd.csv\n",
      "simul/split/p0001/cdd/p0001_cdd.csv\n",
      "simul/split/p0002/cdd/p0002_cdd.csv\n",
      "simul/split/p0003/cdd/p0003_cdd.csv\n",
      "simul/split/p0004/cdd/p0004_cdd.csv\n",
      "simul/split/p0005/cdd/p0005_cdd.csv\n",
      "simul/split/p0006/cdd/p0006_cdd.csv\n",
      "simul/split/p0007/cdd/p0007_cdd.csv\n",
      "simul/split/p0008/cdd/p0008_cdd.csv\n",
      "simul/split/p0009/cdd/p0009_cdd.csv\n",
      "simul/split/p0010/cdd/p0010_cdd.csv\n",
      "simul/split/p0011/cdd/p0011_cdd.csv\n",
      "simul/split/p0012/cdd/p0012_cdd.csv\n",
      "simul/split/p0013/cdd/p0013_cdd.csv\n",
      "simul/split/p0014/cdd/p0014_cdd.csv\n",
      "simul/split/p0015/cdd/p0015_cdd.csv\n",
      "simul/split/p0016/cdd/p0016_cdd.csv\n",
      "simul/split/p0017/cdd/p0017_cdd.csv\n",
      "simul/split/p0018/cdd/p0018_cdd.csv\n",
      "simul/split/p0019/cdd/p0019_cdd.csv\n",
      "simul/split/p0020/cdd/p0020_cdd.csv\n",
      "simul/split/p0021/cdd/p0021_cdd.csv\n",
      "simul/split/p0022/cdd/p0022_cdd.csv\n",
      "simul/split/p0023/cdd/p0023_cdd.csv\n",
      "simul/split/p0024/cdd/p0024_cdd.csv\n",
      "simul/split/p0025/cdd/p0025_cdd.csv\n",
      "simul/split/p0026/cdd/p0026_cdd.csv\n",
      "simul/split/p0027/cdd/p0027_cdd.csv\n",
      "simul/split/p0028/cdd/p0028_cdd.csv\n",
      "simul/split/p0029/cdd/p0029_cdd.csv\n",
      "simul/split/p0030/cdd/p0030_cdd.csv\n",
      "simul/split/p0031/cdd/p0031_cdd.csv\n",
      "simul/split/p0032/cdd/p0032_cdd.csv\n",
      "simul/split/p0033/cdd/p0033_cdd.csv\n",
      "simul/split/p0034/cdd/p0034_cdd.csv\n",
      "simul/split/p0035/cdd/p0035_cdd.csv\n",
      "simul/split/p0036/cdd/p0036_cdd.csv\n",
      "simul/split/p0037/cdd/p0037_cdd.csv\n",
      "simul/split/p0038/cdd/p0038_cdd.csv\n",
      "simul/split/p0039/cdd/p0039_cdd.csv\n",
      "simul/split/p0040/cdd/p0040_cdd.csv\n",
      "simul/split/p0041/cdd/p0041_cdd.csv\n",
      "simul/split/p0042/cdd/p0042_cdd.csv\n",
      "simul/split/p0043/cdd/p0043_cdd.csv\n",
      "simul/split/p0044/cdd/p0044_cdd.csv\n",
      "simul/split/p0045/cdd/p0045_cdd.csv\n",
      "simul/split/p0046/cdd/p0046_cdd.csv\n",
      "simul/split/p0047/cdd/p0047_cdd.csv\n",
      "simul/split/p0048/cdd/p0048_cdd.csv\n",
      "simul/split/p0049/cdd/p0049_cdd.csv\n",
      "simul/split/p0050/cdd/p0050_cdd.csv\n",
      "simul/split/p0051/cdd/p0051_cdd.csv\n",
      "simul/split/p0052/cdd/p0052_cdd.csv\n",
      "simul/split/p0053/cdd/p0053_cdd.csv\n",
      "simul/split/p0054/cdd/p0054_cdd.csv\n",
      "simul/split/p0055/cdd/p0055_cdd.csv\n",
      "simul/split/p0056/cdd/p0056_cdd.csv\n",
      "simul/split/p0057/cdd/p0057_cdd.csv\n",
      "simul/split/p0058/cdd/p0058_cdd.csv\n",
      "simul/split/p0059/cdd/p0059_cdd.csv\n",
      "simul/split/p0060/cdd/p0060_cdd.csv\n",
      "simul/split/p0061/cdd/p0061_cdd.csv\n",
      "simul/split/p0062/cdd/p0062_cdd.csv\n",
      "simul/split/p0063/cdd/p0063_cdd.csv\n",
      "simul/split/p0064/cdd/p0064_cdd.csv\n",
      "simul/split/p0065/cdd/p0065_cdd.csv\n",
      "simul/split/p0066/cdd/p0066_cdd.csv\n",
      "simul/split/p0067/cdd/p0067_cdd.csv\n",
      "simul/split/p0068/cdd/p0068_cdd.csv\n",
      "simul/split/p0069/cdd/p0069_cdd.csv\n",
      "simul/split/p0070/cdd/p0070_cdd.csv\n",
      "simul/split/p0071/cdd/p0071_cdd.csv\n",
      "simul/split/p0072/cdd/p0072_cdd.csv\n",
      "simul/split/p0073/cdd/p0073_cdd.csv\n",
      "simul/split/p0074/cdd/p0074_cdd.csv\n",
      "simul/split/p0075/cdd/p0075_cdd.csv\n",
      "simul/split/p0076/cdd/p0076_cdd.csv\n",
      "simul/split/p0077/cdd/p0077_cdd.csv\n",
      "simul/split/p0078/cdd/p0078_cdd.csv\n",
      "simul/split/p0079/cdd/p0079_cdd.csv\n",
      "simul/split/p0080/cdd/p0080_cdd.csv\n",
      "simul/split/p0081/cdd/p0081_cdd.csv\n",
      "simul/split/p0082/cdd/p0082_cdd.csv\n",
      "simul/split/p0083/cdd/p0083_cdd.csv\n",
      "simul/split/p0084/cdd/p0084_cdd.csv\n",
      "simul/split/p0085/cdd/p0085_cdd.csv\n",
      "simul/split/p0086/cdd/p0086_cdd.csv\n",
      "simul/split/p0087/cdd/p0087_cdd.csv\n",
      "simul/split/p0088/cdd/p0088_cdd.csv\n",
      "simul/split/p0089/cdd/p0089_cdd.csv\n",
      "simul/split/p0090/cdd/p0090_cdd.csv\n",
      "simul/split/p0091/cdd/p0091_cdd.csv\n",
      "simul/split/p0092/cdd/p0092_cdd.csv\n",
      "simul/split/p0093/cdd/p0093_cdd.csv\n",
      "simul/split/p0094/cdd/p0094_cdd.csv\n",
      "simul/split/p0095/cdd/p0095_cdd.csv\n",
      "simul/split/p0096/cdd/p0096_cdd.csv\n",
      "simul/split/p0097/cdd/p0097_cdd.csv\n",
      "simul/split/p0098/cdd/p0098_cdd.csv\n",
      "simul/split/p0099/cdd/p0099_cdd.csv\n",
      "Time to complete 100 single BHM : 54.24978403250376 minutes\n"
     ]
    }
   ],
   "source": [
    "fn = os.path.join('simul','ground_truth.csv')\n",
    "params_df = pd.read_csv(fn,index_col=0)\n",
    "\n",
    "fn = os.path.join('simul','design_set.csv')\n",
    "design_df_single = pd.read_csv(fn,index_col=0)\n",
    "\n",
    "simulated_data = sorted(glob.glob(os.path.join('simul','split','*/cdd/*.csv')))\n",
    "\n",
    "choice_col = 'cdd_choice'\n",
    "\n",
    "tStep0 = time.time()\n",
    "\n",
    "# single\n",
    "kappa_hat,gamma_hat = [],[]\n",
    "# aggregate\n",
    "subj_id,choices,design_list = [],[],[]\n",
    "for index,fn in enumerate(simulated_data):\n",
    "    print(fn)\n",
    "    df = pd.read_csv(fn,index_col=0)\n",
    "    # single\n",
    "    kh,gh = estimate_bhm(subj_id=[0]*len(df[choice_col]),\n",
    "                         design_df=design_df_single,\n",
    "                         choices=df[choice_col],type='single')\n",
    "    kappa_hat += [kh]\n",
    "    gamma_hat += [gh]\n",
    "\n",
    "    # aggregate\n",
    "    choices += df[choice_col].values.tolist()\n",
    "    subj_id += [index]*len(df[choice_col])\n",
    "    design_list += [design_df_single]\n",
    "\n",
    "print('Time to complete {} single BHM : {} minutes'.format(len(simulated_data),(time.time() - tStep0)/60.0))\n",
    "\n",
    "tStep1 = time.time()\n",
    "params_df['kappa_bhm_sing'] = kappa_hat\n",
    "params_df['gamma_bhm_sing'] = gamma_hat\n",
    "\n",
    "design_df_agg = pd.concat(design_list,axis=0)\n",
    "# kappa_hat,gamma_hat = estimate_bhm(subj_id,design_df_agg,choices,type='aggregate')\n",
    "params_df['kappa_bhm_agg'],params_df['gamma_bhm_agg'] = estimate_bhm(\n",
    "    subj_id=subj_id,design_df=design_df_agg,choices=choices,type='aggregate')\n",
    "\n",
    "print('Time to complete {} aggregate BHM : {} minutes'.format(len(simulated_data),(time.time() - tStep1)/60.0))\n",
    "\n",
    "\n",
    "fn = os.path.join('simul','parameter_estimate_bhm.csv')\n",
    "print('Saving estimates to : {}'.format(fn))\n",
    "params_df.to_csv(fn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idm_jupy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

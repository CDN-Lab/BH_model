{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian parameter estimation \n",
    "\n",
    "Written for CPDM task as part of the IDM dataset collected online with Mturk. Here we are using the CASANDRE model (instead of utility model) to analyze the CPDM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in/Generic Imports\n",
    "import os,sys\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# Libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 99\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(\"pymc\")\n",
    "logger.setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the name of the directory\n",
    "# where the this file is present.\n",
    "# current = os.path.dirname(os.path.realpath(__file__))\n",
    "current = os.path.dirname(os.getcwd())\n",
    " \n",
    "# Getting the parent directory name\n",
    "# where the current directory is present.\n",
    "# parent = os.path.dirname(os.path.dirname(current))\n",
    "# parent = current\n",
    "parent = '/Users/pizarror/IDM'\n",
    "# print(parent)\n",
    "#/Users/pizarror/IDM\n",
    "\n",
    "# adding the parent directory to\n",
    "# the sys.path.\n",
    "sys.path.append(parent)\n",
    "\n",
    "from IDM_model.src import model_functions as mf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_load_data(subject='23_IDM_0144',fn='/tmp',cols=[]):\n",
    "    cpdm_df = pd.read_csv(fn)\n",
    "    task='cpdm'\n",
    "    cpdm_df = mf.remap_response(cpdm_df,task=task)\n",
    "    cpdm_df = mf.drop_pract(cpdm_df,task=task)\n",
    "    cpdm_df,response_rate = mf.drop_non_responses(cpdm_df,task=task,verbose=False) \n",
    "    data = mf.get_data(cpdm_df,cols,alpha_hat=1)[0]\n",
    "    data['subject'] = subject\n",
    "    return data \n",
    "\n",
    "def diganostic_plots(trace,experiment='experiment',utility_dir='/tmp/',subject='23_IDM_0001',task='cdd_nlh',coords={},var_names=['kappa','gamma'],figsize=(10,10)):\n",
    "\n",
    "    bh_dir = os.path.join(utility_dir,subject,task,'bh')\n",
    "    if not os.path.exists(bh_dir):\n",
    "        os.makedirs(bh_dir)\n",
    "    print('Saving diagnostic plots to bh_dir : {}'.format(bh_dir))\n",
    "\n",
    "    title_dict = {'fontsize':15}\n",
    "\n",
    "    # 2by2 : rows 2 varialbes, cols 2 for distribution and sampled values\n",
    "    axes = az.plot_trace(trace, var_names=var_names,coords=coords,compact=False)\n",
    "    for r in range(axes.shape[0]):\n",
    "        for c in range(axes.shape[1]):\n",
    "            axes[r,c].set_title('{}: {}'.format(subject,var_names[r]))\n",
    "    plt.tight_layout()\n",
    "    fig_fn = os.path.join(bh_dir,'{}_{}_trace_plot.{}.eps'.format(subject,task,experiment))\n",
    "    plt.savefig(fig_fn,format='eps')\n",
    "    plt.close()\n",
    "    \n",
    "    axes = az.plot_pair(trace,kind='kde', coords=coords,var_names=var_names,marginals=True)\n",
    "    axes[0,0].set_title(subject,fontdict=title_dict)\n",
    "    axes[1,0].set_ylabel(var_names[1])\n",
    "    axes[1,0].set_xlabel(var_names[0])\n",
    "    plt.tight_layout()\n",
    "    fig_fn = os.path.join(bh_dir,'{}_{}_bivariate_densities.{}.eps'.format(subject,task,experiment))\n",
    "    plt.savefig(fig_fn,format='eps')\n",
    "    plt.close()\n",
    "    \n",
    "    axes = az.plot_posterior(trace,var_names=var_names,coords=coords)\n",
    "    # print(axes.shape)\n",
    "    for c in range(axes.shape[0]):\n",
    "        axes[c].set_title('{}: {}'.format(subject,var_names[c]),fontdict=title_dict)\n",
    "    fig_fn = os.path.join(bh_dir,'{}_{}_posterior.{}.eps'.format(subject,task,experiment))\n",
    "    plt.savefig(fig_fn,format='eps')\n",
    "    plt.close()\n",
    "\n",
    "    fig, axes = plt.subplots(1,len(var_names), figsize=figsize)\n",
    "    az.plot_rank(trace,var_names=var_names,coords=coords,ax=axes)\n",
    "    for c in range(axes.shape[0]):\n",
    "        axes[c].set_title('{}: {}'.format(subject,var_names[c]),fontdict=title_dict)\n",
    "    fig.tight_layout()\n",
    "    fig_fn = os.path.join(bh_dir,'{}_{}_rank_plot_bars.{}.eps'.format(subject,task,experiment))\n",
    "    plt.savefig(fig_fn,format='eps')\n",
    "    plt.close()\n",
    "\n",
    "    fig, axes = plt.subplots(1,len(var_names), figsize=figsize)\n",
    "    axes = az.plot_rank(trace,var_names=var_names, coords=coords,kind=\"vlines\",vlines_kwargs={'lw':0}, marker_vlines_kwargs={'lw':3},ax=axes)\n",
    "    for c in range(axes.shape[0]):\n",
    "        axes[c].set_title('{}: {}'.format(subject,var_names[c]),fontdict=title_dict)\n",
    "    fig.tight_layout()\n",
    "    fig_fn = os.path.join(bh_dir,'{}_{}_rank_plot_lines.{}.eps'.format(subject,task,experiment))\n",
    "    plt.savefig(fig_fn,format='eps')\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Hierarchical Model (BHM)\n",
    "\n",
    "We pooled all data together and ran simple BH model in complete_pool_as_prior.\n",
    "\n",
    "We are using complete pooling to generate priors for when we implement a higherarchical bayesian model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "We will load the data from all participants so we can run the modeling schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan nan 'q' 'p' 'p' 'q' 'q' 'q' 'p' 'p' 'q' 'q' 'p' 'p'\n",
      " 'q' 'q' 'q' 'q' 'q' 'q' 'q' 'p' 'q' 'q' 'p' 'p' 'p' 'q' 'p' 'p' 'q' 'q'\n",
      " 'q' 'p' 'p' 'p' 'q' 'q' 'p' 'q' 'p' 'q' 'p' 'q' 'p' 'p' 'q' 'l' 'p' 'p'\n",
      " 'p' 'q' 'p' 'q' 'q' 'p' 'q' 'p' 'q' 'q' 'p' 'p' 'q' 'p' 'p' 'p' 'q' 'p'\n",
      " 'q' 'q' 'a' 'p' 'p' 'a' 'q' 'q' 'q' 'p' 'q' 'q' 'p' 'q' 'p' 'q' 'q' 'q'\n",
      " 'q' 'q' 'q' 'q' 'q' 'q' 'q' 'p' 'p' 'p' 'l' 'a' 'q' 'p' 'q' 'p' 'q' 'p'\n",
      " 'q' 'p' 'q' 'p' 'q' 'q' 'p' 'p' 'p' 'q' 'p' 'p' 'p' 'p' 'p' 'q' 'q' 'q'\n",
      " 'p' 'q' 'q' 'p' 'p' 'q' 'q' 'q' 'p' 'q' 'q' 'p' 'l' 'p' 'p' 'q' 'q' 'p'\n",
      " 'p' 'q' 'q' 'p' 'q' 'p' 'q' 'q' 'p' 'q' 'q' 'p' 'q' 'p' 'p' 'a' 'a' 'p'\n",
      " 'q' 'q' 'a' 'q' 'q' 'q' 'p' 'q' 'q' 'q' 'q' 'p' 'p' 'q' 'q' 'q' 'q' 'p'\n",
      " 'p' 'q' 'p' 'p' 'p' 'p' 'p' 'q' 'p' 'p' 'a' 'p' 'p' 'q' 'q' 'a' 'p' 'p'\n",
      " 'q' 'p' 'p' 'p' 'p' 'q' 'p' nan 'q' 'p' 'q' 'p' 'q' 'q' 'p' 'p' 'a' 'p'\n",
      " 'p' 'p' 'p' 'q' 'p' 'p' 'a' 'p' 'a' 'p' 'p' 'p' 'q' 'p' 'p' 'q' 'q' 'p'\n",
      " 'q' 'q' 'q' 'q' 'q' 'q' 'q' 'q' 'p' 'q' 'q' 'p' 'q' 'q' 'q' 'l' 'p' 'p'\n",
      " 'q' 'p' 'p' 'q' 'q' 'p' 'q' 'p' 'q' 'q' 'q' 'q' 'p' 'p' 'q' 'q' 'p' 'p'\n",
      " 'p' 'q' 'p' 'p' 'q' 'q' 'q' 'p' 'p' 'q' 'q' 'p' 'p' 'p' 'q' 'q' 'q' 'q'\n",
      " 'p' 'p' 'p' 'l' 'p' 'q' 'q' 'p' 'q' 'p' 'p' 'q' 'p' 'q' 'q' 'q' 'q' 'p'\n",
      " 'q' 'p' 'q' 'q' 'q' 'q' 'q' 'a' 'q' 'q' 'p' 'q' 'p' 'q' 'p' 'q' 'p' 'q'\n",
      " 'q' 'p' 'p' 'q' 'p' 'q' 'p' 'p' 'p' 'q' 'a' 'q' 'q' 'q' 'q' 'p' 'l' 'a'\n",
      " 'p' 'a' 'p' 'p' 'a' 'q' 'p' 'p' 'p' 'q' 'p' 'p' 'p' 'p' 'q' 'q' 'p' 'a'\n",
      " 'p' 'q' 'q' 'a' 'p' 'p' 'p' 'a' 'a' 'q' 'p' 'q' 'l' 'l' 'q' 'q' 'q' 'q'\n",
      " 'q' 'p' 'p' 'p' 'p' 'q' 'p' 'p' 'q' 'p' 'q' 'a' 'p' 'q' 'p' 'p' 'p' 'q'\n",
      " 'p' 'q' 'p' 'p' 'q' 'p' 'q' 'q' 'p' 'q' 'q' 'q' 'p' 'q' 'q' 'p' 'q' 'q'\n",
      " 'q' 'q' 'q' 'q' 'p' 'q' 'p' 'p' 'q' 'q' 'q' 'p' 'q' 'q' 'q' 'p' 'p' 'q'\n",
      " 'p' 'q' 'p' 'p' 'q' 'q' 'p' 'p' 'p' 'p' 'p' 'p' 'p' 'p' 'q' 'p' 'q' 'q'\n",
      " 'l' 'p' 'p' 'q' 'p' 'p' 'q' 'p' 'l' 'p' 'q' 'q' 'p' 'l' 'q' 'p' 'p' 'p'\n",
      " 'p' 'q' 'q' 'q' 'q' 'p' 'q' 'q' 'q' 'p' 'p' 'q' 'q' 'q' 'q' 'q' 'q' 'q'\n",
      " 'q' 'q' 'q' 'p' 'p' 'q' 'q' 'q' 'p' 'q' 'q' 'q' 'q' 'p' 'q' 'q' 'p' 'q'\n",
      " 'p' 'p' 'p' 'q' 'q' 'l' 'p' 'q' 'p' 'q' 'q' 'l' 'l' 'p' 'q' 'p' 'q' 'p'\n",
      " 'q' 'p' 'q' 'p' 'p' 'q' 'p' 'p' 'l' 'q' 'q' 'p' 'p' 'q' 'p' 'p' 'p' 'p'\n",
      " 'p' 'p' 'p' 'q' 'p' 'q' 'q' 'p' 'p' 'q' 'q' 'q' 'p' 'q' 'p' 'q' 'p' 'p'\n",
      " 'q' 'q' 'q' 'q' 'q' 'p' 'p' 'l' 'p' 'q' 'q' 'l' 'q' 'q' 'l' 'a' 'p' 'q'\n",
      " 'l' 'a' 'q' 'p' 'p' 'q' 'p' 'q' 'p' 'p' 'p' 'q' 'q' 'p' 'p' 'q' 'p' 'q'\n",
      " 'p' 'p' 'p' 'p' 'q' 'p' 'q' 'p' 'q' 'p' 'q' 'q' 'p' 'q' 'q' 'p' 'p' 'q'\n",
      " 'q' 'p' 'p' 'p' 'p' 'q' 'q' 'p' 'p' 'q' 'p' 'q' 'q' 'q' 'p' 'p' 'q' 'q'\n",
      " 'p' 'q' 'q' 'q' 'q' 'q' 'q' 'p' 'p' 'q' 'p' 'p' 'q' 'p' 'p' 'q' 'p' 'p'\n",
      " 'p' 'q' 'q' 'l' 'p' 'p' 'p' 'q' 'p' 'p' 'q' 'q' 'p' 'p' 'q' 'q' 'q' 'q'\n",
      " 'q' 'p' 'p' 'p' 'p' 'q' 'p' 'q' 'q' 'q' 'a' 'q' 'p' 'q' 'q' 'q' 'q' 'q'\n",
      " 'q' 'p' 'p' 'q' 'l' 'p' 'p' 'p' 'p' 'q' 'p' 'q' 'p' 'p' 'q' 'p' 'q' 'p'\n",
      " 'p' 'q' 'p' 'p' 'q' 'p' 'p' 'p' 'p' 'q' 'p' 'p' 'p' 'q' 'p' 'p' 'p' 'p'\n",
      " 'q' 'q' 'q' 'p' 'p' 'q' 'p' 'q' 'q' 'q' 'p' 'q' 'p' 'p' 'p' 'p' 'p' 'p'\n",
      " 'p' 'p' 'q' 'q' 'p' 'p' 'p' 'p' 'q' 'p' 'p' 'q' 'q' 'q' 'p' 'p' 'p' 'p'\n",
      " 'q' 'l' 'a' 'p' 'q' 'q' 'q' 'p' 'q' 'q' 'q' 'q' 'p' 'p' 'q' 'p' 'q' 'p'\n",
      " 'p' 'q' 'q' 'q' 'q' 'p' 'q' 'p' 'p' 'p' 'p' 'p' 'p' 'q' 'q' 'q' 'p' 'p'\n",
      " 'q' 'p' 'q' 'p' 'q' 'q' 'q' 'q' 'p' 'a' 'a' 'q' 'q']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m fn  \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(s,task,\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(s),task))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(fn):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     subj_data \u001b[39m=\u001b[39m read_load_data(subject\u001b[39m=\u001b[39;49msubject,fn\u001b[39m=\u001b[39;49mfn,cols\u001b[39m=\u001b[39;49mcols)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m cols:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         subj_data[c] \u001b[39m=\u001b[39m subj_data[c]\u001b[39m.\u001b[39mastype(\u001b[39mfloat\u001b[39m)        \n",
      "\u001b[1;32m/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m cpdm_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(fn)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m task\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpdm\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m cpdm_df \u001b[39m=\u001b[39m mf\u001b[39m.\u001b[39;49mremap_response(cpdm_df,task\u001b[39m=\u001b[39;49mtask)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m cpdm_df \u001b[39m=\u001b[39m mf\u001b[39m.\u001b[39mdrop_pract(cpdm_df,task\u001b[39m=\u001b[39mtask)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m cpdm_df,response_rate \u001b[39m=\u001b[39m mf\u001b[39m.\u001b[39mdrop_non_responses(cpdm_df,task\u001b[39m=\u001b[39mtask,verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \n",
      "File \u001b[0;32m~/IDM/IDM_model/src/model_functions.py:94\u001b[0m, in \u001b[0;36mremap_response\u001b[0;34m(df, task)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[39m# create task_choice\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[39mprint\u001b[39m(df[resp_key_col]\u001b[39m.\u001b[39mvalues)\n\u001b[0;32m---> 94\u001b[0m     task_choice \u001b[39m=\u001b[39m [c \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(c)\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m choice_dict[c] \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m df[resp_key_col]\u001b[39m.\u001b[39mvalues]\n\u001b[1;32m     95\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_choice\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(task)] \u001b[39m=\u001b[39m task_choice\n\u001b[1;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     \u001b[39m# create task_choice\u001b[39;00m\n",
      "File \u001b[0;32m~/IDM/IDM_model/src/model_functions.py:94\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[39m# create task_choice\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[39mprint\u001b[39m(df[resp_key_col]\u001b[39m.\u001b[39mvalues)\n\u001b[0;32m---> 94\u001b[0m     task_choice \u001b[39m=\u001b[39m [c \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39;49m(c)\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m choice_dict[c] \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m df[resp_key_col]\u001b[39m.\u001b[39mvalues]\n\u001b[1;32m     95\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_choice\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(task)] \u001b[39m=\u001b[39m task_choice\n\u001b[1;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     \u001b[39m# create task_choice\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "# Takes about 10 seconds\n",
    "\n",
    "# we will change this when we change utility to 1st level analysis (or split)\n",
    "split_dir = '/Volumes/UCDN/datasets/IDM/split/'\n",
    "utility_dir = '/Volumes/UCDN/datasets/IDM/utility/'\n",
    "save_dir = '/Volumes/UCDN/datasets/IDM/utility/BHM/cpdm/'\n",
    "subjs = sorted(glob.glob(os.path.join(split_dir,'23_IDM_*')))\n",
    "task = 'cpdm'\n",
    "cols = ['cpdm_gabor_orient', 'cpdm_gabor_contrast', 'cpdm_run_dimension', 'cpdm_trial_resp_keys','cpdm_trial_resp_rt']\n",
    "data = pd.DataFrame(columns=['subject']+cols)\n",
    "\n",
    "for subj_id,s in enumerate(subjs):\n",
    "    subject = os.path.basename(s)\n",
    "    fn  = os.path.join(s,task,'{}_{}.csv'.format(os.path.basename(s),task))\n",
    "    if os.path.exists(fn):\n",
    "        subj_data = read_load_data(subject=subject,fn=fn,cols=cols)\n",
    "        for c in cols:\n",
    "            subj_data[c] = subj_data[c].astype(float)        \n",
    "        subj_data['subject_id'] = int(subj_id)\n",
    "        data = pd.concat([data,subj_data],ignore_index=True)\n",
    "\n",
    "data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subjects = data['subject'].unique()\n",
    "# subjects = subjects[:10]\n",
    "# print(subjects)\n",
    "data = data.loc[data['subject'].isin(subjects)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subjects = data['subject'].unique()\n",
    "nb_subj = subjects.shape[0]\n",
    "nb_trials = data.shape[0]//nb_subj\n",
    "subj_id_list = data['subject_id'].to_list()\n",
    "subj_id = [int(s) for s in subj_id_list]\n",
    "\n",
    "# subj_id = data['subject'].to_list()\n",
    "\n",
    "old_id = np.array([ [s]*nb_trials for s in range(nb_subj) ]).flatten()\n",
    "\n",
    "delay_amt = data['cdd_delay_amt'].values\n",
    "delay_wait = data['cdd_delay_wait'].values\n",
    "immed_amt = data['cdd_immed_amt'].values\n",
    "immed_wait = data['cdd_immed_wait'].values\n",
    "alpha = data['alpha'].values\n",
    "choices = data['cdd_choice'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Hierarchical Model\n",
    "\n",
    "Developed under parameter receovery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tStep1 = time.time()\n",
    "\n",
    "# We will fit a model for each subject\n",
    "with pm.Model() as model_simple:\n",
    "\n",
    "    # Hyperparameters for kappa and gamma\n",
    "    # estimated from MLE approximations : np.exp(-3.60) = 0.0273, np.sqrt(1.71)=1.308\n",
    "    mu_kappa_hyper = pm.Beta('mu_kappa_hyper',mu=np.exp(-3.60),sigma=0.01)\n",
    "    sd_kappa_hyper = pm.Normal('sd_kappa_hyper',mu=np.sqrt(1.71),sigma=0.1)\n",
    "    # estimated from MLE approximations : np.sqrt(2.30) = 1.517\n",
    "    sd_gamma_hyper = pm.Normal('sd_hyper',mu=np.sqrt(2.30),sigma=0.1)\n",
    "\n",
    "    kappa = pm.LogNormal('kappa',mu=mu_kappa_hyper,sigma=sd_kappa_hyper,shape=np.size(np.unique(subj_id)))\n",
    "    gamma = pm.HalfNormal('gamma',sigma=sd_gamma_hyper,shape=np.size(np.unique(subj_id)))\n",
    "    \n",
    "    prob = pm.Deterministic('prob', 1 / (1 + pm.math.exp(-gamma[subj_id] * ( (delay_amt**alpha[subj_id])/(1+(kappa[subj_id]*delay_wait)) \n",
    "                                                                            - (immed_amt**alpha[subj_id])/(1+(kappa[subj_id]*immed_wait)) ))))\n",
    "\n",
    "    y_1 = pm.Bernoulli('y_1',p=prob,observed=choices)\n",
    "\n",
    "    trace_prior = pm.sample(10000, tune=1000, cores=5,target_accept=0.99,progressbar=False)\n",
    "\n",
    "\n",
    "\n",
    "# This is how you get a nice array. Note that this returns a pandas DataFrame, not a numpy array. Indexing is totally different.\n",
    "summary= az.summary(trace_prior,round_to=10)\n",
    "fn = os.path.join(save_dir,'BHM_model_summary_v004.csv')\n",
    "print('Saving to : {}'.format(fn))\n",
    "summary.to_csv(fn)\n",
    "\n",
    "fn = os.path.join(save_dir,'BHM_model_trace_v004.pkl')\n",
    "print('Saving to : {}'.format(fn))\n",
    "with open(fn,'wb') as buff:\n",
    "    pickle.dump({'trace':trace_prior},buff)\n",
    "    # pm.save_trace(trace_prior,fn)\n",
    "\n",
    "print('Time to complete {} aggregate BHM : {} minutes'.format(len(subjects),(time.time() - tStep1)/60.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract parameter estimates and save\n",
    "\n",
    "We can incorporate this into the script above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mean(fn,var_names=['kappa','gamma'],subjects=[]):\n",
    "    nb_subjects = len(subjects)\n",
    "    df = pd.read_csv(fn,index_col=0)\n",
    "    df_bhm = pd.DataFrame([],columns=var_names)\n",
    "    df_bhm['subject'] = subjects\n",
    "    for var in var_names:\n",
    "        ind_list = ['{}[{}]'.format(var,sub_id) for sub_id in range(nb_subjects)]\n",
    "        df_bhm[var] = df.loc[df.index.isin(ind_list)]['mean'].reset_index(drop=True)\n",
    "    return df_bhm\n",
    "\n",
    "experiment='v004'\n",
    "bhm_dir = '/Volumes/UCDN/datasets/IDM/utility/BHM/'\n",
    "bhm_fn = os.path.join(bhm_dir,'cdd_nlh','BHM_model_summary_{}.csv'.format(experiment))\n",
    "df_bhm = extract_mean(bhm_fn,var_names=['kappa','gamma'],subjects=subjects)\n",
    "split_CDD_fn = os.path.join(bhm_dir,'split_CDD_nlh_BHM.csv')\n",
    "df_bhm.to_csv(split_CDD_fn)\n",
    "df_bhm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostic plots\n",
    "\n",
    "Too many subjects to run `diagnistic_plots()`  but can run them individually\n",
    "Trace, posterior, rank plots\n",
    "\n",
    "... need to figure out for each subject how to plot_pair() which plots the bivariate distirbutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in set(subj_id):\n",
    "    coords={'kappa_dim_0': [s],'gamma_dim_0':[s]}\n",
    "    diganostic_plots(trace_prior,experiment=experiment,utility_dir=utility_dir,subject=subjects[s],task='cdd_nlh',coords=coords,var_names=['kappa','gamma'],figsize=(10,10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09d5eda77765311109b2c027e144dcf58f89dd96008cdf29c2e6b03e99df71a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

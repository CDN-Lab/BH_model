{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian parameter estimation \n",
    "\n",
    "Written for CPDM task as part of the IDM dataset collected online with Mturk. Here we are using the CASANDRE model (instead of utility model) to analyze the CPDM data\n",
    "\n",
    "Extended to work for all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules, libraries, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in/Generic Imports\n",
    "import os,sys\n",
    "import glob,time\n",
    "\n",
    "# Libs :: all are part of idm_env, except for arviz and pymc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# Lognormal inverse cumulative distribution function\n",
    "from scipy.special import ndtri\n",
    "from scipy.stats import norm\n",
    "\n",
    "# This reduces the amount of pymc output, can comment or change level to see more information\n",
    "import logging\n",
    "logger = logging.getLogger(\"pymc\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "# explicitly use a path to let the script know where the IDM_model is located to import model_functions\n",
    "parent = '/Users/pizarror/IDM'\n",
    "# adding the parent directory to the sys.path.\n",
    "sys.path.append(parent)\n",
    "from IDM_model.src import model_functions as mf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostic plots\n",
    "\n",
    "Run `diagnistic_plots()` individually by subject :: trace, posterior, bivariate densities, rank plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def diganostic_plots(trace,experiment='experiment',utility_dir='/tmp/',subject='23_IDM_0001',task='cdd_nlh',coords={},var_names=['kappa','gamma'],figsize=(10,10)):\n",
    "\n",
    "    bh_dir = os.path.join(utility_dir,subject,task,'bh')\n",
    "    if not os.path.exists(bh_dir):\n",
    "        os.makedirs(bh_dir)\n",
    "    print('Saving diagnostic plots to bh_dir : {}'.format(bh_dir))\n",
    "\n",
    "    title_dict = {'fontsize':15}\n",
    "\n",
    "    # 2by2 : rows 2 varialbes, cols 2 for distribution and sampled values\n",
    "    axes = az.plot_trace(trace, var_names=var_names,coords=coords,compact=False)\n",
    "    for r in range(axes.shape[0]):\n",
    "        for c in range(axes.shape[1]):\n",
    "            axes[r,c].set_title('{}: {}'.format(subject,var_names[r]))\n",
    "    plt.tight_layout()\n",
    "    fig_fn = os.path.join(bh_dir,'{}_{}_trace_plot.{}.eps'.format(subject,task,experiment))\n",
    "    plt.savefig(fig_fn,format='eps')\n",
    "    plt.close()\n",
    "    \n",
    "    axes = az.plot_pair(trace,kind='kde', coords=coords,var_names=var_names,marginals=True)\n",
    "    axes[0,0].set_title(subject,fontdict=title_dict)\n",
    "    axes[1,0].set_ylabel(var_names[1])\n",
    "    axes[1,0].set_xlabel(var_names[0])\n",
    "    plt.tight_layout()\n",
    "    fig_fn = os.path.join(bh_dir,'{}_{}_bivariate_densities.{}.eps'.format(subject,task,experiment))\n",
    "    plt.savefig(fig_fn,format='eps')\n",
    "    plt.close()\n",
    "    \n",
    "    axes = az.plot_posterior(trace,var_names=var_names,coords=coords)\n",
    "    # print(axes.shape)\n",
    "    for c in range(axes.shape[0]):\n",
    "        axes[c].set_title('{}: {}'.format(subject,var_names[c]),fontdict=title_dict)\n",
    "    fig_fn = os.path.join(bh_dir,'{}_{}_posterior.{}.eps'.format(subject,task,experiment))\n",
    "    plt.savefig(fig_fn,format='eps')\n",
    "    plt.close()\n",
    "\n",
    "    fig, axes = plt.subplots(1,len(var_names), figsize=figsize)\n",
    "    az.plot_rank(trace,var_names=var_names,coords=coords,ax=axes)\n",
    "    for c in range(axes.shape[0]):\n",
    "        axes[c].set_title('{}: {}'.format(subject,var_names[c]),fontdict=title_dict)\n",
    "    fig.tight_layout()\n",
    "    fig_fn = os.path.join(bh_dir,'{}_{}_rank_plot_bars.{}.eps'.format(subject,task,experiment))\n",
    "    plt.savefig(fig_fn,format='eps')\n",
    "    plt.close()\n",
    "\n",
    "    fig, axes = plt.subplots(1,len(var_names), figsize=figsize)\n",
    "    axes = az.plot_rank(trace,var_names=var_names, coords=coords,kind=\"vlines\",vlines_kwargs={'lw':0}, marker_vlines_kwargs={'lw':3},ax=axes)\n",
    "    for c in range(axes.shape[0]):\n",
    "        axes[c].set_title('{}: {}'.format(subject,var_names[c]),fontdict=title_dict)\n",
    "    fig.tight_layout()\n",
    "    fig_fn = os.path.join(bh_dir,'{}_{}_rank_plot_lines.{}.eps'.format(subject,task,experiment))\n",
    "    plt.savefig(fig_fn,format='eps')\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "We will load the data from all participants so we can run the modeling schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_load_data(subject='23_IDM_0144',fn='/tmp',cols=[]):\n",
    "    cpdm_df = pd.read_csv(fn)\n",
    "    task='cpdm'\n",
    "    cpdm_df = mf.remap_response(cpdm_df,task=task)\n",
    "    cpdm_df = mf.drop_pract(cpdm_df,task=task)\n",
    "    # cpdm_df,response_rate = mf.drop_non_responses(cpdm_df,task=task,verbose=True)\n",
    "    data = mf.get_data(cpdm_df,cols,alpha_hat=1,task=task)[0]\n",
    "    data['subject'] = subject\n",
    "    return data\n",
    "\n",
    "def dirs_and_data(root_dir='/tmp',dataset='IDM',experiment='low_vol_low_risk'):\n",
    "\n",
    "    dataset_dir = os.path.join(root_dir,dataset)\n",
    "    split_dir = os.path.join(dataset_dir,'split')\n",
    "    utility_dir = os.path.join(dataset_dir,'utility')\n",
    "    save_dir = os.path.join(utility_dir,'BHM/cpdm')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Takes about 10 seconds for 149 subjects form Mturk data\n",
    "    task = 'cpdm'\n",
    "    subjs = sorted(glob.glob(os.path.join(split_dir,'*')))\n",
    "    cols = ['cpdm_choice','cpdm_gabor_orient', 'cpdm_gabor_contrast', 'cpdm_run_dimension', 'cpdm_trial_resp.keys','cpdm_trial_resp.rt']\n",
    "    data = pd.DataFrame(columns=['subject']+cols)\n",
    "    subj_id = 0\n",
    "    for s in subjs:\n",
    "        subject = os.path.basename(s)\n",
    "        fn  = os.path.join(s,task,'{}_{}.csv'.format(os.path.basename(s),task))\n",
    "        if os.path.exists(fn):\n",
    "            subj_data = read_load_data(subject=subject,fn=fn,cols=cols)\n",
    "            if subj_data.empty:\n",
    "                continue\n",
    "            subj_data = subj_data.loc[subj_data['cpdm_run_dimension']==experiment]\n",
    "            if len(subj_data['cpdm_gabor_contrast'].unique())>1:\n",
    "                continue\n",
    "            # else:\n",
    "            #     display(subject)\n",
    "            #     display(subj_data.shape)\n",
    "            # for c in cols:\n",
    "            #     subj_data[c] = subj_data[c].astype(float)\n",
    "            subj_data['cpdm_choice'].fillna(-10,inplace=True)\n",
    "            subj_data['subject_id'] = int(subj_id)\n",
    "            subj_id = subj_id+1\n",
    "            data = pd.concat([data,subj_data],ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    return utility_dir,save_dir,data\n",
    "\n",
    "def extract_cols(data):\n",
    "    subjects = data['subject'].unique()\n",
    "    # nb_subj = subjects.shape[0]\n",
    "    # nb_trials = data.shape[0]//nb_subj\n",
    "    subj_id_list = data['subject_id'].to_list()\n",
    "    subj_id = [int(s) for s in subj_id_list]\n",
    "    # old_id = np.array([ [s]*nb_trials for s in range(nb_subj) ]).flatten()\n",
    "\n",
    "    gabor_orient = data['cpdm_gabor_orient'].values\n",
    "    gabor_contrast = data['cpdm_gabor_contrast'].values\n",
    "    run_dimension = data['cpdm_run_dimension'].values\n",
    "    trial_resp_keys = data['cpdm_trial_resp.keys'].values\n",
    "    trial_resp_rt = data['cpdm_trial_resp.rt'].values\n",
    "    choices = data['cpdm_choice'].values\n",
    "\n",
    "    return subjects,subj_id,gabor_orient,gabor_contrast,run_dimension,trial_resp_keys,trial_resp_rt,choices\n",
    "\n",
    "\n",
    "def get_respslong(choices):\n",
    "    enc = LabelBinarizer()\n",
    "    respslong = enc.fit_transform(choices)\n",
    "    # drop nan mapped to first row\n",
    "    respslong = respslong[:,1:]\n",
    "    # return transpose matrix\n",
    "    return respslong.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chop_by_subject(respslong,orislong,nt=200,ns=128):\n",
    "    resps = np.zeros((ns,nt,4))\n",
    "    oris = np.zeros((nt,ns))\n",
    "    for n in range(ns):\n",
    "        resps[n] = respslong[:,n*nt:(n+1)*nt].T\n",
    "        oris[:,n] = orislong[n*nt:(n+1)*nt]\n",
    "    return resps,oris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nt(data,experiment=''):\n",
    "    nt = 0\n",
    "    for s in data['subject'].unique():\n",
    "        if len(experiment)>0:\n",
    "            temp_nt = data.loc[(data['subject']==s) & (data['cpdm_run_dimension']==experiment)].shape[0]\n",
    "        else:\n",
    "            temp_nt = data.loc[data['subject']==s].shape[0]\n",
    "        if temp_nt > nt:\n",
    "            nt = temp_nt\n",
    "    return nt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Hierarchical Model\n",
    "\n",
    "Developing choice of prior distribution and parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llhC(guess_rate,sds,se,confidence_criterion,n=1):\n",
    "    # se is (nt,ns)\n",
    "    # sds is (sampn,ns)\n",
    "    # confidence_criterion is (ns,1)\n",
    "    llhC = np.zeros(se.shape[0],4)\n",
    "    avgs = np.multiply(np.broadcast_to(se[:,n],sds[:,n].shape),np.broadcast_to(sds[:,n],se[:,n].shape))\n",
    "    for tr in range(se.shape[0]):\n",
    "        raws = np.zeros((3,sds.shape[0]))\n",
    "        for rws in range(sds.shape[0]):\n",
    "            raws[0,rws] = norm.cdf(-confidence_criterion,avgs[tr,rws],sds[rws,n])\n",
    "            raws[1,rws] = norm.cdf(0,avgs[tr,rws],sds[rws,n])\n",
    "            raws[2,rws] = norm.cdf(confidence_criterion,avgs[tr,rws],sds[rws,n])\n",
    "        ratiodist = np.mean(raws,axis=1)\n",
    "        llhC[tr,1] = (guess_rate[n]/4) + (1-guess_rate[n])*ratiodist[1]\n",
    "        llhC[tr,2] = (guess_rate[n]/4) + (1-guess_rate[n])*(ratiodist[2]-ratiodist[1])\n",
    "        llhC[tr,3] = (guess_rate[n]/4) + (1-guess_rate[n])*(ratiodist[3]-ratiodist[2])\n",
    "        llhC[tr,4] = (guess_rate[n]/4) + (1-guess_rate[n])*(1-ratiodist[3])\n",
    "    return llhC\n",
    "            \n",
    "\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "# This is the meat of the script that is used to estimate the parameters of the BHM \n",
    "\n",
    "def run_BHM(save_dir,experiment,data):\n",
    "    \n",
    "    subjects,subj_id,gabor_orient,gabor_contrast,run_dimension,trial_resp_keys,trial_resp_rt,choices = extract_cols(data)\n",
    "    # number of samples/subjects\n",
    "    ns = np.unique(subj_id).shape[0]\n",
    "    # number of trials\n",
    "    nt = get_nt(data)\n",
    "    sampn = 30\n",
    "    sampx = np.linspace(0.5/30,1-0.5/30,sampn)\n",
    "\n",
    "    orislong = gabor_orient\n",
    "    respslong = get_respslong(choices)\n",
    "    resps,oris = chop_by_subject(respslong,orislong,nt=nt,ns=ns)\n",
    "\n",
    "    tStep1 = time.time()\n",
    "\n",
    "    # We will fit a model for each subject\n",
    "    with pm.Model() as model_simple:\n",
    "\n",
    "        # Hyperparameters for stimulus sensitivity (sens), decision criterion (deci), \n",
    "        # meta-uncertainty (meta), and confidence criterion (conf)\n",
    "        mu_sens_hyper = pm.Normal('mu_sens_hyper',mu=0,sigma=1.0)\n",
    "        sd_sens_hyper = pm.LogNormal('sd_sens_hyper',mu=0,sigma=1.0)\n",
    "        mu_deci_hyper = pm.Normal('mu_deci_hyper',mu=0,sigma=1.0)\n",
    "        sd_deci_hyper = pm.LogNormal('sd_deci_hyper',mu=0,sigma=1.0)\n",
    "        mu_meta_hyper = pm.Normal('mu_meta_hyper',mu=0,sigma=1.0)\n",
    "        sd_meta_hyper = pm.LogNormal('sd_meta_hyper',mu=0,sigma=1.0)\n",
    "        mu_conf_hyper = pm.Normal('mu_conf_hyper',mu=0,sigma=1.0)\n",
    "        sd_conf_hyper = pm.LogNormal('sd_conf_hyper',mu=0,sigma=1.0)\n",
    "\n",
    "        # Priors\n",
    "        # where does Corey get these values? mu, sigma or alpha beta?\n",
    "        guess_rate = pm.Beta('guess_rate',mu=1,sigma=193.0/3.0)\n",
    "        stimulus_sensitivity = pm.LogNormal('stimulus_sensitivity',mu=mu_sens_hyper,sigma=sd_sens_hyper,shape=np.size(np.unique(subj_id)))\n",
    "        decision_criterion = pm.Normal('decision_criterion',mu=mu_deci_hyper,sigma=sd_deci_hyper,shape=np.size(np.unique(subj_id)))\n",
    "        meta_uncertainty = pm.LogNormal('meta_uncertainty',mu=mu_meta_hyper,sigma=sd_meta_hyper,shape=np.size(np.unique(subj_id)))\n",
    "        confidence_criterion = pm.LogNormal('confidence_criterion',mu=mu_conf_hyper,sigma=sd_conf_hyper,shape=np.size(np.unique(subj_id)))\n",
    "\n",
    "        # rescaled stimulus sensitivity\n",
    "        sm = np.multiply(oris,np.broadcast_to(stimulus_sensitivity,oris.shape))\n",
    "        # rescaled confidence criterion\n",
    "        sc = np.multiply(decision_criterion,stimulus_sensitivity)\n",
    "        # difference matrix\n",
    "        se = sm - np.broadcast_to(sc,sm.shape)\n",
    "        \n",
    "        # xtrans = logncdfinv(sampx,log(1/sqrt(meta.^2+1)),sqrt(log(meta.^2+1)));\n",
    "        muLogN = np.log(1/np.sqrt(meta_uncertainty**2 + 1.0))\n",
    "        sigmaLogN = np.sqrt(np.log(meta_uncertainty**2 + 1.0))\n",
    "        # transformed xaxis per subject, lognormal inverse CDF\n",
    "        xtrans = ndtri(sampx, loc=muLogN, scale=sigmaLogN)\n",
    "        # sigma parameters for likelihood\n",
    "        sds = 1.0/xtrans\n",
    "\n",
    "        llhC = get_llhC(guess_rate,sds,se,confidence_criterion,n=subj_id)\n",
    "\n",
    "        # kappa = pm.LogNormal('kappa',mu=mu_kappa_hyper,sigma=sd_kappa_hyper,shape=np.size(np.unique(subj_id)))\n",
    "        # gamma = pm.HalfNormal('gamma',sigma=sd_gamma_hyper,shape=np.size(np.unique(subj_id)))\n",
    "        \n",
    "        # prob = pm.Deterministic('prob', 1 / (1 + pm.math.exp(-gamma[subj_id] * ( (delay_amt**alpha[subj_id])/(1+(kappa[subj_id]*delay_wait)) \n",
    "        #                                                                         - (immed_amt**alpha[subj_id])/(1+(kappa[subj_id]*immed_wait)) ))))\n",
    "        # in MLE, we use bernouli.logpmf as LL\n",
    "        # the logpmf is built into the pm.Bernoulli\n",
    "        y_1 = pm.Bernoulli('y_1',logit_p=llhC,observed=choices)\n",
    "\n",
    "        trace_prior = pm.sample(10000, tune=1000, cores=5,target_accept=0.99,progressbar=False)\n",
    "\n",
    "\n",
    "    # This is how you get a nice array. Note that this returns a pandas DataFrame, not a numpy array. Indexing is totally different.\n",
    "    summary= az.summary(trace_prior,round_to=10)\n",
    "    fn = os.path.join(save_dir,'BHM_model_summary_{}.csv'.format(experiment))\n",
    "    print('Saving to : {}'.format(fn))\n",
    "    summary.to_csv(fn)\n",
    "\n",
    "    fn = os.path.join(save_dir,'BHM_model_trace_{}.pkl'.format(experiment))\n",
    "    print('Saving to : {}'.format(fn))\n",
    "    with open(fn,'wb') as buff:\n",
    "        pickle.dump({'trace':trace_prior},buff)\n",
    "        # pm.save_trace(trace_prior,fn)\n",
    "\n",
    "    print('Time to complete {} aggregate BHM : {} minutes'.format(len(subjects),(time.time() - tStep1)/60.0))\n",
    "    return trace_prior,subjects,subj_id\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25600, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['low_vol_low_risk'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['23_IDM_0022', '23_IDM_0023', '23_IDM_0024', '23_IDM_0025',\n",
       "       '23_IDM_0026', '23_IDM_0027', '23_IDM_0028', '23_IDM_0029',\n",
       "       '23_IDM_0030', '23_IDM_0031', '23_IDM_0032', '23_IDM_0033',\n",
       "       '23_IDM_0034', '23_IDM_0035', '23_IDM_0036', '23_IDM_0037',\n",
       "       '23_IDM_0038', '23_IDM_0039', '23_IDM_0040', '23_IDM_0041',\n",
       "       '23_IDM_0042', '23_IDM_0043', '23_IDM_0044', '23_IDM_0045',\n",
       "       '23_IDM_0046', '23_IDM_0047', '23_IDM_0048', '23_IDM_0049',\n",
       "       '23_IDM_0050', '23_IDM_0051', '23_IDM_0052', '23_IDM_0053',\n",
       "       '23_IDM_0054', '23_IDM_0055', '23_IDM_0056', '23_IDM_0057',\n",
       "       '23_IDM_0058', '23_IDM_0059', '23_IDM_0060', '23_IDM_0061',\n",
       "       '23_IDM_0062', '23_IDM_0063', '23_IDM_0064', '23_IDM_0065',\n",
       "       '23_IDM_0066', '23_IDM_0067', '23_IDM_0068', '23_IDM_0069',\n",
       "       '23_IDM_0070', '23_IDM_0071', '23_IDM_0072', '23_IDM_0073',\n",
       "       '23_IDM_0074', '23_IDM_0075', '23_IDM_0076', '23_IDM_0077',\n",
       "       '23_IDM_0078', '23_IDM_0079', '23_IDM_0080', '23_IDM_0081',\n",
       "       '23_IDM_0082', '23_IDM_0083', '23_IDM_0084', '23_IDM_0085',\n",
       "       '23_IDM_0086', '23_IDM_0087', '23_IDM_0088', '23_IDM_0089',\n",
       "       '23_IDM_0090', '23_IDM_0091', '23_IDM_0092', '23_IDM_0093',\n",
       "       '23_IDM_0094', '23_IDM_0095', '23_IDM_0096', '23_IDM_0097',\n",
       "       '23_IDM_0098', '23_IDM_0099', '23_IDM_0100', '23_IDM_0101',\n",
       "       '23_IDM_0102', '23_IDM_0103', '23_IDM_0104', '23_IDM_0105',\n",
       "       '23_IDM_0106', '23_IDM_0107', '23_IDM_0108', '23_IDM_0109',\n",
       "       '23_IDM_0110', '23_IDM_0111', '23_IDM_0112', '23_IDM_0113',\n",
       "       '23_IDM_0114', '23_IDM_0115', '23_IDM_0116', '23_IDM_0117',\n",
       "       '23_IDM_0118', '23_IDM_0119', '23_IDM_0120', '23_IDM_0121',\n",
       "       '23_IDM_0122', '23_IDM_0123', '23_IDM_0124', '23_IDM_0125',\n",
       "       '23_IDM_0126', '23_IDM_0127', '23_IDM_0128', '23_IDM_0129',\n",
       "       '23_IDM_0130', '23_IDM_0131', '23_IDM_0132', '23_IDM_0133',\n",
       "       '23_IDM_0134', '23_IDM_0135', '23_IDM_0136', '23_IDM_0137',\n",
       "       '23_IDM_0138', '23_IDM_0139', '23_IDM_0140', '23_IDM_0141',\n",
       "       '23_IDM_0142', '23_IDM_0143', '23_IDM_0144', '23_IDM_0145',\n",
       "       '23_IDM_0146', '23_IDM_0147', '23_IDM_0148', '23_IDM_0149'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(25600, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['low_vol_high_risk'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['23_IDM_0022', '23_IDM_0023', '23_IDM_0024', '23_IDM_0025',\n",
       "       '23_IDM_0026', '23_IDM_0027', '23_IDM_0028', '23_IDM_0029',\n",
       "       '23_IDM_0030', '23_IDM_0031', '23_IDM_0032', '23_IDM_0033',\n",
       "       '23_IDM_0034', '23_IDM_0035', '23_IDM_0036', '23_IDM_0037',\n",
       "       '23_IDM_0038', '23_IDM_0039', '23_IDM_0040', '23_IDM_0041',\n",
       "       '23_IDM_0042', '23_IDM_0043', '23_IDM_0044', '23_IDM_0045',\n",
       "       '23_IDM_0046', '23_IDM_0047', '23_IDM_0048', '23_IDM_0049',\n",
       "       '23_IDM_0050', '23_IDM_0051', '23_IDM_0052', '23_IDM_0053',\n",
       "       '23_IDM_0054', '23_IDM_0055', '23_IDM_0056', '23_IDM_0057',\n",
       "       '23_IDM_0058', '23_IDM_0059', '23_IDM_0060', '23_IDM_0061',\n",
       "       '23_IDM_0062', '23_IDM_0063', '23_IDM_0064', '23_IDM_0065',\n",
       "       '23_IDM_0066', '23_IDM_0067', '23_IDM_0068', '23_IDM_0069',\n",
       "       '23_IDM_0070', '23_IDM_0071', '23_IDM_0072', '23_IDM_0073',\n",
       "       '23_IDM_0074', '23_IDM_0075', '23_IDM_0076', '23_IDM_0077',\n",
       "       '23_IDM_0078', '23_IDM_0079', '23_IDM_0080', '23_IDM_0081',\n",
       "       '23_IDM_0082', '23_IDM_0083', '23_IDM_0084', '23_IDM_0085',\n",
       "       '23_IDM_0086', '23_IDM_0087', '23_IDM_0088', '23_IDM_0089',\n",
       "       '23_IDM_0090', '23_IDM_0091', '23_IDM_0092', '23_IDM_0093',\n",
       "       '23_IDM_0094', '23_IDM_0095', '23_IDM_0096', '23_IDM_0097',\n",
       "       '23_IDM_0098', '23_IDM_0099', '23_IDM_0100', '23_IDM_0101',\n",
       "       '23_IDM_0102', '23_IDM_0103', '23_IDM_0104', '23_IDM_0105',\n",
       "       '23_IDM_0106', '23_IDM_0107', '23_IDM_0108', '23_IDM_0109',\n",
       "       '23_IDM_0110', '23_IDM_0111', '23_IDM_0112', '23_IDM_0113',\n",
       "       '23_IDM_0114', '23_IDM_0115', '23_IDM_0116', '23_IDM_0117',\n",
       "       '23_IDM_0118', '23_IDM_0119', '23_IDM_0120', '23_IDM_0121',\n",
       "       '23_IDM_0122', '23_IDM_0123', '23_IDM_0124', '23_IDM_0125',\n",
       "       '23_IDM_0126', '23_IDM_0127', '23_IDM_0128', '23_IDM_0129',\n",
       "       '23_IDM_0130', '23_IDM_0131', '23_IDM_0132', '23_IDM_0133',\n",
       "       '23_IDM_0134', '23_IDM_0135', '23_IDM_0136', '23_IDM_0137',\n",
       "       '23_IDM_0138', '23_IDM_0139', '23_IDM_0140', '23_IDM_0141',\n",
       "       '23_IDM_0142', '23_IDM_0143', '23_IDM_0144', '23_IDM_0145',\n",
       "       '23_IDM_0146', '23_IDM_0147', '23_IDM_0148', '23_IDM_0149'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the paths needed to find the data and load\n",
    "\n",
    "# The user of the script can edit root_dir and dataset variables to get it to work for their dataset.\n",
    "# This script will work given the data is stored in the appropriate BIDS format in the split directory\n",
    "root_dir = '/Volumes/UCDN/datasets/'\n",
    "dataset = 'IDM'\n",
    "\n",
    "# log the BHM experiment version, in case we cahnge it in the future\n",
    "\n",
    "\n",
    "for experiment in ['low_vol_low_risk','low_vol_high_risk']:\n",
    "\n",
    "    utility_dir,save_dir,data = dirs_and_data(root_dir=root_dir,dataset=dataset,experiment=experiment)\n",
    "    display(data.shape)\n",
    "    subjects,subj_id,gabor_orient,gabor_contrast,run_dimension,trial_resp_keys,trial_resp_rt,choices = extract_cols(data)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.5       , 0.33333333])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = 1/a\n",
    "display(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6757, 5637, 5779, 7353])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "respslong = get_respslong(choices)\n",
    "display(np.sum(respslong,axis=1))\n",
    "# respslong = respslong[1:,:]\n",
    "# display(np.sum(respslong,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((2,3,4))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 200, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(200, 128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orislong = gabor_orient\n",
    "resps,oris = chop_by_subject(respslong,orislong,nt=200,ns=128)\n",
    "display(resps.shape)\n",
    "display(oris.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 95, 79,  5])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(respslong[:,:200].T,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20., 95., 79.,  5.])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(resps[0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nt = get_nt(data,experiment = 'low_vol_high_risk')\n",
    "display(nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(run_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orislong = gabor_orient\n",
    "sc = 3*orislong\n",
    "display(sc.shape)\n",
    "sc = np.broadcast_to(orislong,(5,len(orislong)))\n",
    "display(sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "trace_prior,subjects,subj_id = run_BHM(save_dir,experiment,data)\n",
    "df_bhm = extract_mean(save_dir=save_dir,var_names=['kappa','gamma'],\n",
    "                        subjects=subjects,experiment=experiment)\n",
    "for s in set(subj_id):\n",
    "    coords={'kappa_dim_0': [s],'gamma_dim_0':[s]}\n",
    "    diganostic_plots(trace_prior,experiment=experiment,utility_dir=utility_dir,subject=subjects[s],\n",
    "                        coords=coords,var_names=['kappa','gamma'],figsize=(10,10))\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Takes about 10 seconds\n",
    "\n",
    "# # we will change this when we change utility to 1st level analysis (or split)\n",
    "# split_dir = '/Volumes/UCDN/datasets/IDM/split/'\n",
    "# utility_dir = '/Volumes/UCDN/datasets/IDM/utility/'\n",
    "# save_dir = '/Volumes/UCDN/datasets/IDM/utility/BHM/cpdm/'\n",
    "# subjs = sorted(glob.glob(os.path.join(split_dir,'23_IDM_*')))\n",
    "# task = 'cpdm'\n",
    "# cols = ['cpdm_choice','cpdm_gabor_orient', 'cpdm_gabor_contrast', 'cpdm_run_dimension', 'cpdm_trial_resp.keys','cpdm_trial_resp.rt']\n",
    "# data = pd.DataFrame(columns=['subject']+cols)\n",
    "\n",
    "# for subj_id,s in enumerate(subjs):\n",
    "#     subject = os.path.basename(s)\n",
    "#     fn  = os.path.join(s,task,'{}_{}.csv'.format(os.path.basename(s),task))\n",
    "#     if os.path.exists(fn):\n",
    "#         subj_data = read_load_data(subject=subject,fn=fn,cols=cols)\n",
    "#         # for c in cols:\n",
    "#             # subj_data[c] = subj_data[c].astype(float)        \n",
    "#         subj_data['subject_id'] = int(subj_id)\n",
    "#         data = pd.concat([data,subj_data],ignore_index=True)\n",
    "\n",
    "# data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# parameters to model\n",
    "# nParams = 2 + numTasks + totRel + numTasks*nConfCrit; % [Guess rate, stimulus criterion], [meta-uncertainty], [stimulus sensitivity], [confidence criteria]\n",
    "\n",
    "\n",
    "# Required order for getLlhChoice: [guess rate, stim sens, stim crit, meta-uncertainty, conf criteria]\n",
    "\n",
    "\n",
    "# 2 :: [Guess rate, stimulus criterion]\n",
    "# numTasks :: [meta-uncertainty]\n",
    "# totRel :: [stimulus sensitivity]\n",
    "# numTasks*nConfCrit :: [confidence criteria]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09d5eda77765311109b2c027e144dcf58f89dd96008cdf29c2e6b03e99df71a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

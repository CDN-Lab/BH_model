{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian parameter estimation \n",
    "\n",
    "** WORK IN PROGRESS **\n",
    "\n",
    "Written for CPDM task as part of the IDM dataset collected online with Mturk. Here we are using the CASANDRE model (instead of utility model) to analyze the CPDM data\n",
    "\n",
    "Extended to work for all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules, libraries, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in/Generic Imports\n",
    "import os,sys\n",
    "import glob,time\n",
    "\n",
    "# Libs :: all are part of idm_env, except for arviz and pymc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# Lognormal inverse cumulative distribution function\n",
    "from scipy.special import erfinv,ndtri_exp\n",
    "from scipy.stats import norm, lognorm\n",
    "\n",
    "# This reduces the amount of pymc output, can comment or change level to see more information\n",
    "import logging\n",
    "logger = logging.getLogger(\"pymc\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "# explicitly use a path to let the script know where the IDM_model is located to import model_functions\n",
    "parent = '/Users/pizarror/IDM'\n",
    "# adding the parent directory to the sys.path.\n",
    "sys.path.append(parent)\n",
    "from IDM_model.src import model_functions as mf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostic plots\n",
    "\n",
    "Run `diagnistic_plots()` individually by subject :: trace, posterior, bivariate densities, rank plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def diganostic_plots(trace,experiment='experiment',utility_dir='/tmp/',subject='23_IDM_0001',task='cdd_nlh',coords={},var_names=['kappa','gamma'],figsize=(10,10)):\n",
    "\n",
    "    bh_dir = os.path.join(utility_dir,subject,task,'bh')\n",
    "    if not os.path.exists(bh_dir):\n",
    "        os.makedirs(bh_dir)\n",
    "    print('Saving diagnostic plots to bh_dir : {}'.format(bh_dir))\n",
    "\n",
    "    title_dict = {'fontsize':15}\n",
    "\n",
    "    # 2by2 : rows 2 varialbes, cols 2 for distribution and sampled values\n",
    "    axes = az.plot_trace(trace, var_names=var_names,coords=coords,compact=False)\n",
    "    for r in range(axes.shape[0]):\n",
    "        for c in range(axes.shape[1]):\n",
    "            axes[r,c].set_title('{}: {}'.format(subject,var_names[r]))\n",
    "    plt.tight_layout()\n",
    "    fig_fn = os.path.join(bh_dir,'{}_{}_trace_plot.{}.eps'.format(subject,task,experiment))\n",
    "    plt.savefig(fig_fn,format='eps')\n",
    "    plt.close()\n",
    "    \n",
    "    axes = az.plot_pair(trace,kind='kde', coords=coords,var_names=var_names,marginals=True)\n",
    "    axes[0,0].set_title(subject,fontdict=title_dict)\n",
    "    axes[1,0].set_ylabel(var_names[1])\n",
    "    axes[1,0].set_xlabel(var_names[0])\n",
    "    plt.tight_layout()\n",
    "    fig_fn = os.path.join(bh_dir,'{}_{}_bivariate_densities.{}.eps'.format(subject,task,experiment))\n",
    "    plt.savefig(fig_fn,format='eps')\n",
    "    plt.close()\n",
    "    \n",
    "    axes = az.plot_posterior(trace,var_names=var_names,coords=coords)\n",
    "    # print(axes.shape)\n",
    "    for c in range(axes.shape[0]):\n",
    "        axes[c].set_title('{}: {}'.format(subject,var_names[c]),fontdict=title_dict)\n",
    "    fig_fn = os.path.join(bh_dir,'{}_{}_posterior.{}.eps'.format(subject,task,experiment))\n",
    "    plt.savefig(fig_fn,format='eps')\n",
    "    plt.close()\n",
    "\n",
    "    fig, axes = plt.subplots(1,len(var_names), figsize=figsize)\n",
    "    az.plot_rank(trace,var_names=var_names,coords=coords,ax=axes)\n",
    "    for c in range(axes.shape[0]):\n",
    "        axes[c].set_title('{}: {}'.format(subject,var_names[c]),fontdict=title_dict)\n",
    "    fig.tight_layout()\n",
    "    fig_fn = os.path.join(bh_dir,'{}_{}_rank_plot_bars.{}.eps'.format(subject,task,experiment))\n",
    "    plt.savefig(fig_fn,format='eps')\n",
    "    plt.close()\n",
    "\n",
    "    fig, axes = plt.subplots(1,len(var_names), figsize=figsize)\n",
    "    axes = az.plot_rank(trace,var_names=var_names, coords=coords,kind=\"vlines\",vlines_kwargs={'lw':0}, marker_vlines_kwargs={'lw':3},ax=axes)\n",
    "    for c in range(axes.shape[0]):\n",
    "        axes[c].set_title('{}: {}'.format(subject,var_names[c]),fontdict=title_dict)\n",
    "    fig.tight_layout()\n",
    "    fig_fn = os.path.join(bh_dir,'{}_{}_rank_plot_lines.{}.eps'.format(subject,task,experiment))\n",
    "    plt.savefig(fig_fn,format='eps')\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "We will load the data from all participants so we can run the modeling schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_load_data(subject='23_IDM_0144',fn='/tmp',cols=[]):\n",
    "    cpdm_df = pd.read_csv(fn)\n",
    "    task='cpdm'\n",
    "    cpdm_df = mf.remap_response(cpdm_df,task=task)\n",
    "    cpdm_df = mf.drop_pract(cpdm_df,task=task)\n",
    "    # cpdm_df,response_rate = mf.drop_non_responses(cpdm_df,task=task,verbose=True)\n",
    "    data = mf.get_data(cpdm_df,cols,alpha_hat=1,task=task)[0]\n",
    "    data['subject'] = subject\n",
    "    return data\n",
    "\n",
    "def dirs_and_data(root_dir='/tmp',dataset='IDM',experiment='low_vol_low_risk'):\n",
    "\n",
    "    dataset_dir = os.path.join(root_dir,dataset)\n",
    "    split_dir = os.path.join(dataset_dir,'split')\n",
    "    utility_dir = os.path.join(dataset_dir,'utility')\n",
    "    save_dir = os.path.join(utility_dir,'BHM/cpdm')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Takes about 10 seconds for 149 subjects form Mturk data\n",
    "    task = 'cpdm'\n",
    "    subjs = sorted(glob.glob(os.path.join(split_dir,'*')))\n",
    "    cols = ['cpdm_choice','cpdm_gabor_orient', 'cpdm_gabor_contrast', 'cpdm_run_dimension', 'cpdm_trial_resp.keys','cpdm_trial_resp.rt']\n",
    "    data = pd.DataFrame(columns=['subject']+cols)\n",
    "    subj_id = 0\n",
    "    for s in subjs:\n",
    "        subject = os.path.basename(s)\n",
    "        fn  = os.path.join(s,task,'{}_{}.csv'.format(os.path.basename(s),task))\n",
    "        if os.path.exists(fn):\n",
    "            subj_data = read_load_data(subject=subject,fn=fn,cols=cols)\n",
    "            if subj_data.empty:\n",
    "                continue\n",
    "            subj_data = subj_data.loc[subj_data['cpdm_run_dimension']==experiment]\n",
    "            if len(subj_data['cpdm_gabor_contrast'].unique())>1:\n",
    "                continue\n",
    "            # else:\n",
    "            #     display(subject)\n",
    "            #     display(subj_data.shape)\n",
    "            # for c in cols:\n",
    "            #     subj_data[c] = subj_data[c].astype(float)\n",
    "            subj_data['cpdm_choice'].fillna(-10,inplace=True)\n",
    "            subj_data['subject_id'] = int(subj_id)\n",
    "            subj_id = subj_id+1\n",
    "            data = pd.concat([data,subj_data],ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    return utility_dir,save_dir,data\n",
    "\n",
    "def extract_cols(data):\n",
    "    subjects = data['subject'].unique()\n",
    "    # nb_subj = subjects.shape[0]\n",
    "    # nb_trials = data.shape[0]//nb_subj\n",
    "    subj_id_list = data['subject_id'].to_list()\n",
    "    subj_id = [int(s) for s in subj_id_list]\n",
    "    # old_id = np.array([ [s]*nb_trials for s in range(nb_subj) ]).flatten()\n",
    "\n",
    "    gabor_orient = data['cpdm_gabor_orient'].values\n",
    "    gabor_contrast = data['cpdm_gabor_contrast'].values\n",
    "    run_dimension = data['cpdm_run_dimension'].values\n",
    "    trial_resp_keys = data['cpdm_trial_resp.keys'].values\n",
    "    trial_resp_rt = data['cpdm_trial_resp.rt'].values\n",
    "    choices = data['cpdm_choice'].values\n",
    "\n",
    "    return subjects,subj_id,gabor_orient,gabor_contrast,run_dimension,trial_resp_keys,trial_resp_rt,choices\n",
    "\n",
    "\n",
    "def get_respslong(choices):\n",
    "    enc = LabelBinarizer()\n",
    "    respslong = enc.fit_transform(choices)\n",
    "    # drop nan mapped to first row\n",
    "    respslong = respslong[:,1:]\n",
    "    # return transpose matrix\n",
    "    return respslong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chop_by_subject(respslong,orislong,nt=200,ns=128):\n",
    "    resps = np.zeros((ns,nt,4))\n",
    "    oris = np.zeros((nt,ns))\n",
    "    for n in range(ns):\n",
    "        resps[n] = respslong[n*nt:(n+1)*nt,:]\n",
    "        oris[:,n] = orislong[n*nt:(n+1)*nt]\n",
    "    return resps,oris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nt(data,experiment=''):\n",
    "    nt = 0\n",
    "    for s in data['subject'].unique():\n",
    "        if len(experiment)>0:\n",
    "            temp_nt = data.loc[(data['subject']==s) & (data['cpdm_run_dimension']==experiment)].shape[0]\n",
    "        else:\n",
    "            temp_nt = data.loc[data['subject']==s].shape[0]\n",
    "        if temp_nt > nt:\n",
    "            nt = temp_nt\n",
    "    return nt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Hierarchical Model\n",
    "\n",
    "Developing choice of prior distribution and parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CASANDRE_ll(nt,sampn,resps,guess_rate,sds,se,confidence_criterion,subj_id):\n",
    "    ll = 0\n",
    "    for n in subj_id:\n",
    "        # se is (nt,ns)\n",
    "        # sds is (sampn,ns)\n",
    "        # confidence_criterion is (ns,1)\n",
    "        llhC = np.zeros((se.shape[0],4))\n",
    "        # (nt,sampn)\n",
    "        # display(se[:,n].shape)\n",
    "        # display(sds[:,n].shape)\n",
    "        # display(np.broadcast_to(se[:,n],(nt,sampn)))\n",
    "        # display(np.broadcast_to(sds[:,n],(nt,sampn)))\n",
    "        avgs = np.multiply( np.tile(se[:,n],sampn).reshape(nt,sampn),\n",
    "                            np.tile(sds[:,n],nt).reshape(sampn,nt).T )\n",
    "        # np.tile(se[:,0],30).reshape(nt,30).shape\n",
    "        # avgs = np.multiply(np.broadcast_to(se[:,n].T,(nt,sampn)),np.broadcast_to(sds[:,n],(nt,sampn)))\n",
    "        for tr in range(se.shape[0]):\n",
    "            raws = np.zeros((3,sds.shape[0]))\n",
    "            for rws in range(sds.shape[0]):\n",
    "                raws[0,rws] = norm.cdf(-confidence_criterion[n],avgs[tr,rws],sds[rws,n])\n",
    "                raws[1,rws] = norm.cdf(0,avgs[tr,rws],sds[rws,n])\n",
    "                raws[2,rws] = norm.cdf(confidence_criterion[n],avgs[tr,rws],sds[rws,n])\n",
    "            ratiodist = np.mean(raws,axis=1)\n",
    "            llhC[tr,1] = (guess_rate[n]/4) + (1-guess_rate[n])*ratiodist[1]\n",
    "            llhC[tr,2] = (guess_rate[n]/4) + (1-guess_rate[n])*(ratiodist[2]-ratiodist[1])\n",
    "            llhC[tr,3] = (guess_rate[n]/4) + (1-guess_rate[n])*(ratiodist[3]-ratiodist[2])\n",
    "            llhC[tr,4] = (guess_rate[n]/4) + (1-guess_rate[n])*(1-ratiodist[3])\n",
    "        ll = ll + pm.Bernoulli('ll',logit_p=llhC,observed=resps[n])\n",
    "    return ll\n",
    "            \n",
    "# def logncdfinv(x,m,s):\n",
    "#     display(np.broadcast_to(erfinv(-2*x + 2),m.size(dim=0) ).shape)\n",
    "#     display(np.broadcast_to(m,x.shape[0]).shape)\n",
    "#     display(np.broadcast_to(s,x.shape[0]).shape)\n",
    "#     y = np.exp( np.multiply( np.broadcast_to(s,x.shape[0])*np.sqrt(2) , \n",
    "#                             np.broadcast_to(erfinv(-2*x + 2),m.shape[0] ) ) +\n",
    "#                             np.broadcast_to(m,x.shape[0]) )\n",
    "#     return y\n",
    "\n",
    "# This is the meat of the script that is used to estimate the parameters of the BHM \n",
    "\n",
    "def run_BHM(save_dir,experiment,data):\n",
    "    \n",
    "    subjects,subj_id,gabor_orient,gabor_contrast,run_dimension,trial_resp_keys,trial_resp_rt,choices = extract_cols(data)\n",
    "    # number of samples/subjects\n",
    "    ns = np.unique(subj_id).shape[0]\n",
    "    # number of trials\n",
    "    nt = get_nt(data)\n",
    "    sampn = 30\n",
    "    sampx = np.linspace(0.5/30,1-0.5/30,sampn)\n",
    "\n",
    "    orislong = gabor_orient\n",
    "    respslong = get_respslong(choices)\n",
    "    resps,oris = chop_by_subject(respslong,orislong,nt=nt,ns=ns)\n",
    "\n",
    "    tStep1 = time.time()\n",
    "\n",
    "    # We will fit a model for each subject\n",
    "    with pm.Model() as model_simple:\n",
    "\n",
    "        # Hyperparameters for stimulus sensitivity (sens), decision criterion (deci), \n",
    "        # meta-uncertainty (meta), and confidence criterion (conf)\n",
    "        mu_sens_hyper = pm.Normal('mu_sens_hyper',mu=0,sigma=1.0)\n",
    "        sd_sens_hyper = pm.LogNormal('sd_sens_hyper',mu=0,sigma=1.0)\n",
    "        mu_deci_hyper = pm.Normal('mu_deci_hyper',mu=0,sigma=1.0)\n",
    "        sd_deci_hyper = pm.LogNormal('sd_deci_hyper',mu=0,sigma=1.0)\n",
    "        mu_meta_hyper = pm.Normal('mu_meta_hyper',mu=0,sigma=1.0)\n",
    "        sd_meta_hyper = pm.LogNormal('sd_meta_hyper',mu=0,sigma=1.0)\n",
    "        mu_conf_hyper = pm.Normal('mu_conf_hyper',mu=0,sigma=1.0)\n",
    "        sd_conf_hyper = pm.LogNormal('sd_conf_hyper',mu=0,sigma=1.0)\n",
    "\n",
    "        # Priors\n",
    "        # where does Corey get these values? mu, sigma or alpha beta?\n",
    "        guess_rate = pm.Beta('guess_rate',mu=1,sigma=193.0/3.0,shape=np.size(np.unique(subj_id)))\n",
    "        stimulus_sensitivity = pm.LogNormal('stimulus_sensitivity',mu=mu_sens_hyper,sigma=sd_sens_hyper,shape=np.size(np.unique(subj_id)))\n",
    "        decision_criterion = pm.Normal('decision_criterion',mu=mu_deci_hyper,sigma=sd_deci_hyper,shape=np.size(np.unique(subj_id)))\n",
    "        meta_uncertainty = pm.LogNormal('meta_uncertainty',mu=mu_meta_hyper,sigma=sd_meta_hyper,shape=np.size(np.unique(subj_id)))\n",
    "        confidence_criterion = pm.LogNormal('confidence_criterion',mu=mu_conf_hyper,sigma=sd_conf_hyper,shape=np.size(np.unique(subj_id)))\n",
    "\n",
    "        # rescaled stimulus sensitivity (nt*ns,1)\n",
    "        sm = np.multiply(orislong,stimulus_sensitivity)\n",
    "        # sm = np.multiply(oris,np.broadcast_to(stimulus_sensitivity,oris.shape))\n",
    "        display(sm.shape)\n",
    "        # rescaled confidence criterion (nt*ns,1)\n",
    "        sc = np.multiply(decision_criterion,stimulus_sensitivity)\n",
    "        # difference matrix (nt*ns,1)\n",
    "        se = sm - np.broadcast_to(sc,sm.shape)\n",
    "        display(se.shape)\n",
    "        # (ns,1)\n",
    "        muLogN = np.log(1/np.sqrt(meta_uncertainty**2 + 1.0))\n",
    "        sigmaLogN = np.sqrt(np.log(meta_uncertainty**2 + 1.0))\n",
    "        # transformed xaxis per subject, lognormal inverse CDF (sampn,ns)\n",
    "        # xtrans = np.broadcast_to(muLogN.T,(sampn,ns)) + np.multiply(np.broadcast_to(sigmaLogN.T,(sampn,ns)),\n",
    "        #                                                           np.broadcast_to(ndtri_exp(sampx),(ns,sampn)).T)\n",
    "        xtrans = np.broadcast_to(muLogN,(ns,sampn)) + np.multiply(np.broadcast_to(sigmaLogN,(ns,sampn)),\n",
    "                                                                  np.broadcast_to(ndtri_exp(sampx),(ns,sampn)))\n",
    "        # sigma parameters for likelihood, (ns,sampn)\n",
    "        sds = 1.0/xtrans\n",
    "        display(sds.shape)\n",
    "\n",
    "        llhC = np.zeros((se.shape[0],4))\n",
    "        # matrix of mu values for the noraml cdfs (ns*nt, sampn)\n",
    "        avgs = np.multiply( np.tile(se,sampn).reshape(ns*nt,sampn),\n",
    "                            np.tile(sds,nt).reshape(ns*nt,sampn) )\n",
    "        # np.tile(se[:,0],30).reshape(nt,30).shape\n",
    "        # avgs = np.multiply(np.broadcast_to(se[:,n].T,(nt,sampn)),np.broadcast_to(sds[:,n],(nt,sampn)))\n",
    "        for tr in range(se.shape[0]):\n",
    "            raws = np.zeros((3,sds.shape[1]))\n",
    "            # sampn\n",
    "            raws[1] = avgs[tr] #+ norm.cdf(0)*sds[tr//nt]\n",
    "            for rws in range(sds.shape[1]):\n",
    "                raws[1,rws] = norm.cdf(0,avgs[tr,rws],sds[tr//nt,rws])\n",
    "                raws[0,rws] = norm.cdf(-confidence_criterion,avgs[tr,rws],sds[tr//nt,rws])\n",
    "                raws[2,rws] = norm.cdf(confidence_criterion,avgs[tr,rws],sds[tr//nt,rws])\n",
    "            ratiodist = np.mean(raws,axis=1)\n",
    "            llhC[tr,1] = (guess_rate[subj_id]/4) + (1-guess_rate[subj_id])*ratiodist[1]\n",
    "            llhC[tr,2] = (guess_rate[subj_id]/4) + (1-guess_rate[subj_id])*(ratiodist[2]-ratiodist[1])\n",
    "            llhC[tr,3] = (guess_rate[subj_id]/4) + (1-guess_rate[subj_id])*(ratiodist[3]-ratiodist[2])\n",
    "            llhC[tr,4] = (guess_rate[subj_id]/4) + (1-guess_rate[subj_id])*(1-ratiodist[3])\n",
    "        ll = ll + pm.Bernoulli('ll',logit_p=llhC,observed=resps[subj_id])\n",
    "\n",
    "\n",
    "        # ll = get_CASANDRE_ll(nt,sampn,resps,guess_rate,sds,se,confidence_criterion,subj_id)\n",
    "\n",
    "        # kappa = pm.LogNormal('kappa',mu=mu_kappa_hyper,sigma=sd_kappa_hyper,shape=np.size(np.unique(subj_id)))\n",
    "        # gamma = pm.HalfNormal('gamma',sigma=sd_gamma_hyper,shape=np.size(np.unique(subj_id)))\n",
    "        \n",
    "        # prob = pm.Deterministic('prob', 1 / (1 + pm.math.exp(-gamma[subj_id] * ( (delay_amt**alpha[subj_id])/(1+(kappa[subj_id]*delay_wait)) \n",
    "        #                                                                         - (immed_amt**alpha[subj_id])/(1+(kappa[subj_id]*immed_wait)) ))))\n",
    "        # in MLE, we use bernouli.logpmf as LL\n",
    "        # the logpmf is built into the pm.Bernoulli\n",
    "        # y_1 = pm.Bernoulli('y_1',logit_p=llhC,observed=choices)\n",
    "\n",
    "        trace_prior = pm.sample(200, tune=10, cores=2,target_accept=0.95,progressbar=True)\n",
    "\n",
    "\n",
    "    # This is how you get a nice array. Note that this returns a pandas DataFrame, not a numpy array. Indexing is totally different.\n",
    "    summary= az.summary(trace_prior,round_to=10)\n",
    "    fn = os.path.join(save_dir,'BHM_model_summary_{}.csv'.format(experiment))\n",
    "    print('Saving to : {}'.format(fn))\n",
    "    summary.to_csv(fn)\n",
    "\n",
    "    fn = os.path.join(save_dir,'BHM_model_trace_{}.pkl'.format(experiment))\n",
    "    print('Saving to : {}'.format(fn))\n",
    "    with open(fn,'wb') as buff:\n",
    "        pickle.dump({'trace':trace_prior},buff)\n",
    "        # pm.save_trace(trace_prior,fn)\n",
    "\n",
    "    print('Time to complete {} aggregate BHM : {} minutes'.format(len(subjects),(time.time() - tStep1)/60.0))\n",
    "    return trace_prior,subjects,subj_id\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25600,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(25600,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(128, 30)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'TensorVariable'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m utility_dir,save_dir,data \u001b[39m=\u001b[39m dirs_and_data(root_dir\u001b[39m=\u001b[39mroot_dir,dataset\u001b[39m=\u001b[39mdataset,experiment\u001b[39m=\u001b[39mexperiment)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# subjects,subj_id,gabor_orient,gabor_contrast,run_dimension,trial_resp_keys,trial_resp_rt,choices = extract_cols(data)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m trace_prior,subjects,subj_id \u001b[39m=\u001b[39m run_BHM(save_dir,experiment,data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# display(data.shape)\u001b[39;00m\n",
      "\u001b[1;32m/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X22sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m raws \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39m3\u001b[39m,sds\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X22sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m \u001b[39m# sampn\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X22sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m raws[\u001b[39m1\u001b[39;49m] \u001b[39m=\u001b[39m avgs[tr] \u001b[39m#+ norm.cdf(0)*sds[tr//nt]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X22sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39mfor\u001b[39;00m rws \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(sds\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X22sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m     raws[\u001b[39m1\u001b[39m,rws] \u001b[39m=\u001b[39m norm\u001b[39m.\u001b[39mcdf(\u001b[39m0\u001b[39m,avgs[tr,rws],sds[tr\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mnt,rws])\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Set the paths needed to find the data and load\n",
    "\n",
    "# The user of the script can edit root_dir and dataset variables to get it to work for their dataset.\n",
    "# This script will work given the data is stored in the appropriate BIDS format in the split directory\n",
    "root_dir = '/Volumes/UCDN/datasets/'\n",
    "dataset = 'IDM'\n",
    "\n",
    "# log the BHM experiment version, in case we cahnge it in the future\n",
    "\n",
    "\n",
    "for experiment in ['low_vol_low_risk']:#,'low_vol_high_risk']:\n",
    "\n",
    "    utility_dir,save_dir,data = dirs_and_data(root_dir=root_dir,dataset=dataset,experiment=experiment)\n",
    "    # subjects,subj_id,gabor_orient,gabor_contrast,run_dimension,trial_resp_keys,trial_resp_rt,choices = extract_cols(data)\n",
    "    trace_prior,subjects,subj_id = run_BHM(save_dir,experiment,data)\n",
    "\n",
    "    # display(data.shape)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10//10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.0"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25600/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [1, 2, 3, 4],\n",
       "       [1, 2, 3, 4]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a= np.array([1,2,3,4])\n",
    "display(a)\n",
    "b = np.broadcast_to(a,(3,4))\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "trace_prior,subjects,subj_id = run_BHM(save_dir,experiment,data)\n",
    "df_bhm = extract_mean(save_dir=save_dir,var_names=['kappa','gamma'],\n",
    "                        subjects=subjects,experiment=experiment)\n",
    "for s in set(subj_id):\n",
    "    coords={'kappa_dim_0': [s],'gamma_dim_0':[s]}\n",
    "    diganostic_plots(trace_prior,experiment=experiment,utility_dir=utility_dir,subject=subjects[s],\n",
    "                        coords=coords,var_names=['kappa','gamma'],figsize=(10,10))\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Takes about 10 seconds\n",
    "\n",
    "# # we will change this when we change utility to 1st level analysis (or split)\n",
    "# split_dir = '/Volumes/UCDN/datasets/IDM/split/'\n",
    "# utility_dir = '/Volumes/UCDN/datasets/IDM/utility/'\n",
    "# save_dir = '/Volumes/UCDN/datasets/IDM/utility/BHM/cpdm/'\n",
    "# subjs = sorted(glob.glob(os.path.join(split_dir,'23_IDM_*')))\n",
    "# task = 'cpdm'\n",
    "# cols = ['cpdm_choice','cpdm_gabor_orient', 'cpdm_gabor_contrast', 'cpdm_run_dimension', 'cpdm_trial_resp.keys','cpdm_trial_resp.rt']\n",
    "# data = pd.DataFrame(columns=['subject']+cols)\n",
    "\n",
    "# for subj_id,s in enumerate(subjs):\n",
    "#     subject = os.path.basename(s)\n",
    "#     fn  = os.path.join(s,task,'{}_{}.csv'.format(os.path.basename(s),task))\n",
    "#     if os.path.exists(fn):\n",
    "#         subj_data = read_load_data(subject=subject,fn=fn,cols=cols)\n",
    "#         # for c in cols:\n",
    "#             # subj_data[c] = subj_data[c].astype(float)        \n",
    "#         subj_data['subject_id'] = int(subj_id)\n",
    "#         data = pd.concat([data,subj_data],ignore_index=True)\n",
    "\n",
    "# data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# parameters to model\n",
    "# nParams = 2 + numTasks + totRel + numTasks*nConfCrit; % [Guess rate, stimulus criterion], [meta-uncertainty], [stimulus sensitivity], [confidence criteria]\n",
    "\n",
    "\n",
    "# Required order for getLlhChoice: [guess rate, stim sens, stim crit, meta-uncertainty, conf criteria]\n",
    "\n",
    "\n",
    "# 2 :: [Guess rate, stimulus criterion]\n",
    "# numTasks :: [meta-uncertainty]\n",
    "# totRel :: [stimulus sensitivity]\n",
    "# numTasks*nConfCrit :: [confidence criteria]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorConstant{(1,) of 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(200, 128)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorConstant{(1,) of 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorConstant{(1,) of 128}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subjects,subj_id,gabor_orient,gabor_contrast,run_dimension,trial_resp_keys,trial_resp_rt,choices = extract_cols(data)\n",
    "# number of samples/subjects\n",
    "ns = np.unique(subj_id).shape[0]\n",
    "# number of trials\n",
    "nt = get_nt(data)\n",
    "\n",
    "orislong = gabor_orient\n",
    "respslong = get_respslong(choices)\n",
    "resps,oris = chop_by_subject(respslong,orislong,nt=200,ns=ns)\n",
    "\n",
    "with pm.Model() as model_simple:\n",
    "    # Hyperparameters for stimulus sensitivity (sens), decision criterion (deci), \n",
    "    # meta-uncertainty (meta), and confidence criterion (conf)\n",
    "    mu_sens_hyper = pm.Normal('mu_sens_hyper',mu=0,sigma=1.0)\n",
    "    sd_sens_hyper = pm.LogNormal('sd_sens_hyper',mu=0,sigma=1.0)\n",
    "    mu_deci_hyper = pm.Normal('mu_deci_hyper',mu=0,sigma=1.0)\n",
    "    sd_deci_hyper = pm.LogNormal('sd_deci_hyper',mu=0,sigma=1.0)\n",
    "    mu_meta_hyper = pm.Normal('mu_meta_hyper',mu=0,sigma=1.0)\n",
    "    sd_meta_hyper = pm.LogNormal('sd_meta_hyper',mu=0,sigma=1.0)\n",
    "    mu_conf_hyper = pm.Normal('mu_conf_hyper',mu=0,sigma=1.0)\n",
    "    sd_conf_hyper = pm.LogNormal('sd_conf_hyper',mu=0,sigma=1.0)\n",
    "\n",
    "    # Priors\n",
    "    # where does Corey get these values? mu, sigma or alpha beta?\n",
    "    guess_rate = pm.Beta('guess_rate',mu=1,sigma=193.0/3.0)\n",
    "    stimulus_sensitivity = pm.LogNormal('stimulus_sensitivity',mu=mu_sens_hyper,sigma=sd_sens_hyper,shape=np.size(np.unique(subj_id)))\n",
    "    decision_criterion = pm.Normal('decision_criterion',mu=mu_deci_hyper,sigma=sd_deci_hyper,shape=np.size(np.unique(subj_id)))\n",
    "    meta_uncertainty = pm.LogNormal('meta_uncertainty',mu=mu_meta_hyper,sigma=sd_meta_hyper,shape=np.size(np.unique(subj_id)))\n",
    "    confidence_criterion = pm.LogNormal('confidence_criterion',mu=mu_conf_hyper,sigma=sd_conf_hyper,shape=np.size(np.unique(subj_id)))\n",
    "    \n",
    "    # rescaled stimulus sensitivity (nt,ns)\n",
    "    sm = np.multiply(oris,np.broadcast_to(stimulus_sensitivity,oris.shape))\n",
    "    display(sm.shape)\n",
    "    \n",
    "    # rescaled confidence criterion (ns,1)\n",
    "    sc = np.multiply(decision_criterion,stimulus_sensitivity)\n",
    "    display(sc.shape)\n",
    "    # difference matrix (nt,ns)\n",
    "    se = sm - np.broadcast_to(sc,sm.shape)\n",
    "    display(se.shape)\n",
    "    \n",
    "\n",
    "    # xtrans = logncdfinv(sampx,log(1/sqrt(meta.^2+1)),sqrt(log(meta.^2+1)));\n",
    "    # \n",
    "    muLogN = np.log(1/np.sqrt(meta_uncertainty**2 + 1.0))\n",
    "    display(muLogN.shape)\n",
    "    sigmaLogN = np.sqrt(np.log(meta_uncertainty**2 + 1.0))\n",
    "    display(sigmaLogN.shape)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 25600)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respslong.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 30)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(se[:,0],30).reshape(nt,30).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0,\n",
       "       Elemwise{sub,no_inplace}.0, Elemwise{sub,no_inplace}.0],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with remapped shapes [original->remapped]: (200,)  and requested shape (200,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m n\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m display(np\u001b[39m.\u001b[39marray(se[:,n]\u001b[39m.\u001b[39mT))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pizarror/IDM/BH_model/cpdm/hierarchical_model.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m np\u001b[39m.\u001b[39;49mbroadcast_to(se[:,n]\u001b[39m.\u001b[39;49mT,(nt,\u001b[39m1\u001b[39;49m))\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mbroadcast_to\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/idm_jupy/lib/python3.9/site-packages/numpy/lib/stride_tricks.py:413\u001b[0m, in \u001b[0;36mbroadcast_to\u001b[0;34m(array, shape, subok)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_broadcast_to_dispatcher, module\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbroadcast_to\u001b[39m(array, shape, subok\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    369\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Broadcast an array to a new shape.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \n\u001b[1;32m    371\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[39m           [1, 2, 3]])\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     \u001b[39mreturn\u001b[39;00m _broadcast_to(array, shape, subok\u001b[39m=\u001b[39;49msubok, readonly\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/idm_jupy/lib/python3.9/site-packages/numpy/lib/stride_tricks.py:349\u001b[0m, in \u001b[0;36m_broadcast_to\u001b[0;34m(array, shape, subok, readonly)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mall elements of broadcast shape must be non-\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    347\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mnegative\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    348\u001b[0m extras \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 349\u001b[0m it \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mnditer(\n\u001b[1;32m    350\u001b[0m     (array,), flags\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mmulti_index\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrefs_ok\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mzerosize_ok\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m+\u001b[39;49m extras,\n\u001b[1;32m    351\u001b[0m     op_flags\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mreadonly\u001b[39;49m\u001b[39m'\u001b[39;49m], itershape\u001b[39m=\u001b[39;49mshape, order\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    352\u001b[0m \u001b[39mwith\u001b[39;00m it:\n\u001b[1;32m    353\u001b[0m     \u001b[39m# never really has writebackifcopy semantics\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     broadcast \u001b[39m=\u001b[39m it\u001b[39m.\u001b[39mitviews[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with remapped shapes [original->remapped]: (200,)  and requested shape (200,1)"
     ]
    }
   ],
   "source": [
    "n=2\n",
    "display(np.array(se[:,n].T))\n",
    "np.broadcast_to(se[:,n].T,(nt,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09d5eda77765311109b2c027e144dcf58f89dd96008cdf29c2e6b03e99df71a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

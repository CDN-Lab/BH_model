{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy as sp\n",
    "from scipy import optimize\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd \n",
    "import glob\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import statistics as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pizarror/IDM\n"
     ]
    }
   ],
   "source": [
    "# getting the name of the directory\n",
    "# where the this file is present.\n",
    "# current = os.path.dirname(os.path.realpath(__file__))\n",
    "current = os.path.dirname(os.getcwd())\n",
    " \n",
    "# Getting the parent directory name\n",
    "# where the current directory is present.\n",
    "# parent = os.path.dirname(os.path.dirname(current))\n",
    "parent = current\n",
    "print(parent)\n",
    "#/Users/pizarror/IDM\n",
    "\n",
    "# adding the parent directory to\n",
    "# the sys.path.\n",
    "sys.path.append(parent)\n",
    "\n",
    "from IDM_model.src import model_functions as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_load_data(subject='23_IDM_0144',fn='/tmp',alpha0=1.0,cols=[]):\n",
    "    df = pd.read_csv(fn)\n",
    "    # remove practice trials\n",
    "    df = df.loc[df['cdd_trial_type']=='task']\n",
    "    # insert probability as choice into data\n",
    "    # cols = ['cdd_trial_resp.corr','cdd_immed_amt','cdd_delay_amt','cdd_immed_wait','cdd_delay_wait','alpha']\n",
    "    # also returns percent_reward which we do not need here\n",
    "    data = mf.get_data(df,cols,alpha_hat=alpha0)[0]\n",
    "    data['subject'] = subject\n",
    "    # vn = data['cdd_immed_amt'].to_list()\n",
    "    # vr = data['cdd_delay_amt'].to_list()\n",
    "    # tn = data['cdd_immed_wait'].to_list()\n",
    "    # tr = data['cdd_delay_wait'].to_list()\n",
    "    # choice = data['cdd_trial_resp.corr'].to_list()\n",
    "    return data # vn,vr,tn,tr,choice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Pooling\n",
    "\n",
    "Complete pooling ignores the group-level information and considers all data as belonging to the same category. All groups are described with the same model. \n",
    "\n",
    "We are using complete pooling to generate priors for when we implement a higherarchical bayesian model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "We will load the data from all participants so we can run the modeling schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           subject  cdd_choice  cdd_immed_amt  cdd_delay_amt  cdd_immed_wait   \n",
      "0      23_IDM_0001         1.0            5.0           26.0             0.0  \\\n",
      "1      23_IDM_0001         1.0            2.0           22.0             0.0   \n",
      "2      23_IDM_0001         0.0            2.0            6.0             0.0   \n",
      "3      23_IDM_0001         0.0           15.0           15.0             0.0   \n",
      "4      23_IDM_0001         1.0           15.0           55.0             0.0   \n",
      "...            ...         ...            ...            ...             ...   \n",
      "14222  23_IDM_0149         1.0            2.0           12.0             0.0   \n",
      "14223  23_IDM_0149         0.0           20.0           24.0             0.0   \n",
      "14224  23_IDM_0149         0.0            2.0           41.0             0.0   \n",
      "14225  23_IDM_0149         1.0           10.0           65.0             0.0   \n",
      "14226  23_IDM_0149         0.0           20.0           25.0             0.0   \n",
      "\n",
      "       cdd_delay_wait  alpha  \n",
      "0                29.0    1.0  \n",
      "1                90.0    1.0  \n",
      "2                90.0    1.0  \n",
      "3               151.0    1.0  \n",
      "4                90.0    1.0  \n",
      "...               ...    ...  \n",
      "14222            11.0    1.0  \n",
      "14223           150.0    1.0  \n",
      "14224           149.0    1.0  \n",
      "14225             4.0    1.0  \n",
      "14226            59.0    1.0  \n",
      "\n",
      "[14227 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Takes about 16 seconds\n",
    "\n",
    "# we will change this when we change utility to 1st level analysis (or split)\n",
    "split_dir = '/Volumes/UCDN/datasets/IDM/split/'\n",
    "save_dir = '/Volumes/UCDN/datasets/IDM/BH/csv'\n",
    "subjs = sorted(glob.glob(os.path.join(split_dir,'23_IDM_*')))\n",
    "task = 'cdd'\n",
    "# cols = ['cdd_trial_resp.corr','cdd_immed_amt','cdd_delay_amt','cdd_immed_wait','cdd_delay_wait','alpha']\n",
    "cols = ['cdd_choice','cdd_immed_amt','cdd_delay_amt','cdd_immed_wait','cdd_delay_wait','alpha']\n",
    "data = pd.DataFrame(columns=['subject']+cols)\n",
    "\n",
    "for s in subjs:\n",
    "    subject = os.path.basename(s)\n",
    "    fn  = os.path.join(s,task,'{}_{}.csv'.format(os.path.basename(s),task))\n",
    "    if os.path.exists(fn):\n",
    "        # SSA (Ben's smaller sooner amount) is vn (value_null)\n",
    "        # LLA (Ben's larger later amount) is vr (value_reward)\n",
    "        # SSD (Ben's smaller sooner delay) is tn (time_null)\n",
    "        # LLD (Ben's larger later delay) is tr (time_reward)\n",
    "        # y_pool (Ben's) is choice \n",
    "        # vn,vr,tn,tr,choice = read_load_data(fn=fn,alpha0=1.0)\n",
    "        subj_data = read_load_data(subject=subject,fn=fn,alpha0=1.0,cols=cols)\n",
    "        data = pd.concat([data,subj_data],ignore_index=True)\n",
    "        # VNa = VNa + vn\n",
    "        # VRa = VRa + vr\n",
    "        # TNa = TNa + tn\n",
    "        # TRa = TRa + tr\n",
    "        # Cha = Cha + choice\n",
    "    else:\n",
    "        print('Could not find : {}'.format(fn))\n",
    "\n",
    "print(data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate pooled model\n",
    "\n",
    "Now we can run this completely model to get a prior for when we run the next round of BH model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cdd_trial_resp.corr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/idm_jupy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/idm_jupy/lib/python3.9/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/idm_jupy/lib/python3.9/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cdd_trial_resp.corr'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m     gamma \u001b[39m=\u001b[39m pm\u001b[39m.\u001b[39mHalfNormal(\u001b[39m'\u001b[39m\u001b[39mgamma\u001b[39m\u001b[39m'\u001b[39m,sigma\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m,shape\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m#np.size(SSA)) #mu=0.1, mu=0.005,sd=0.01 mu=0,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     prob \u001b[39m=\u001b[39m pm\u001b[39m.\u001b[39mDeterministic(\u001b[39m'\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m pm\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mgamma \u001b[39m*\u001b[39m ( data[\u001b[39m'\u001b[39m\u001b[39mcdd_delay_amt\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\u001b[39m/\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m+\u001b[39m(kappa\u001b[39m*\u001b[39mdata[\u001b[39m'\u001b[39m\u001b[39mcdd_delay_wait\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues)) \u001b[39m-\u001b[39m \n\u001b[1;32m     11\u001b[0m                                                                    data[\u001b[39m'\u001b[39m\u001b[39mcdd_immed_amt\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\u001b[39m/\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m+\u001b[39m(kappa\u001b[39m*\u001b[39mdata[\u001b[39m'\u001b[39m\u001b[39mcdd_immed_wait\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues)) ))))\n\u001b[0;32m---> 13\u001b[0m     y_1 \u001b[39m=\u001b[39m pm\u001b[39m.\u001b[39mBernoulli(\u001b[39m'\u001b[39m\u001b[39my_1\u001b[39m\u001b[39m'\u001b[39m,p\u001b[39m=\u001b[39mprob,observed\u001b[39m=\u001b[39mdata[\u001b[39m'\u001b[39;49m\u001b[39mcdd_trial_resp.corr\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     15\u001b[0m     trace_pool \u001b[39m=\u001b[39m pm\u001b[39m.\u001b[39msample(\u001b[39m10000\u001b[39m, tune\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m, cores\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,target_accept\u001b[39m=\u001b[39m\u001b[39m0.99\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[39m# Call the trace whatever you like. This just saves it. You don't want to run a whole model and then accidentally x-out your window or refresh or something and lose it all!\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/idm_jupy/lib/python3.9/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/idm_jupy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cdd_trial_resp.corr'"
     ]
    }
   ],
   "source": [
    "# This is a pool model. This is not good for our case. It will just get one k and one b for each session, but it won't incorporate everyone's data. Returns wonky values as far as I remember.\n",
    "\n",
    "# takes approximately 10 minutes\n",
    "\n",
    "with pm.Model() as model_pool:\n",
    "\n",
    "    kappa = pm.Beta('kappa',mu=0.3,sigma=0.2,shape=1)#np.size(SSA)) #mu=0.1, mu=0.07,sd=0.1 05\n",
    "    gamma = pm.HalfNormal('gamma',sigma=0.5,shape=1)#np.size(SSA)) #mu=0.1, mu=0.005,sd=0.01 mu=0,\n",
    "    \n",
    "    prob = pm.Deterministic('prob', 1 / (1 + pm.math.exp(-gamma * ( data['cdd_delay_amt'].values/(1+(kappa*data['cdd_delay_wait'].values)) - \n",
    "                                                                   data['cdd_immed_amt'].values/(1+(kappa*data['cdd_immed_wait'].values)) ))))\n",
    "\n",
    "    y_1 = pm.Bernoulli('y_1',p=prob,observed=data['cdd_choice'])\n",
    "\n",
    "    trace_pool = pm.sample(10000, tune=10000, cores=4,target_accept=0.99)\n",
    "\n",
    "\n",
    "# Call the trace whatever you like. This just saves it. You don't want to run a whole model and then accidentally x-out your window or refresh or something and lose it all!\n",
    "az.plot_trace(trace_pool, var_names=[\"kappa\",\"gamma\"],compact=False)\n",
    "# This is how you get a nice array. Note that this returns a pandas DataFrame, not a numpy array. Indexing is totally different.\n",
    "Summary= az.summary(trace_pool,round_to=10)\n",
    "# Again, call it what you want (yeah - call it what you want tooooo)\n",
    "fn = os.path.join(save_dir,\"completely_pooled_model.csv\")\n",
    "Summary.to_csv(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa, use the following (mu,sigma) : (0.0199494192,0.0004967748)\n",
      "For gamma, use the following (mu,sigma) : (0.1290928433,0.0027119263)\n"
     ]
    }
   ],
   "source": [
    "fn = os.path.join(save_dir,'completely_pooled_model.csv')\n",
    "pool_model = pd.read_csv(fn,index_col=0)\n",
    "\n",
    "mu_kappa,std_kappa = pool_model.loc['kappa[0]','mean'],pool_model.loc['kappa[0]','sd']\n",
    "mu_gamma,std_gamma = pool_model.loc['gamma[0]','mean'],pool_model.loc['gamma[0]','sd']\n",
    "\n",
    "print('For kappa, use the following (mu,sigma) : ({},{})'.format(mu_kappa, std_kappa))\n",
    "print('For gamma, use the following (mu,sigma) : ({},{})'.format(mu_gamma, std_gamma))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09d5eda77765311109b2c027e144dcf58f89dd96008cdf29c2e6b03e99df71a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy as sp\n",
    "from scipy import optimize\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------------------------------------------------##\n",
    "##                             Executor                             ##\n",
    "##  This code loops through the files and applies the full set of   ##\n",
    "##  functions to each. It takes the output, compiles it into        ##\n",
    "##  tables, and exports the full tables to csv files.               ##\n",
    "##                                                                  ##\n",
    "##  Exports: Model Information Table: a table of b, k, etc. for     ##  \n",
    "##               ID/day. This does not contain SV information.      ##   \n",
    "##           Full Subjective Value Table: a table of calculated     ##\n",
    "##               SVs, model parameters, etc. etc. This contains     ##\n",
    "##               all of the information provided by these           ##\n",
    "##               functions.                                         ##\n",
    "##                                                                  ##\n",
    "##  Please see the functions for more details on what each one      ##\n",
    "##  does!                                                           ##\n",
    "##------------------------------------------------------------------##\n",
    "\n",
    "# Titles for \"Model Information Table\"\n",
    "bigarray = np.array([\"ID\",\"Day\",\"File Name\", \"k\", \"beta\", \"LL\", \"AIC\", \"BIC\", \"r2\", \"Percent correct - model predicting choices\",\"Percent correct answer on catch trials\"])  #, \"g_LL\", \"g_AIC\", \"g_BIC\",\"g_r2\",\"g_Percent correct\"])\n",
    "\n",
    "# Titles for \"Full Subjective Value Table\"\n",
    "largeSVarray = np.array([\"Trial Number\",\"Stimulus Time\", \"Response Time\",\"SS amount\",\"LL amount\",\"SS delay\",\"LL delay\",\"Response\",\"SS SV\",\"LL SV\",\"k\",\"beta\",\"ID\",\"Day\",\"Date\",\"Time\",\"LL\",\"AIC\",\"BIC\",\"r2\",\"Percent correct - model predicting choices\",\"Percent correct answer on catch trials\"])\n",
    "\n",
    "# Make the title array horizontal. This works (with np.append below), but I'm not sure if I tried the seemingly simpler vstack option that I'm using for bigarray. This works fine regardless.\n",
    "largeSVarray.shape = (1,largeSVarray.size)\n",
    "\n",
    "# \"files\" is established in some code below all of the functions. Run that first!\n",
    "for filename in files:\n",
    "    k,beta,LL,LL0,AIC,BIC,r2,correct,modelchart,p_d0s,p_i1000s,p_overall = run_functions(filename)  #given_k,,g_LL,g_LL0,g_AIC,g_BIC,g_r2,g_correct  <- use this if comparing to a given k. See outdated function at the bottom of the file.\n",
    "\n",
    "    # IDs (my randomized ones) and days (1, 2, sometimes 3) can be found in the file names at these points.\n",
    "    ID_for_results = filename[34:38]\n",
    "    day_for_results = filename[39:40]\n",
    "\n",
    "    # This is used for Model Information Table - one line per file.\n",
    "    resultarray = np.array([ID_for_results,day_for_results,filename,k,beta,LL,AIC,BIC,r2,correct,p_overall])  #,g_LL,g_AIC,g_BIC,g_r2,g_correct   <- use this if comparing to a given k. See outdated function at the bottom of the file. #p_d0s,p_i1000s, <- use these if you want to get more specific with people's catch trial failures.\n",
    "    \n",
    "    # In both of the below lines, I'm adding the new content arrays to the bottom of the title arrays. This feels like an easy way to iteratively add everything.\n",
    "    bigarray = np.vstack((bigarray,resultarray))\n",
    "\n",
    "    # This is used for the Full Subjective Value Table. The indexing removes the title line on the modelchart.\n",
    "    largeSVarray = np.append(largeSVarray,modelchart[1:,:], axis=0)\n",
    "\n",
    "# I use the pandas to_csv function, which requires converting the numpy arrays into DataFrames. This has worked best for me so far. I'm sure it's technically inefficient, but this whole code runs in around 3 seconds, so I think it's fine.\n",
    "pd.DataFrame(bigarray).to_csv(\"tests\\Model Information Table %100.csv\", header=False, index=False)\n",
    "pd.DataFrame(largeSVarray).to_csv(\"tests\\Full Subjective Value Table %100.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------------------------------------------------##\n",
    "##                          Run Functions                           ##\n",
    "##  This code just runs all of the main functions on one individual ##\n",
    "##  file. The code above will loop through the files and call this  ##\n",
    "##  on each one, constructing the output tables from the arrays     ##\n",
    "##  that this function returns.                                     ##\n",
    "##                                                                  ##\n",
    "##  Inputs: filename: name of file to be read                       ##\n",
    "##                                                                  ##\n",
    "##  Outputs: k: the optimized k (kappa), the discount parameter.    ##\n",
    "##           beta: the optimized b, the stochasticity=of-choice     ##\n",
    "##                factor.                                           ##\n",
    "##           LL: log likelihood (high is good)                      ##\n",
    "##           LL0: log likelihood of a model that just coin flips    ##\n",
    "##                every choice guess (probability of LL = 0.5). The ##\n",
    "##                analysis is mostly judging LL vs LL0.             ##\n",
    "##           AIC: Akaike Information Criterion (high is bad)        ##\n",
    "##           BIC: Bayesian Information Criterion (high is bad)      ##\n",
    "##           r2: r squared                                          ##\n",
    "##           correct: the percentage of choices that the model      ##\n",
    "##                accurately predicts based on k                    ##\n",
    "##           modelchart: the output of write_SVs(). This is a huge  ##\n",
    "##                table of information - check out the write_SVs()  ##\n",
    "##                documentation for more information.               ##\n",
    "##           percent_of_dzeros: the percent of correct answers in   ##\n",
    "##              catch trials in which LL delay is zero.             ##\n",
    "##           percent_of_imm1000s: the percent of correct answers in ##\n",
    "##              catch trials in which LL delay is zero.             ##\n",
    "##           overall_percent: the overall percent of correct        ##\n",
    "##              answers in all catch trials.                        ##\n",
    "##                                                                  ##\n",
    "##  Note: modelchart comes with a header. We'll remove that when    ##\n",
    "##        it's compiled into a big table.                           ##\n",
    "##                                                                  ##\n",
    "##  Note: Currently, the only catch trial accuracy measure used is  ##\n",
    "##        the overall_percent. I wrote it to be able to provide     ##\n",
    "##        more specific output, but I don't think that level of     ##\n",
    "##        detail is necessary in the output tables. In the main     ##\n",
    "##        code above, I accept but never use the first two values.  ##\n",
    "##        If you want to analyze them, you can use them there.      ##\n",
    "##------------------------------------------------------------------##\n",
    "\n",
    "def run_functions(filename):\n",
    "    #import data\n",
    "    immediate_vals, immediate_times, delay_vals, delay_times, choices, p_immediate_vals, p_immediate_times, p_delay_vals, p_delay_times, p_choices = import_data(filename)\n",
    "\n",
    "    #check attention\n",
    "    percent_of_dzeros, percent_of_imm1000s, overall_percent = checkAttention(p_immediate_vals, p_delay_times, p_choices)\n",
    "\n",
    "    #optimize k and beta\n",
    "    negLL, beta, k, SS_V, SS_D, LL_V, LL_D, risk = optimizer(immediate_vals, immediate_times, delay_vals, delay_times, choices)\n",
    "\n",
    "    #run primary analysis\n",
    "    LL,LL0,AIC,BIC,r2,correct = analysis(negLL,choices,SS_V,SS_D,LL_V,LL_D,risk,k,beta)\n",
    "\n",
    "    #get SVs\n",
    "    modelchart = write_SVs(filename,k,beta,LL,AIC,BIC,r2,correct,risk,overall_percent)\n",
    "\n",
    "    #use given k value\n",
    "    #negLL = LLfromGiven(given_k, given_b, choices, SS_V, SS_D, LL_V, LL_D, risk)\n",
    "\n",
    "    #run alternative analysis\n",
    "    #g_LL,g_LL0,g_AIC,g_BIC,g_r2,g_correct = analysis(negLL,choices,SS_V,SS_D,LL_V,LL_D,risk,given_k,beta)\n",
    "\n",
    "    return k,beta,LL,LL0,AIC,BIC,r2,correct,modelchart,percent_of_dzeros, percent_of_imm1000s, overall_percent   #given_k,,g_LL,g_LL0,g_AIC,g_BIC,g_r2,g_correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------------------------------------------------##\n",
    "##                           Import Data                            ##\n",
    "##  This code imports data from the cleaned \"FixedFixed\" files.     ##\n",
    "##  This function is called one-by-one on these files.              ##\n",
    "##                                                                  ##\n",
    "##  Inputs: filename: name of file to be read                       ##\n",
    "##                                                                  ##\n",
    "##  Outputs: Lists (length=number of trials):                       ##\n",
    "##               immediate_vals: the array of SS values             ##\n",
    "##               immediate_times: the array of SS delays            ##\n",
    "##               delay_vals: the array of LL values                 ##\n",
    "##               delay_times: the array of LL delays                ##\n",
    "##               choices: the array of choices (1=delay, 0=imm.)    ##\n",
    "##               p_immediate_vals: above but only catch trials      ##\n",
    "##               p_immediate_times: above but only catch trials     ##\n",
    "##               p_delay_vals: above but only catch trials          ##\n",
    "##               p_delay_times: above but only catch trials         ##\n",
    "##               p_choices: above but only catch trials             ##\n",
    "##------------------------------------------------------------------##\n",
    "\n",
    "def import_data(filename):\n",
    "    \n",
    "    # This is a numpy function, so it creates a numpy array. Note: if ever importing data that aren't already cleaned (write_SVs() below does this), you'll have to do more here. The commas in \"$1,000\" interfere with a direct delimiter-based strategy like this one. It's ok in this context, but be careful if using this code on other files.\n",
    "    data = genfromtxt(filename, delimiter=',')\n",
    "\n",
    "    immediate_vals = data[1:,1]\n",
    "    immediate_times = data[1:,3]\n",
    "    delay_vals = data[1:,2]\n",
    "    delay_times = data[1:,4]\n",
    "    choices = data[1:,5]\n",
    "\n",
    "    # This will select only the non-catch trials.\n",
    "    boolselector = (immediate_vals<1000)*(delay_times>0)\n",
    "\n",
    "    # This will select the catch trials.\n",
    "    catchtrials = np.logical_or((immediate_vals==1000),(delay_times==0))\n",
    "\n",
    "    p_immediate_vals = immediate_vals[catchtrials]\n",
    "    p_immediate_times = immediate_times[catchtrials]\n",
    "    p_delay_vals = delay_vals[catchtrials]\n",
    "    p_delay_times = delay_times[catchtrials]\n",
    "    p_choices = choices[catchtrials]\n",
    "\n",
    "    # You might also test scaling down all of the values by some factor. The k gets scaled up by the same amount. I think this might improve model performance when beta gets large.\n",
    "    testfactor = 1   \n",
    "\n",
    "    immediate_vals = immediate_vals[boolselector]/testfactor\n",
    "    immediate_times = immediate_times[boolselector]/testfactor\n",
    "    delay_vals = delay_vals[boolselector]/testfactor\n",
    "    delay_times = delay_times[boolselector]/testfactor\n",
    "    choices = choices[boolselector]\n",
    "\n",
    "    return immediate_vals, immediate_times, delay_vals, delay_times, choices, p_immediate_vals, p_immediate_times, p_delay_vals, p_delay_times, p_choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------------------------------------------------##\n",
    "##                            Optimizer                             ##\n",
    "##  This code runs the optimization function that calls most of the ##\n",
    "##  other functions. It relies on sp.optimize.minimize, which seems ##\n",
    "##  to work very well in this case. You can specify the method that ##\n",
    "##  you want (e.g., \"Nelder-Mead\"), but the default has             ##\n",
    "##  outperformed these options as far as I've seen. We can pass in  ##\n",
    "##  bounds on the parameters using this method, which is not true   ##\n",
    "##  of every optimization protocol (and seems necessary). To        ##\n",
    "##  actuallly run the optimizer, you must have a function that      ##\n",
    "##  takes exactly two inputs - the parameter (or array of           ##\n",
    "##  parameters) and an array of the other necessary inputs into     ##\n",
    "##  that function. In this case, we pass in optimize_me(), an       ##\n",
    "##  array of beta and k, and an array of the other things           ##\n",
    "##  necessary - the choices, values, delays, etc. The magic machine ##\n",
    "##  does its thing and we end up with 'results', which is a         ##\n",
    "##  complicated array but which has everything we need. results.x   ##\n",
    "##  contains our optimized parameters in the same shape as our      ##\n",
    "##  input array. results.fun gives the thing that was optimized -   ##\n",
    "##  in this case, the negLL.                                        ##\n",
    "##                                                                  ##\n",
    "##                                                                  ##\n",
    "##  Inputs: Lists (length=number of trials):                        ##\n",
    "##              immediate_vals: the array of SS values              ##\n",
    "##              immediate_times: the array of SS delays             ##\n",
    "##              delay_vals: the array of LL values                  ##\n",
    "##              delay_times: the array of LL delays                 ##\n",
    "##              choices: the array of choices (1=delay, 0=imm.)     ##\n",
    "##                                                                  ##\n",
    "##  Outputs: negLL: the negLL of the final optimized set of         ##\n",
    "##               parameters. This will be used a lot in the         ##\n",
    "##               analysis section below.                            ##\n",
    "##           beta: the optimized b, the stochasticity=of-choice     ##\n",
    "##               factor.                                            ##\n",
    "##           k: the optimized k (kappa), the discount parameter.    ##\n",
    "##           SS_V: the array of values of the immediate options.    ##\n",
    "##           SS_D: the array of delays of the immediate options.    ##\n",
    "##           LL_V: the array of values of the delay options.        ##\n",
    "##           LL_D: the array of delays of the delay options.        ##\n",
    "##           risk: not currently in use; more information below.    ##\n",
    "##------------------------------------------------------------------##\n",
    "\n",
    "def optimizer(immediate_vals, immediate_times, delay_vals, delay_times, choices):\n",
    "    # We do start the optimizer off with the guesses below, but those aren't updated like Bayesian priors. They are simply a starting point in parameter space for the optimizer. Changes here could be an avenue to explore when seeking to improve performance.\n",
    "    guesses = [0.005, 0.01]\n",
    "\n",
    "    # These are the bounds on k and beta. The first tuple corresponds to beta, the second to kappa.\n",
    "    bkbounds = ((0,8),(0.00000001,6.4))\n",
    "    risk = 1\n",
    "    SS_V = immediate_vals.tolist()  \n",
    "    SS_D = immediate_times.tolist()\n",
    "    LL_V = delay_vals.tolist()\n",
    "    LL_D = delay_times.tolist()\n",
    "\n",
    "    # These are the inputs of the local_negLL function. They'll be passed through optimize_me()\n",
    "    inputs = [choices,SS_V,SS_D,LL_V,LL_D,risk]\n",
    "\n",
    "    # If seeking to improve performance, could change optimization method, could change maxiter(ations), or could fiddle with other things. You might be able to change the distance between steps in the optimzation.\n",
    "    results = sp.optimize.minimize(optimize_me,guesses,inputs, bounds = bkbounds, tol=None, callback = None, options={'maxiter':10000, 'disp': False})\n",
    "    negLL = results.fun\n",
    "    beta = results.x[0]\n",
    "    k = results.x[1]\n",
    "    \n",
    "    return negLL, beta, k, SS_V, SS_D, LL_V, LL_D, risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess performance on catch trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------------------------------------------------##\n",
    "##                          Check Attention                         ##\n",
    "##  This code determines the percentage of catch trials that a      ##\n",
    "##  participant got right. These can be instances in which the      ##\n",
    "##  delay for LL was 0 or the value of SS was $1,000. In both       ##\n",
    "##  cases, this is $1,000 immediately, which is  strictly the       ##\n",
    "##  optimal choice given basic assumptions about the persons's      ##\n",
    "##  priorities and environment.                                     ##\n",
    "##                                                                  ##\n",
    "##  Inputs: p_immediate_vals: the SS values in choices              ##\n",
    "##              identified as catch trials (in import_data()). I    ##\n",
    "##              used \"p_\" because I originally referred to these as ##\n",
    "##              \"prank trials\" before I knew the real name. Change  ##\n",
    "##              to \"c_\" or \"catch_\" or whatever if you want it to   ##\n",
    "##              be more accurately descriptive, but make sure to    ##\n",
    "##              change it everywhere.                               ## \n",
    "##              optimization                                        ##\n",
    "##          p_delay_times: the LL delay times in choices identified ##\n",
    "##              as catch trials.                                    ##\n",
    "##          p_choices: the selected options in choices identified   ##\n",
    "##              as catch trials.                                    ##\n",
    "##                                                                  ##\n",
    "##  Outputs: percent_of_dzeros: the percent of correct answers in   ##\n",
    "##              catch trials in which LL delay is zero.             ##\n",
    "##           percent_of_imm1000s: the percent of correct answers in ##\n",
    "##              catch trials in which LL delay is zero.             ##\n",
    "##           overall_percent: the overall percent of correct        ##\n",
    "##              answers in all catch trials.                        ##\n",
    "##                                                                  ##\n",
    "##  Note: Currently, the only output used is the overall_percent.   ##\n",
    "##        I wrote it to be able to provide more specific output,    ##\n",
    "##        but I don't think that level of detail is necessary in    ##\n",
    "##        the output tables, so when this function is called, I     ##\n",
    "##        accept but never use the first two values. If you want to ##\n",
    "##        analyze them, you can use them there.                     ##\n",
    "##------------------------------------------------------------------##\n",
    "\n",
    "def checkAttention(p_immediate_vals, p_delay_times, p_choices):\n",
    "    # p_choices is 1 when LL is chosen. p_choices[p_delay_times==0] gets all instances in which the delay time is zero. These are \"correct\" when p_choices is 1, so we take the sum and divide by the total trials to get the average correct.\n",
    "    percent_of_dzeros = np.sum(p_choices[p_delay_times==0])/np.size(p_choices[p_delay_times==0])\n",
    "    # Same as above, except we're indexing based on SS_V being 1000 and we're dividing (the total number of options - the number of delay choices (\"wrong\" selections) in that set) by the total number of options.\n",
    "    # For example, if someone is given 3 trials in which SS_V=1000 and they choose SS/SS/LL, choices[] = [0,0,1]. np.size(<-) = 3, np.sum(<-) = 1, (size-sum)/size = 2/3 = 66% correct.\n",
    "    percent_of_imm1000s = (np.size(p_choices[p_immediate_vals==1000])-np.sum(p_choices[p_immediate_vals==1000]))/np.size(p_choices[p_immediate_vals==1000])\n",
    "    # Weighted average of the above two.\n",
    "    overall_percent = (percent_of_dzeros*np.size(p_choices[p_delay_times==0]) + percent_of_imm1000s*np.size(p_choices[p_immediate_vals==1000]))/np.size(p_choices)\n",
    "    return percent_of_dzeros, percent_of_imm1000s, overall_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------------------------------------------------##\n",
    "##                             Analysis                             ##\n",
    "##  This code runs the log-likelihood-based analysis of model fit.  ##\n",
    "##  It also computes the percentage of choices that the model       ##\n",
    "##  accurately predicts.                                            ##\n",
    "##                                                                  ##\n",
    "##  Inputs: negLL: negative log likelihood of the best option after ## \n",
    "##              optimization                                        ##\n",
    "##          choices: the set of choices from the participants       ##\n",
    "##          Lists (length=number of trials):                        ##\n",
    "##              SS_V: values of the smaller-sooner (immediate) ops. ##\n",
    "##              SS_D: delays of the smaller-sooner (immediate) ops. ##\n",
    "##              LL_V: values of the larger-later (delayed) options  ##\n",
    "##              LL_D: delays of the larger-later (delayed) options  ##\n",
    "##          risk: not currently in use (always 1 in this code). If  ##\n",
    "##              you want to use a risk parameter (alpha), you       ##\n",
    "##              can input one here.                                 ##\n",
    "##          given_k: the optimized k value. If used elsewhere, this ##\n",
    "##              function could also accept and test any k value     ##\n",
    "##          beta: this is actually unnecessary. I originally wrote  ##\n",
    "##              this to use the choice_prob function to determine   ##\n",
    "##              the probability of each choice based on k, but it's ##\n",
    "##              turned into a binary anyway, so the beta is not     ##\n",
    "##              really taken into account. However, as they say, it ##\n",
    "##              ain't broke, so I haven't fixed it.                 ##\n",
    "##                                                                  ##\n",
    "##  Outputs: LL: log likelihood (high is good)                      ##\n",
    "##           LL0: log likelihood of a model that just coin flips    ##\n",
    "##                every choice guess (probability of LL = 0.5). The ##\n",
    "##                analysis is mostly judging LL vs LL0.             ##\n",
    "##           AIC: Akaike Information Criterion (high is bad)        ##\n",
    "##           BIC: Bayesian Information Criterion (high is bad)      ##\n",
    "##           r2: r squared                                          ##\n",
    "##           correct: the percentage of choices that the model      ##\n",
    "##                accurately predicts based on k                    ##\n",
    "##------------------------------------------------------------------##\n",
    "\n",
    "def analysis(negLL,choices,SS_V,SS_D,LL_V,LL_D,risk,given_k,beta):\n",
    "    # Unrestricted log-likelihood\n",
    "    LL = -negLL\n",
    "\n",
    "    # Restricted log-likelihood\n",
    "    LL0 = np.sum((choices==1)*math.log(0.5) + (1-(choices==1))*math.log(0.5))\n",
    "\n",
    "    # Akaike Information Criterion\n",
    "    AIC = -2*LL + 2*2  #CHANGE TO len(results.x) IF USING A DIFFERENT MODEL (parameters != 2)\n",
    "\n",
    "    # Bayesian information criterion\n",
    "    BIC = -2*LL + 2*math.log(len(SS_V))  #len(results.x)\n",
    "\n",
    "    #R squared\n",
    "    r2 = 1 - LL/LL0\n",
    "\n",
    "    #Percent accuracy\n",
    "    k_for_accuracy = given_k\n",
    "    beta_and_k_array_for_accuracy = [beta,k_for_accuracy] \n",
    "    parray = np.array(choice_prob(SS_V,SS_D,LL_V,LL_D,beta_and_k_array_for_accuracy,risk))  # gets an array of probabilities of choosing the LL choice\n",
    "    correct =sum((parray>=0.5)==choices)/len(SS_V)                                          # LL is 1 in choices, so when the parray is > 0.5 and choices==1, the model has correctly predicted a choice.\n",
    "    \n",
    "    return(LL,LL0,AIC,BIC,r2,correct)\n",
    "\n",
    "# Hessian unavailable in this optimization function, but would use results.hess_inv here\n",
    "#Tester line if you want: print(\"LL\",LL,\"AIC\",AIC,\"BIC\",BIC,\"R2\",r2,\"correct\",correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------------------------------------------------##\n",
    "##                        Discount function                         ##\n",
    "##  This code performs the hyperbolic discounting function.         ##\n",
    "##                                                                  ##\n",
    "##  Inputs: v: value                                                ##\n",
    "##          d: delay                                                ##\n",
    "##          kappa: k, the discount parameter                        ##\n",
    "##          risk: not currently in use (always 1 in this code). If  ##\n",
    "##                you want to use a risk parameter (alpha), you     ##\n",
    "##                can input one here.                               ##\n",
    "##                                                                  ##\n",
    "##  Outputs: SV: subjective value                                   ##\n",
    "##------------------------------------------------------------------##\n",
    "\n",
    "\n",
    "\n",
    "def discount(v,d,kappa,risk):\n",
    "    SV = (v**risk)/(1+kappa*d)\n",
    "    return SV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------------------------------------------------##\n",
    "##                        Choice Probability                        ##\n",
    "##  This code uses the stochasticity factor beta to determine,      ##\n",
    "##  based on the subjective values of the immediate and delayed     ##\n",
    "##  options, the chance of someone choosing the delay option.       ##\n",
    "##  Note: whether the chance is above or below 0.5 (50%) is         ##\n",
    "##  entirely based on which SV is higher. This could hypothetically ##\n",
    "##  be changed with some bias factor, but isn't currently. In other ##\n",
    "##  words, if SSsv is greater than LLsv, the chance that the person ##\n",
    "##  chooses the SS option will be >0.5 regardless, but beta         ##\n",
    "##  determines if it's 0.51 or 0.99.                                ##\n",
    "##                                                                  ##\n",
    "##  Inputs: Lists (length=number of trials):                        ##\n",
    "##              SS_V: values of the smaller-sooner (immediate) ops. ##\n",
    "##              SS_D: delays of the smaller-sooner (immediate) ops. ##\n",
    "##              LL_V: values of the larger-later (delayed) options  ##\n",
    "##              LL_D: delays of the larger-later (delayed) options  ##\n",
    "##          beta_and_k_array: two item array consisting of:         ##\n",
    "##              [0]: beta (b), the stochasticity-of-choice factor   ##\n",
    "##              [1]: kappa (k), the discount parameter              ##\n",
    "##          risk: not currently in use (always 1 in this code). If  ##\n",
    "##                you want to use a risk parameter (alpha), you     ##\n",
    "##                can input one here.                               ##\n",
    "##                                                                  ##\n",
    "##  Outputs: ps: this is a list of probabilities of choosing the    ##\n",
    "##               delay option. Note that when the probability of    ##\n",
    "##               choosing the delay option approaches 1, it also    ##\n",
    "##               approaches the indicator of that option in the     ##\n",
    "##               choice data (1).                                   ##\n",
    "##               This is a list of size = num of trials. It gets    ##\n",
    "##               turned into a numpy array in local_negLL()         ##\n",
    "##                                                                  ##\n",
    "##  Note: (SS_V,SS_D,LL_V,LL_D) were once called (v1,d1,v2,d2).     ##\n",
    "##        I've changed them here for clarity, but if you see those  ##\n",
    "##        terms elsewhere, now you know.                            ##\n",
    "##                                                                  ##\n",
    "##  Note: This code used to use \"beta\" to mean the array of the     ##\n",
    "##        stochasticity factor and kappa. We're calling the         ##\n",
    "##        stochasticity factor \"beta\" now, however, making this     ##\n",
    "##        terminology confusing. For clarity, I've changed the name ##\n",
    "##        of the array to \"beta_and_k_array\".                       ##\n",
    "##------------------------------------------------------------------##\n",
    "\n",
    "def choice_prob(SS_V,SS_D,LL_V,LL_D,beta_and_k_array,risk):\n",
    "    ps = []\n",
    "\n",
    "    for n in range(len(SS_V)):\n",
    "\n",
    "        SS_SV = discount(SS_V[n],SS_D[n],beta_and_k_array[1],risk)\n",
    "        LL_SV = discount(LL_V[n],LL_D[n],beta_and_k_array[1],risk)\n",
    "\n",
    "        try: \n",
    "            p = 1 / (1 + math.exp(beta_and_k_array[0]*(SS_SV-LL_SV)))     ## Math.exp does e^(). In other words, if the smaller-sooner SV is higher than the larger-later SV, e^x will be larger, making the denominator larger, making 1/denom closer to zero (low probability of choosing delay). If the LL SV is higher, the e^x will be lower, making 1/denom close to 1 (high probability of choosing delay). If they are the same, e^0=1, 1/(1+1) = 0.5, 50% chance of choosing delay.\n",
    "        except OverflowError:                                             ## Sometimes the SS_SV is very much higher than the LL_SV. If beta gets too high, the exponent on e will get huge. Math.exp will throw an OverflowError if the numbers get too big. In that case, 1/(1+[something huge]) is essentially zero, so we just set it to 0.\n",
    "            p = 0\n",
    "        ps.append(p)\n",
    "        \n",
    "    return ps\n",
    "\n",
    "## If you're getting weird results or think there are problems with the data, inserting the below indicator might provide clarity.\n",
    "#print(\"beta:\",beta[0],\"k:\",beta[1],\"SV_1:\",SV_1,\"SV_2\",SV_2,\"imm val\",SS_V[n],\"imm delay\",SS_D[n], \"del val\",LL_V[n],\"del del\",LL_D[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------------------------------------------------##\n",
    "##                  Local Negative Log Likelihood                   ##\n",
    "##  This code uses choice probabilities computed by choice_prob()   ##\n",
    "##  to calculate the negative log likelihood of the data given      ##\n",
    "##  given those choice probabilities. Essentially, we're seeing if  ##\n",
    "##  the provided k and b are a good match for the data we have.     ##\n",
    "##  If it isn't clear, the optimization algorithm will be looping   ##\n",
    "##  through this over and over, finding the negative log likelihood ##\n",
    "##  at each point in the parameter space and \"surfing\" downwards to ##\n",
    "##  find the ideal point (the minimum)                              ##\n",
    "##                                                                  ##\n",
    "##  Inputs: beta_and_k_array: two item array consisting of:         ##\n",
    "##              [0]: beta (b), the stochasticity-of-choice factor   ##\n",
    "##              [1]: kappa (k), the discount parameter              ##\n",
    "##          Lists (length=number of trials):                        ##\n",
    "##              choices_list: choices (1 or 0, 1=delay option)      ##\n",
    "##              SS_V: values of the smaller-sooner (immediate) ops. ##\n",
    "##              SS_D: delays of the smaller-sooner (immediate) ops. ##\n",
    "##              LL_V: values of the larger-later (delayed) options  ##\n",
    "##              LL_D: delays of the larger-later (delayed) options  ##\n",
    "##          risk: not currently in use (always 1 in this code). If  ##\n",
    "##                you want to use a risk parameter (alpha), you     ##\n",
    "##                can input one here.                               ##\n",
    "##                                                                  ##\n",
    "##  Outputs: sumerr: this is the negative log likelihood - the sum  ##\n",
    "##           of the error at each choice (or trial). The error is   ##\n",
    "##           determined as follows: if the choice was 1, the        ##\n",
    "##           calculated probability would ideally be 1. Log(1) is   ##\n",
    "##           0, indicating no error. The same idea applies for the  ##\n",
    "##           choices of 0 - error=0 when p=0. As p deviates from    ##\n",
    "##           the actual choice, error increases. This is all done   ##\n",
    "##           at once using numpy arrays.                            ##\n",
    "##                                                                  ##\n",
    "##  Note: if anyone is reading this who is new to arrays in Python, ##\n",
    "##  doing (array==x) returns an array boolean values indicating the ##\n",
    "##  points at which the original array equals x. Array2[array1==x]  ##\n",
    "##  feeds that boolean array into array2, getting the values of     ##\n",
    "##  array2 where array1 equals x. This basic form is used all       ##\n",
    "##  thoughout this program, so I thought I'd mention it.            ##\n",
    "##------------------------------------------------------------------##\n",
    "\n",
    "def local_negLL(beta_and_k_array,choices_list,SS_V,SS_D,LL_V,LL_D,risk):\n",
    "\n",
    "    ps = np.array(choice_prob(SS_V,SS_D,LL_V,LL_D,beta_and_k_array,risk))\n",
    "    choices = np.array(choices_list)\n",
    "\n",
    "    # Trap log(0). This will prevent the code from trying to calculate the log of 0 in the next section.\n",
    "    ps[ps==0] = 0.0001\n",
    "    ps[ps==1] = 0.9999\n",
    "    \n",
    "    # Log-likelihood\n",
    "    err = (choices==1)*np.log(ps) + ((choices==0))*np.log(1-ps)\n",
    "\n",
    "    # Sum of -log-likelihood\n",
    "    sumerr = -sum(err)\n",
    "\n",
    "    return sumerr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------------------------------------------------##\n",
    "##                           Optimize Me                            ##\n",
    "##  This code gives the optimization algorithm a specific function  ##\n",
    "##  to optimize. The scipy -sp.optimize.minimize- requests a        ##\n",
    "##  function followed by two inputs: an array of the parameters to  ##\n",
    "##  optimize (which is why we have them in a beta_and_k_array in    ##\n",
    "##  the first place) and the other inputs to that function.         ##\n",
    "##  Our local_negLL() is formulated slightly differently than that, ##\n",
    "##  so we're using this code to construct it properly.              ##\n",
    "##                                                                  ##\n",
    "##  Inputs: beta_and_k_array_to_optimize: two item array:           ##\n",
    "##              [0]: beta (b), the stochasticity-of-choice factor   ##\n",
    "##              [1]: kappa (k), the discount parameter              ##\n",
    "##          inputs: six item array, the first five of which are     ##\n",
    "##          Lists (length=number of trials):                        ##\n",
    "##              [0]: choices_list: choices (1 or 0, 1=delay option) ##\n",
    "##              [1]: SS_V: values of the SS (immediate) ops.        ##\n",
    "##              [2]: SS_D: delays of the SS (immediate) ops.        ##\n",
    "##              [3]: LL_V: values of the LL (delayed) options       ##\n",
    "##              [4]: LL_D: delays of the LL (delayed) options       ##\n",
    "##              [5]: risk: not currently in use (always 1 in this   ##\n",
    "##                      code). If you want to use a risk parameter  ##\n",
    "##                      (alpha), you can input one here.            ##\n",
    "##                                                                  ##\n",
    "##  Outputs: the output of the local_negLL function applied to the  ##\n",
    "##           given b, k, choices, values, and delays.               ##\n",
    "##------------------------------------------------------------------##\n",
    "\n",
    "def optimize_me(beta_and_k_array_to_optimize, inputs):\n",
    "    choices_list,SS_V,SS_D,LL_V,LL_D,risk = inputs\n",
    "    return local_negLL(beta_and_k_array_to_optimize,choices_list,SS_V,SS_D,LL_V,LL_D,risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------------------------------------------------##\n",
    "##                            Write SVs                             ##\n",
    "##  This code is just array management - nothing clever. It writes  ##\n",
    "##  the calculated SVs into arrays for export into individual       ##\n",
    "##  files and into an overall SV table. I think the individual      ##\n",
    "##  files are less useful. The large one is critical though - it's  ##\n",
    "##  used by the Bayesian method as a source for the values and IDs. ##\n",
    "##                                                                  ##\n",
    "##  Inputs: filename: name of the file (includes \"FixedFixed\")      ##\n",
    "##          k: kappa, the discount parameter                        ##\n",
    "##          b: beta, the stochasticity-of-choice factor             ##\n",
    "##          LL: log likelihood                                      ##\n",
    "##          AIC: Akaike Information Criterion (from analysis())     ##\n",
    "##          BIC: Bayesian Information Criterion (from analysis())   ##\n",
    "##          r2: r squared                                           ##\n",
    "##          correct: the percentage of choices that the model       ##\n",
    "##                accurately predicted                              ##\n",
    "##          risk: not currently in use (always 1 in this code). If  ##\n",
    "##                you want to use a risk parameter (alpha), you     ##\n",
    "##                can input one here.                               ##\n",
    "##          catchtrialpercentcorrect: percentage of the catch       ##\n",
    "##                trials accurately predicted by the model          ##\n",
    "##                                                                  ##\n",
    "##  Outputs: the output of the local_negLL function applied to the  ##\n",
    "##           given b, k, choices, values, and delays.               ##\n",
    "##------------------------------------------------------------------##\n",
    "\n",
    "def write_SVs(filename,k,beta,LL,AIC,BIC,r2,correct,risk,catchtrialpercentcorrect):\n",
    "    # Construct the file path to the \"SVFixed\" files. \n",
    "    # Example input file name:  All Files Folder\\FixedFixedAdjAmt_1083_1_01-16-2020_16h-31m.csv\n",
    "    #                                          17^  22^\n",
    "    # Example output file name: All Files Folder\\SVFixedAdjAmt_1083_1_01-16-2020_16h-31m.csv\n",
    "    newpath = filename[:17]+\"SV\"+filename[22:]\n",
    "    a=pd.read_csv(newpath,quotechar='\"',skipinitialspace=True, header=None,index_col=None, usecols=[0,1,2,3,4,5,6,7]).to_numpy()\n",
    "    \n",
    "    # Define new arrays based on the shape of the old columns. The -1 allows the height of the new arrays to be anything (based on a column number of 1)\n",
    "    SV_imm_array = np.empty((a.shape[0]-1,1))\n",
    "    SV_del_array = np.empty((a.shape[0]-1,1))\n",
    "\n",
    "    # Note: This is looping through one person/one session at a time. We only find one k, beta, etc. per session, so our k column should just be a column all of the same number. While a little goofy, I suspect this will become convenient for analysis later when using the large SV table, as multiple sessions are combined there.\n",
    "    k_array = np.full((a.shape[0]-1,1),k)\n",
    "    beta_array = np.full((a.shape[0]-1,1),beta)\n",
    "    LL_array = np.full((a.shape[0]-1,1),LL)\n",
    "    AIC_array = np.full((a.shape[0]-1,1),AIC)\n",
    "    BIC_array = np.full((a.shape[0]-1,1),BIC)\n",
    "    r2_array = np.full((a.shape[0]-1,1),r2)\n",
    "    correct_array = np.full((a.shape[0]-1,1),correct)\n",
    "    catchtrialpercentcorrect_array = np.full((a.shape[0]-1,1),catchtrialpercentcorrect)\n",
    "\n",
    "    # Same as above but for identifiers\n",
    "    ID_array = np.full((a.shape[0]-1,1),filename[34:38])\n",
    "    firstorsecond_array = np.full((a.shape[0]-1,1),filename[39:40])\n",
    "    date_array = np.full((a.shape[0]-1,1),filename[41:51])\n",
    "    time_array = np.full((a.shape[0]-1,1),filename[52:59])\n",
    "\n",
    "    # Here's the arrangement of column titles in the output file:\n",
    "    # Trial number\tStimulus onset\tResponse time\tSS amount\tLL amount\tSS delay\tLL delay\tResponse [-1O_0I_1D]\tSS SV\tLL SV\tk\tbeta\tID\tDay\tDate\tTime\tLL\tAIC\tBIC\tr2\tPercent correct\tPercent correct on catch trials\n",
    "    # value_col in range(3,5) | value_col==3:           3                     (3+2)\n",
    "    #                         | value_col==4:                       4                     (4+2)               \n",
    "\n",
    "    for value_col in range(3,5):                  # see above\n",
    "        for row in range(1,a.shape[0]):           # iterate through columns, computing all the SS SVs and then the LL SVs. I don't think this code is especially elegant - feel free to change (though it works)\n",
    "            v = float(a[row,value_col])\n",
    "            d = float(a[row,(value_col+2)])\n",
    "            SV = discount(v,d,k,risk)\n",
    "            if value_col==3: SV_imm_array[row-1] = (SV)\n",
    "            if value_col==4: SV_del_array[row-1] = (SV)\n",
    "    \n",
    "    # Here we're constructing the final output array. The various numerical columns are hstacked together:\n",
    "    #    [0]    +    [2]     =    [0][2]\n",
    "    #    [1]         [3]          [1][3]\n",
    "    # Then we vstack the titles onto the number array:\n",
    "    #    [\"SS SV\"][\"LL SV\"]  +    [0][2]    =    [\"SS SV\"][\"LL SV\"] \n",
    "    #                             [1][3]         [   0   ][   2   ]\n",
    "    #                                            [   1   ][   3   ]\n",
    "    # Then we hstack the original array (which was read in from the SV file, but we only used the first eight columns, so this contains primarily the original value and delay information)\n",
    "    #   [\"Trial Number\"][\"Stimulus Onset\"]     +     [\"SS SV\"][\"LL SV\"]     =     [\"Trial Number\"][\"Stimulus Onset\"][\"SS SV\"][\"LL SV\"]\n",
    "    #   [       1      ][       10       ]           [   0   ][   2   ]           [       1      ][       10       ][   0   ][   2   ]\n",
    "    #   [       2      ][       20       ]           [   1   ][   3   ]     =     [       2      ][       20       ][   1   ][   3   ]\n",
    "\n",
    "\n",
    "    titles = np.array([\"SS SV\",\"LL SV\",\"k\",\"beta\",\"ID\",\"Day\",\"Date\",\"Time\",\"LL\",\"AIC\",\"BIC\",\"r2\",\"Percent correct\",\"Percent correct on catch trials\"])\n",
    "    numbers = np.hstack((SV_imm_array,SV_del_array,k_array,beta_array,ID_array,firstorsecond_array,date_array,time_array,LL_array,AIC_array,BIC_array,r2_array,correct_array,catchtrialpercentcorrect_array))\n",
    "    sidebar = np.vstack((titles,numbers))\n",
    "    output = np.hstack((a,sidebar))\n",
    "    \n",
    "    # This will write individual SV files. Comment out to prevent this.\n",
    "    #pd.DataFrame(output).to_csv(newpath, header=False, index=False)  \n",
    "    return output\n",
    "\n",
    "\n",
    "# I had previously had trouble with this code, so I wrote some testing lines. I'll leave them here in case they're useful in the future (though the original issues were solved)\n",
    "    #print(a.shape, SV_imm_array.shape, SV_del_array.shape)\n",
    "    #print(\"initial array\")\n",
    "    #print(a[:4,:])\n",
    "    #print(\"sv i\")\n",
    "    #print(SV_imm_array[:4,:])\n",
    "    #print(\"sv d\")\n",
    "    #print(SV_del_array[:4,:])   \n",
    "    # \n",
    "    #print(\"output\")\n",
    "    #print(output[:4,:])\n",
    "    #print(newpath)\n",
    "    #print(a[4,:],SV_imm_array[4,:],SV_del_array[4,:])     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct File Name List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------------------------------------------------##\n",
    "##                     Construct File Name List                     ##\n",
    "##  This code is very simple. It begins with the full list of       ##\n",
    "##  original file names (those original files are no longer used    ##\n",
    "##  here). I had run these files through some code cleaned them up: ##\n",
    "##  for example, \"get $1,000\" is changed to \"1000\". The             ##\n",
    "##  \"FixedFixed\" files are the correct ones.                        ##\n",
    "##------------------------------------------------------------------##\n",
    "\n",
    "datanames = [\"All Files Folder\\AdjAmt_1083_1_01-16-2020_16h-31m.csv\",\"All Files Folder\\AdjAmt_1083_1_03-03-2022_16h-12m.csv\",\"All Files Folder\\AdjAmt_1539_1_04-04-2019_17h-21m.csv\",\"All Files Folder\\AdjAmt_1539_2_04-11-2019_16h-51m.csv\",\"All Files Folder\\AdjAmt_2008_1_11-14-2019_17h-30m.csv\",\"All Files Folder\\AdjAmt_2008_2_11-19-2019_16h-16m.csv\",\"All Files Folder\\AdjAmt_2020_1_09-24-2019_16h-25m.csv\",\"All Files Folder\\AdjAmt_2020_2_10-10-2019_16h-47m.csv\",\"All Files Folder\\AdjAmt_2042_1_09-30-2021_17h-13m.csv\",\"All Files Folder\\AdjAmt_2042_2_04-07-2022_15h-40m.csv\",\"All Files Folder\\AdjAmt_2399_1_07-16-2019_16h-53m.csv\",\"All Files Folder\\AdjAmt_2475_1_12-05-2019_16h-44m.csv\",\"All Files Folder\\AdjAmt_2475_2_01-09-2020_16h-36m.csv\",\"All Files Folder\\AdjAmt_2503_1_01-07-2020_16h-48m.csv\",\"All Files Folder\\AdjAmt_2503_2_02-06-2020_16h-03m.csv\",\"All Files Folder\\AdjAmt_2506_1_10-01-2019_16h-33m.csv\",\"All Files Folder\\AdjAmt_2506_2_12-10-2019_16h-32m.csv\",\"All Files Folder\\AdjAmt_2874_1_04-05-2018_16h-54m.csv\",\"All Files Folder\\AdjAmt_2874_2_06-21-2018_15h-26m.csv\",\"All Files Folder\\AdjAmt_3278_1_05-10-2018_16h-29m.csv\",\"All Files Folder\\AdjAmt_3278_2_06-07-2018_16h-59m.csv\",\"All Files Folder\\AdjAmt_3458_1_01-31-2019_16h-21m.csv\",\"All Files Folder\\AdjAmt_3546_1_05-05-2022_16h-34m.csv\",\"All Files Folder\\AdjAmt_3546_2_06-02-2022_14h-16m.csv\",\"All Files Folder\\AdjAmt_3638_1_10-12-2017_16h-41m.csv\",\"All Files Folder\\AdjAmt_3638_2_11-28-2017_17h-14m.csv\",\"All Files Folder\\AdjAmt_3687_1_03-18-2021_16h-35m.csv\",\"All Files Folder\\AdjAmt_3687_2_03-25-2021_16h-50m.csv\",\"All Files Folder\\AdjAmt_3710_1_05-10-2022_16h-30m.csv\",\"All Files Folder\\AdjAmt_3812_1_08-08-2019_16h-55m.csv\",\"All Files Folder\\AdjAmt_3812_2_11-12-2019_13h-36m.csv\",\"All Files Folder\\AdjAmt_3924_1_04-14-2022_16h-24m.csv\",\"All Files Folder\\AdjAmt_3924_2_04-21-2022_15h-10m.csv\",\"All Files Folder\\AdjAmt_3937_1_12-03-2020_17h-33m.csv\",\"All Files Folder\\AdjAmt_4292_1_05-14-2019_17h-32m.csv\",\"All Files Folder\\AdjAmt_4292_2_05-21-2019_17h-27m.csv\",\"All Files Folder\\AdjAmt_4513_1_09-27-2018_16h-19m.csv\",\"All Files Folder\\AdjAmt_4513_2_10-11-2018_16h-08m.csv\",\"All Files Folder\\AdjAmt_4513_3_05-16-2019_12h-55m.csv\",\"All Files Folder\\AdjAmt_4556_1_03-14-2019_16h-22m.csv\",\"All Files Folder\\AdjAmt_4556_2_01-14-2021_15h-35m.csv\",\"All Files Folder\\AdjAmt_4776_1_09-26-2019_17h-08m.csv\",\"All Files Folder\\AdjAmt_4776_2_10-17-2019_16h-28m.csv\",\"All Files Folder\\AdjAmt_4805_1_12-12-2017_17h-42m.csv\",\"All Files Folder\\AdjAmt_4805_2_01-04-2018_16h-27m.csv\",\"All Files Folder\\AdjAmt_4884_1_08-27-2019_16h-41m.csv\",\"All Files Folder\\AdjAmt_4884_2_08-29-2019_16h-20m.csv\",\"All Files Folder\\AdjAmt_4884_3_07-26-2021_16h-18m.csv\",\"All Files Folder\\AdjAmt_5334_1_01-07-2021_16h-51m.csv\",\"All Files Folder\\AdjAmt_5625_1_08-15-2019_16h-30m.csv\",\"All Files Folder\\AdjAmt_5625_2_09-12-2019_16h-20m.csv\",\"All Files Folder\\AdjAmt_5667_1_07-11-2019_16h-46m.csv\",\"All Files Folder\\AdjAmt_5667_2_08-13-2019_16h-58m.csv\",\"All Files Folder\\AdjAmt_5764_1_10-26-2017_16h-57m.csv\",\"All Files Folder\\AdjAmt_5764_2_11-20-2017_15h-19m.csv\",\"All Files Folder\\AdjAmt_5897_1_11-08-2018_16h-56m.csv\",\"All Files Folder\\AdjAmt_5897_2_11-29-2018_16h-24m.csv\",\"All Files Folder\\AdjAmt_5900_1_01-18-2018_16h-33m.csv\",\"All Files Folder\\AdjAmt_5900_2_02-01-2018_16h-39m.csv\",\"All Files Folder\\AdjAmt_5940_1_04-29-2021_16h-42m.csv\",\"All Files Folder\\AdjAmt_5940_2_06-24-2021_15h-25m.csv\",\"All Files Folder\\AdjAmt_6059_1_12-20-2018_16h-31m.csv\",\"All Files Folder\\AdjAmt_6059_2_01-03-2019_16h-16m.csv\",\"All Files Folder\\AdjAmt_6269_1_03-08-2018_16h-54m.csv\",\"All Files Folder\\AdjAmt_6269_2_03-29-2018_16h-43m.csv\",\"All Files Folder\\AdjAmt_6281_1_07-12-2018_16h-49m.csv\",\"All Files Folder\\AdjAmt_6281_2_07-26-2018_16h-30m.csv\",\"All Files Folder\\AdjAmt_6382_1_04-18-2022_17h-06m.csv\",\"All Files Folder\\AdjAmt_6463_1_02-15-2018_16h-46m.csv\",\"All Files Folder\\AdjAmt_6463_2_03-15-2018_16h-16m.csv\",\"All Files Folder\\AdjAmt_6617_1_05-12-2022_16h-17m.csv\",\"All Files Folder\\AdjAmt_6624_1_08-09-2018_16h-36m.csv\",\"All Files Folder\\AdjAmt_6624_2_09-13-2018_16h-02m.csv\",\"All Files Folder\\AdjAmt_6630_2_06-16-2022_16h-29m.csv\",\"All Files Folder\\AdjAmt_6707_1.2_06-22-2021_15h-17m.csv\",\"All Files Folder\\AdjAmt_6707_2.2_07-12-2021_15h-47m.csv\",\"All Files Folder\\AdjAmt_6817_1_10-25-2021_16h-25m.csv\",\"All Files Folder\\AdjAmt_6817_2_11-16-2021_16h-47m.csv\",\"All Files Folder\\AdjAmt_7054_1_12-02-2021_17h-19m.csv\",\"All Files Folder\\AdjAmt_7054_2_12-09-2021_16h-23m.csv\",\"All Files Folder\\AdjAmt_7238_1_02-05-2019_17h-02m.csv\",\"All Files Folder\\AdjAmt_7238_2_02-07-2019_16h-26m.csv\",\"All Files Folder\\AdjAmt_7309_1_09-10-2019_16h-30m.csv\",\"All Files Folder\\AdjAmt_7309_1_10-03-2019_16h-02m.csv\",\"All Files Folder\\AdjAmt_7387_1_06-13-2019_17h-16m.csv\",\"All Files Folder\\AdjAmt_7387_2_09-05-2019_16h-06m.csv\",\"All Files Folder\\AdjAmt_7467_1_01-13-2022_16h-48m.csv\",\"All Files Folder\\AdjAmt_7626_1_06-09-2022_15h-17m.csv\",\"All Files Folder\\AdjAmt_7637_1_07-29-2020_17h-09m.csv\",\"All Files Folder\\AdjAmt_7637_2_10-22-2020_15h-46m.csv\",\"All Files Folder\\AdjAmt_7897_1_09-03-2020_17h-33m.csv\",\"All Files Folder\\AdjAmt_7897_2_10-08-2020_16h-28m.csv\",\"All Files Folder\\AdjAmt_8267_2_05-09-2019_16h-07m.csv\",\"All Files Folder\\AdjAmt_8276_1_07-25-2019_17h-30m.csv\",\"All Files Folder\\AdjAmt_8276_2_08-06-2019_16h-56m.csv\",\"All Files Folder\\AdjAmt_8370_1_04-17-2018_16h-35m.csv\",\"All Files Folder\\AdjAmt_8370_2_04-26-2018_16h-35m.csv\",\"All Files Folder\\AdjAmt_8416_1_04-12-2018_16h-17m.csv\",\"All Files Folder\\AdjAmt_8416_2_05-03-2018_15h-12m.csv\",\"All Files Folder\\AdjAmt_8474_1_08-30-2018_16h-12m.csv\",\"All Files Folder\\AdjAmt_8474_2_10-04-2018_16h-10m.csv\",\"All Files Folder\\AdjAmt_8475_1_01-30-2020_17h-01m.csv\",\"All Files Folder\\AdjAmt_8475_3_03-15-2021_15h-48m.csv\",\"All Files Folder\\AdjAmt_8532_1_12-14-2017_16h-45m.csv\",\"All Files Folder\\AdjAmt_8532_2_12-21-2017_15h-20m.csv\",\"All Files Folder\\AdjAmt_8568_1_01-21-2021_16h-52m.csv\",\"All Files Folder\\AdjAmt_8568_2_01-28-2021_16h-23m.csv\",\"All Files Folder\\AdjAmt_8582_2_04-26-2022_15h-00m.csv\",\"All Files Folder\\AdjAmt_8890_1_12-03-2019_16h-11m.csv\",\"All Files Folder\\AdjAmt_8890_2_01-14-2020_16h-24m.csv\",\"All Files Folder\\AdjAmt_9337_1_04-16-2019_15h-26m.csv\",\"All Files Folder\\AdjAmt_9337_2_04-23-2019_15h-42m.csv\",\"All Files Folder\\AdjAmt_9639_1_03-07-2019_16h-16m.csv\",\"All Files Folder\\AdjAmt_9639_2_04-25-2019_16h-16m.csv\",\"All Files Folder\\AdjAmt_9716_1_05-23-2019_16h-24m.csv\",\"All Files Folder\\AdjAmt_9716_2_07-09-2019_16h-35m.csv\"]\n",
    "files = []\n",
    "for nam in datanames:\n",
    "    newpath = nam[:17]+\"FixedFixed\"+nam[17:]\n",
    "    files.append(newpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus Outdated Function - LL From Given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets the negLL from a given k and b. It's no longer in use, as I'm no longer doing those comparisons. However, because the run_functions() code allows for the option to run a comparison like this, I figured I'd leave the code here. \n",
    "\n",
    "def LLfromGiven(given_k, given_b, choices, SS_V, SS_D, LL_V, LL_D, risk):\n",
    "    given_beta_and_k_array = [given_b,given_k]\n",
    "    negLL = local_negLL(given_beta_and_k_array,choices,SS_V,SS_D,LL_V,LL_D,risk)\n",
    "    return negLL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09d5eda77765311109b2c027e144dcf58f89dd96008cdf29c2e6b03e99df71a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

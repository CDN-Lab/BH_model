{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy as sp\n",
    "from scipy import optimize\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd \n",
    "#import pymc3 as pm\n",
    "#import arviz as az\n",
    "import statistics as stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import a data set to 1) use the formatting, and 2) use the values of the choice options. I didn't want to make up choice options for fear of departing from what we are attempting to simulate.\n",
    "data = genfromtxt(\"Full Square\\FAKE RANDOMIZED Full Subjective Value Table Full Square.csv\", delimiter=',', dtype=str)\n",
    "#Column titles: Trial Number\tStimulus Time\tResponse Time\tSS amount\tLL amount\tSS delay\tLL delay\tResponse\tSS SV\tLL SV\tk\tbeta\tID\tDay\tDate\tTime\tLL\tAIC\tBIC\tr2\tcorrect percent\n",
    "IDsUncut = np.array(data[1:,12])\n",
    "DaysUncut = np.array(data[1:,13])\n",
    "DatesUncut = np.array(data[1:,14])\n",
    "TimesUncut = np.array(data[1:,15])\n",
    "IdentifiersUncut = np.array([i + j + k + l for i, j, k, l in zip(IDsUncut, DaysUncut, DatesUncut, TimesUncut)])\n",
    "\n",
    "SSAmountUncut = np.array(data[1:,3], dtype=float)\n",
    "SSDelayUncut = np.array(data[1:,5], dtype=float)\n",
    "LLAmountUncut = np.array(data[1:,4], dtype=float)\n",
    "LLDelayUncut = np.array(data[1:,6], dtype=float)\n",
    "\n",
    "# Use these lines if using the original k and b later.\n",
    "#OGChoices = np.array(data[1:,7], dtype=int)\n",
    "#OGBetas = np.array(data[1:,11], dtype=float)\n",
    "#OGKappas = np.array(data[1:,10], dtype=float)\n",
    "\n",
    "boolselector = (SSAmountUncut<1000)*(LLDelayUncut>0)\n",
    "\n",
    "IDs = IDsUncut[boolselector]\n",
    "Days = DaysUncut[boolselector]\n",
    "Dates = DatesUncut[boolselector]\n",
    "Times = TimesUncut[boolselector]\n",
    "Identifiers = IdentifiersUncut[boolselector]\n",
    "SSAmount = SSAmountUncut[boolselector]\n",
    "SSDelay = SSDelayUncut[boolselector]\n",
    "LLAmount = LLAmountUncut[boolselector]\n",
    "LLDelay = LLDelayUncut[boolselector]\n",
    "#OGChoices = OGChoices[boolselector]\n",
    "#OGBetas = OGBetas[boolselector]\n",
    "#OGKappas = OGKappas[boolselector]\n",
    "\n",
    "groups = np.full(len(IDs),100000)\n",
    "alreadycoded = []\n",
    "indextocount = 0\n",
    "\n",
    "for identifier in Identifiers:\n",
    "    if identifier not in alreadycoded:\n",
    "        groups[Identifiers==identifier] = indextocount\n",
    "        indextocount = indextocount + 1\n",
    "        alreadycoded.append(identifier)\n",
    "\n",
    "groupsUncut = np.full(len(IDsUncut),100000)\n",
    "alreadycodedUncut = []\n",
    "indextocountUncut = 0\n",
    "\n",
    "for identifierUncut in IdentifiersUncut:\n",
    "    if identifierUncut not in alreadycodedUncut:\n",
    "        groupsUncut[IdentifiersUncut==identifierUncut] = indextocountUncut\n",
    "        indextocountUncut = indextocountUncut + 1\n",
    "        alreadycodedUncut.append(identifierUncut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make K and B Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  *******Determine k and b by normal distribution*******\n",
    "# Note: I'm actually doing a truncated normal distribution here, but at the time I didn't know that that was a thing. This code would be more elegant if it just used the numpy truncated normal option. I'm not using it anyway, though.\n",
    "\n",
    "k_min_buffer = 0.0000001\n",
    "b_min_buffer = 0.00001\n",
    "\n",
    "k_mu = 0.057357942 - k_min_buffer #10^-6 seemed about as low as it gets, so this is one more decimal place for range\n",
    "k_sigma = 0.134691313\n",
    "b_mu = 0.012280441 - b_min_buffer #0.0002 was the lowest in the set\n",
    "b_sigma = 0.010641449\n",
    "\n",
    "# Keep these commented out if you're not using this method.\n",
    "#ks = np.absolute(np.random.normal(k_mu, k_sigma, (np.size(np.unique(groups)))))+k_min_buffer #no k will be below buffer\n",
    "#bs = np.absolute(np.random.normal(b_mu, b_sigma, (np.size(np.unique(groups)))))+b_min_buffer # /\\ for b\n",
    "\n",
    "#  *******Determine k and b by beta distribution*******\n",
    "\n",
    "k_alpha = 0.113586413\n",
    "k_beta = 1.866721959\n",
    "b_alpha = 1.30312618\n",
    "b_beta = 104.810832\n",
    "#ks = np.random.beta(k_alpha, k_beta, (np.size(np.unique(groups))))\n",
    "#bs = np.random.beta(b_alpha, b_beta, (np.size(np.unique(groups))))\n",
    "\n",
    "#  *******Determine k and b by importing previously determine k and b vals*******\n",
    "# This would let you directly assess BHM performance on the range of B and K provided by MLE. This method might acquire reasonable k/b values but is very generous toward MLE. You also have the trouble of simulating data on b/k values that weren't legitimate - sometimes MLE returns ludicrous values when it fails, and those are included here.\n",
    "\n",
    "#ks = OGKappas[np.unique(Identifiers,return_index=True)[1]]\n",
    "#bs = OGBetas[np.unique(Identifiers,return_index=True)[1]]\n",
    "\n",
    "#  *******Determine k and b by tiling*******\n",
    "# This is the main method I'm using. You could change the max and min of the tile. I'm not doing more than 121 participant sessions because the BHM improves as the number of data sets increases (at least it's supposed to). You could do a million sessions, but this would probably be unfair to MLE (and would take a billion years to run)\n",
    "min_k = 0.00000114\n",
    "max_k = 0.768891643\n",
    "min_b = 0.002337451 #min among non-exploded models\n",
    "max_b = 0.113426498\n",
    "\n",
    "n = round((np.size(np.unique(groups))**0.5)+0.49) # = 11\n",
    "\n",
    "lilks = np.arange(min_k, max_k, (max_k-min_k)/n)\n",
    "lilbs = np.arange(min_b, max_b, (max_b-min_b)/n)\n",
    "ks = []\n",
    "bs = []\n",
    "\n",
    "# This is a simple way to pair up the k and b values. I think there's actually a numpy function that does this directly.\n",
    "for i in range(n):\n",
    "    for ii in range(n):\n",
    "        ks.append(lilks[i])\n",
    "        bs.append(lilbs[ii])\n",
    "\n",
    "ks = np.array(ks) #[:116] # Use the index here if you want to restrict to 116 trials. If you don't use an edited Full SV Table (ie if you use one that only has 116 sessions), you'll need to put the index or code below will fail.\n",
    "bs = np.array(bs) #[:116]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(v**risk)/(1+kappa*d)\n",
    "#p = 1 / (1 + math.exp(beta[0]*(SV_1-SV_2)))\n",
    "\n",
    "# More details in the explanation document\n",
    "choices = np.random.binomial(1,(1/(1 + np.exp(bs[groups]*((SSAmount/(1+ks[groups]*SSDelay))-(LLAmount/(1+ks[groups]*LLDelay)))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Fake Choice Data for Bayesian Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here I'm using the \"Uncut\" arrays because I need this to match to the length in the randomized full sv table (and they need to match correctly numerically too). However the Bayesian Simpler program will re-remove the catch trials, so I'm testing on the same set anyway.\n",
    "\n",
    "choices_for_table = np.random.binomial(1,(1/(1 + np.exp(bs[groupsUncut]*((SSAmountUncut/(1+ks[groupsUncut]*SSDelayUncut))-(LLAmountUncut/(1+ks[groupsUncut]*LLDelayUncut)))))))\n",
    "pd.DataFrame(np.reshape(choices_for_table,[-1,1])).to_csv(\"Full Square\\Fake choices column generated by tiling smaller range full square.csv\", index=False, header=False)\n",
    "\n",
    "# IMPORTANT:\n",
    "# The column generated here has to be manually copy/pasted into the FAKE RANDOMIZED SV table to feed into the Bayesian code. You could make this code put it in automatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore Previously Randomized Choices (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices_from_previously_generated = genfromtxt(\"Full Square\\Fake choices column generated by tiling smaller range full square.csv\", delimiter=',', dtype=str)\n",
    "choices = choices_from_previously_generated[boolselector].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize with MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLEOutput = np.array([\"ID\", \"Day\", \"NegLL\", \"beta\", \"k\", \"realK\", \"realB\"])\n",
    "\n",
    "# Loops through sessions, runs MLE on each, returns a nice array.\n",
    "\n",
    "for pick in range(np.size(np.unique(groups))):\n",
    "    boolpick = groups==(np.unique(groups)[pick])\n",
    "    Is = IDs[boolpick]\n",
    "    Ds = Days[boolpick]\n",
    "    Dts = Dates[boolpick]\n",
    "    Ts = Times[boolpick]\n",
    "    Idents = Identifiers[boolpick]\n",
    "    SSA = SSAmount[boolpick]\n",
    "    SSD = SSDelay[boolpick]\n",
    "    LLA = LLAmount[boolpick]\n",
    "    LLD = LLDelay[boolpick]\n",
    "    Cs = choices[boolpick]  #to test against control: OGChoices\n",
    "    realK = ks[pick]\n",
    "    realB = bs[pick]\n",
    "\n",
    "    negLL, beta, k = optimizer(SSA, SSD, LLA, LLD, Cs) #, v1, d1, v2, d2, risk\n",
    "    \n",
    "    row = np.array([Is[0],Ds[0], negLL, beta, k, realK, realB])\n",
    "    MLEOutput = np.vstack((MLEOutput, row))\n",
    "\n",
    "#pd.DataFrame(MLEOutput).to_csv(\"MLEOutput on OG.csv\")\n",
    "pd.DataFrame(MLEOutput).to_csv(\"Full Square\\MLEOutput on Generated Smaller Range Full Square Restored Test Again.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare (don't need to run this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(MLEOutput[1:,4].astype(float),ks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions from MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is all mostly copied from the MLE code.\n",
    "\n",
    "#naming conventions that I've kept for consistency with the original code - \"v1\" refers to SSAmount, and \"v2\" refers to LLAmount.\n",
    "def optimizer(immediate_vals, immediate_times, delay_vals, delay_times, choices):\n",
    "    guesses = [0.005, 0.01]\n",
    "    bkbounds = ((0,8),(0.00000001,6.4))\n",
    "    risk = 1\n",
    "    v1 = immediate_vals.tolist()  \n",
    "    d1 = immediate_times.tolist() \n",
    "    v2 = delay_vals.tolist() \n",
    "    d2 = delay_times.tolist()\n",
    "\n",
    "    inputs = [choices,v1,d1,v2,d2,risk]\n",
    "\n",
    "    results = sp.optimize.minimize(optimize_me,guesses,inputs, bounds = bkbounds, tol=None, callback = None, options={'maxiter':10000, 'disp': False})\n",
    "    negLL = results.fun\n",
    "    beta = results.x[0]\n",
    "    k = results.x[1]\n",
    "    return negLL, beta, k #, v1, d1, v2, d2, risk\n",
    "\n",
    "def analysis(choices,v1,d1,v2,d2,risk,given_k,beta):\n",
    "    # Changed from other program: added LLfromGiven right here\n",
    "    given_beta = [beta,given_k]  #FYI: name conventions are wonky here. The pairing of b,k is referred to as \"beta\". We also call that stochasticity factor \"beta\". I think the latter is not quite proper, but I don't know.\n",
    "    negLL = local_negLL(given_beta,choices,v1,d1,v2,d2,risk)\n",
    "    \n",
    "    # Unrestricted log-likelihood\n",
    "    LL = -negLL\n",
    "\n",
    "    # Restricted log-likelihood\n",
    "    LL0 = np.sum((choices==1)*math.log(0.5) + (1-(choices==1))*math.log(0.5))\n",
    "\n",
    "    # Akaike Information Criterion\n",
    "    AIC = -2*LL + 2*2  #CHANGE TO len(results.x) IF USING A DIFFERENT MODEL (parameters != 2)\n",
    "\n",
    "    # Bayesian information criterion\n",
    "    BIC = -2*LL + 2*math.log(len(v1))  #len(results.x)\n",
    "\n",
    "    #R squared\n",
    "    r2 = 1 - LL/LL0\n",
    "\n",
    "    #Percent accuracy\n",
    "    k_for_accuracy = given_k\n",
    "    beta_for_accuracy = [beta,k_for_accuracy]\n",
    "    parray = np.array(choice_prob(v1,d1,v2,d2,beta_for_accuracy,risk))\n",
    "    correct =sum((parray>=0.5)==choices)/len(v1)\n",
    "\n",
    "    #print(\"k=\",given_k)\n",
    "    #print(\"LL\",LL,\"AIC\",AIC,\"BIC\",BIC,\"R2\",r2,\"correct\",correct)\n",
    "    return(LL,LL0,AIC,BIC,r2,correct)\n",
    "\n",
    "def LLfromGiven(given_k, given_b, choices, v1, d1, v2, d2, risk):   #not currently in use\n",
    "    given_beta = [given_b,given_k]\n",
    "    negLL = local_negLL(given_beta,choices,v1,d1,v2,d2,risk)\n",
    "    return negLL\n",
    "\n",
    "def local_negLL(beta,choices_list,v1,d1,v2,d2,risk):\n",
    "    \n",
    "    #print(\"beta\", beta)\n",
    "    #print(\"risk\", risk)\n",
    "\n",
    "    ps = np.array(choice_prob(v1,d1,v2,d2,beta,risk))\n",
    "    choices = np.array(choices_list)\n",
    "\n",
    "    # Trap log(0)\n",
    "    ps[ps==0] = 0.0001\n",
    "    ps[ps==1] = 0.9999\n",
    "    \n",
    "    # Log-likelihood\n",
    "\n",
    "    err = (choices==1)*np.log(ps) + ((choices==0))*np.log(1-ps)\n",
    "    # Sum of -log-likelihood\n",
    "    sumerr = -sum(err)\n",
    "    return sumerr\n",
    "\n",
    "def choice_prob(v1,d1,v2,d2,beta,risk):\n",
    "    ps = []\n",
    "\n",
    "    # NOTETHIS:THIS IS COMING IN AS A LIST (v1[n])\n",
    "    #print(len(v1)) #list has no shape\n",
    "    #print(v1)\n",
    "\n",
    "    for n in range(len(v1)):\n",
    "        #print(v1[n])\n",
    "        SV_1 = discount(v1[n],d1[n],beta[1],risk)\n",
    "        SV_2 = discount(v2[n],d2[n],beta[1],risk)\n",
    "        try: \n",
    "            p = 1 / (1 + math.exp(beta[0]*(SV_1-SV_2)))\n",
    "        except OverflowError:\n",
    "            #print(\"beta:\",beta[0],\"k:\",beta[1],\"SV_1:\",SV_1,\"SV_2\",SV_2,\"imm val\",v1[n],\"imm delay\",d1[n], \"del val\",v2[n],\"del del\",d2[n])\n",
    "            p = 0\n",
    "            #raise SystemExit(0)\n",
    "            #break\n",
    "        ps.append(p)\n",
    "        \n",
    "    return ps\n",
    "\n",
    "def discount(v,d,kappa,risk):\n",
    "    SV = (v**risk)/(1+kappa*d)\n",
    "    return SV\n",
    "\n",
    "def optimize_me(to_optimize, inputs):\n",
    "    choices_list,v1,d1,v2,d2,risk = inputs\n",
    "    return local_negLL(to_optimize,choices_list,v1,d1,v2,d2,risk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09d5eda77765311109b2c027e144dcf58f89dd96008cdf29c2e6b03e99df71a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy as sp\n",
    "from scipy import optimize\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd \n",
    "import glob\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import statistics as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pizarror/IDM\n"
     ]
    }
   ],
   "source": [
    "# getting the name of the directory\n",
    "# where the this file is present.\n",
    "# current = os.path.dirname(os.path.realpath(__file__))\n",
    "current = os.path.dirname(os.getcwd())\n",
    " \n",
    "# Getting the parent directory name\n",
    "# where the current directory is present.\n",
    "# parent = os.path.dirname(os.path.dirname(current))\n",
    "parent = current\n",
    "# print(parent)\n",
    "#/Users/pizarror/IDM\n",
    "\n",
    "# adding the parent directory to\n",
    "# the sys.path.\n",
    "sys.path.append(parent)\n",
    "\n",
    "from IDM_model.src import model_functions as mf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_load_data(subject='23_IDM_0144',fn='/tmp',alpha0=1.0,cols=[]):\n",
    "    cdd_df = pd.read_csv(fn)\n",
    "    task='cdd'\n",
    "    cdd_df = mf.drop_pract(cdd_df,task=task)\n",
    "    cdd_df,response_rate = mf.drop_non_responses(cdd_df,task=task,verbose=False) \n",
    "    data = mf.get_data(cdd_df,cols,alpha_hat=alpha0)[0]\n",
    "    data['subject'] = subject\n",
    "    return data # vn,vr,tn,tr,choice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Pooling\n",
    "\n",
    "Complete pooling ignores the group-level information and considers all data as belonging to the same category. All groups are described with the same model. \n",
    "\n",
    "We are using complete pooling to generate priors for when we implement a higherarchical bayesian model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "We will load the data from all participants so we can run the modeling schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>cdd_choice</th>\n",
       "      <th>cdd_immed_amt</th>\n",
       "      <th>cdd_delay_amt</th>\n",
       "      <th>cdd_immed_wait</th>\n",
       "      <th>cdd_delay_wait</th>\n",
       "      <th>alpha</th>\n",
       "      <th>subject_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23_IDM_0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23_IDM_0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23_IDM_0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23_IDM_0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23_IDM_0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23_IDM_0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23_IDM_0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23_IDM_0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23_IDM_0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23_IDM_0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject  cdd_choice  cdd_immed_amt  cdd_delay_amt  cdd_immed_wait   \n",
       "0  23_IDM_0001         1.0            5.0           26.0             0.0  \\\n",
       "1  23_IDM_0001         1.0            2.0           22.0             0.0   \n",
       "2  23_IDM_0001         0.0            2.0            6.0             0.0   \n",
       "3  23_IDM_0001         0.0           15.0           15.0             0.0   \n",
       "4  23_IDM_0001         1.0           15.0           55.0             0.0   \n",
       "5  23_IDM_0001         1.0            2.0           65.0             0.0   \n",
       "6  23_IDM_0001         0.0           15.0           20.0             0.0   \n",
       "7  23_IDM_0001         1.0           15.0           26.0             0.0   \n",
       "8  23_IDM_0001         0.0            5.0           10.0             0.0   \n",
       "9  23_IDM_0001         0.0            5.0            4.0             0.0   \n",
       "\n",
       "   cdd_delay_wait  alpha  subject_id  \n",
       "0            29.0    1.0         0.0  \n",
       "1            90.0    1.0         0.0  \n",
       "2            90.0    1.0         0.0  \n",
       "3           151.0    1.0         0.0  \n",
       "4            90.0    1.0         0.0  \n",
       "5            29.0    1.0         0.0  \n",
       "6            59.0    1.0         0.0  \n",
       "7             6.0    1.0         0.0  \n",
       "8           149.0    1.0         0.0  \n",
       "9            31.0    1.0         0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes about 10 seconds\n",
    "\n",
    "# we will change this when we change utility to 1st level analysis (or split)\n",
    "split_dir = '/Volumes/UCDN/datasets/IDM/split/'\n",
    "save_dir = '/Volumes/UCDN/datasets/IDM/BH/csv'\n",
    "subjs = sorted(glob.glob(os.path.join(split_dir,'23_IDM_*')))\n",
    "task = 'cdd'\n",
    "# VNa,VRa,TNa,TRa,Cha = [[]]*5\n",
    "cols = ['cdd_choice','cdd_immed_amt','cdd_delay_amt','cdd_immed_wait','cdd_delay_wait','alpha']\n",
    "data = pd.DataFrame(columns=['subject']+cols)\n",
    "\n",
    "for subj_id,s in enumerate(subjs):\n",
    "    subject = os.path.basename(s)\n",
    "    fn  = os.path.join(s,task,'{}_{}.csv'.format(os.path.basename(s),task))\n",
    "    if os.path.exists(fn):\n",
    "        subj_data = read_load_data(subject=subject,fn=fn,alpha0=1.0,cols=cols)\n",
    "        for c in cols:\n",
    "            subj_data[c] = subj_data[c].astype(float)        \n",
    "        subj_data['subject_id'] = int(subj_id)\n",
    "        data = pd.concat([data,subj_data],ignore_index=True)\n",
    "\n",
    "data.head(10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple Hierarchical Model\n",
    "\n",
    "We pooled all data together and ran simple BH model in complete_pool_as_prior.\n",
    "\n",
    "Here we can select the number of subjects we want to analyze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subjects = data['subject'].unique()\n",
    "# subjects = subjects[:10]\n",
    "# print(subjects)\n",
    "data = data.loc[data['subject'].isin(subjects)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subjects = data['subject'].unique()\n",
    "nb_subj = subjects.shape[0]\n",
    "nb_trials = data.shape[0]//nb_subj\n",
    "subj_id_list = data['subject_id'].to_list()\n",
    "subj_id = [int(s) for s in subj_id_list]\n",
    "old_id = np.array([ [s]*nb_trials for s in range(nb_subj) ]).flatten()\n",
    "\n",
    "delay_amt = data['cdd_delay_amt'].values\n",
    "delay_wait = data['cdd_delay_wait'].values\n",
    "immed_amt = data['cdd_immed_amt'].values\n",
    "immed_wait = data['cdd_immed_wait'].values\n",
    "choices = data['cdd_choice'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results from completely_pooled_model\n",
    "\n",
    "We use the kappa and gamma from previous run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kappa, use the following (mu,sigma) : (0.0199494192,0.0004967748)\n",
      "For gamma, use the following (mu,sigma) : (0.1290928433,0.0027119263)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fn = os.path.join(save_dir,'completely_pooled_model.csv')\n",
    "pool_model = pd.read_csv(fn,index_col=0)\n",
    "\n",
    "mu_kappa_hat,std_kappa_hat = pool_model.loc['kappa[0]','mean'],pool_model.loc['kappa[0]','sd']\n",
    "mu_gamma_hat,std_gamma_hat = pool_model.loc['gamma[0]','mean'],pool_model.loc['gamma[0]','sd']\n",
    "\n",
    "print('For kappa, use the following (mu,sigma) : ({},{})'.format(mu_kappa_hat, std_kappa_hat))\n",
    "print('For gamma, use the following (mu,sigma) : ({},{})'.format(mu_gamma_hat, std_gamma_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (5 chains in 5 jobs)\n",
      "NUTS: [sd_kappa_hyper, kappa, gamma]\n",
      "Process worker_chain_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pizarror/opt/anaconda3/envs/idm_jupy/lib/python3.9/site-packages/pymc/sampling/parallel.py\", line 122, in run\n",
      "    self._start_loop()\n",
      "  File \"/Users/pizarror/opt/anaconda3/envs/idm_jupy/lib/python3.9/site-packages/pymc/sampling/parallel.py\", line 161, in _start_loop\n",
      "    msg = self._recv_msg()\n",
      "  File \"/Users/pizarror/opt/anaconda3/envs/idm_jupy/lib/python3.9/site-packages/pymc/sampling/parallel.py\", line 153, in _recv_msg\n",
      "    return self._msg_pipe.recv()\n",
      "  File \"/Users/pizarror/opt/anaconda3/envs/idm_jupy/lib/python3.9/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/Users/pizarror/opt/anaconda3/envs/idm_jupy/lib/python3.9/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/pizarror/opt/anaconda3/envs/idm_jupy/lib/python3.9/multiprocessing/connection.py\", line 383, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pizarror/opt/anaconda3/envs/idm_jupy/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/pizarror/opt/anaconda3/envs/idm_jupy/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/pizarror/opt/anaconda3/envs/idm_jupy/lib/python3.9/site-packages/pymc/sampling/parallel.py\", line 194, in _run_process\n",
      "    _Process(*args).run()\n",
      "  File \"/Users/pizarror/opt/anaconda3/envs/idm_jupy/lib/python3.9/site-packages/pymc/sampling/parallel.py\", line 129, in run\n",
      "    self._msg_pipe.send((\"error\", e))\n",
      "  File \"/Users/pizarror/opt/anaconda3/envs/idm_jupy/lib/python3.9/multiprocessing/connection.py\", line 206, in send\n",
      "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
      "  File \"/Users/pizarror/opt/anaconda3/envs/idm_jupy/lib/python3.9/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/pizarror/opt/anaconda3/envs/idm_jupy/lib/python3.9/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m     prob \u001b[39m=\u001b[39m pm\u001b[39m.\u001b[39mDeterministic(\u001b[39m'\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m pm\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mgamma[subj_id] \u001b[39m*\u001b[39m ( delay_amt\u001b[39m/\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m+\u001b[39m(kappa[subj_id]\u001b[39m*\u001b[39mdelay_wait)) \n\u001b[1;32m     14\u001b[0m                                                                             \u001b[39m-\u001b[39m immed_amt\u001b[39m/\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m+\u001b[39m(kappa[subj_id]\u001b[39m*\u001b[39mimmed_wait)) ))))\n\u001b[1;32m     16\u001b[0m     y_1 \u001b[39m=\u001b[39m pm\u001b[39m.\u001b[39mBernoulli(\u001b[39m'\u001b[39m\u001b[39my_1\u001b[39m\u001b[39m'\u001b[39m,p\u001b[39m=\u001b[39mprob,observed\u001b[39m=\u001b[39mchoices)\n\u001b[0;32m---> 18\u001b[0m     trace_prior \u001b[39m=\u001b[39m pm\u001b[39m.\u001b[39;49msample(\u001b[39m10000\u001b[39;49m, tune\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m, cores\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,target_accept\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m)\n\u001b[1;32m     20\u001b[0m \u001b[39m# increase number of subplots\u001b[39;00m\n\u001b[1;32m     21\u001b[0m az\u001b[39m.\u001b[39mrcParams[\u001b[39m\"\u001b[39m\u001b[39mplot.max_subplots\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m600\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/idm_jupy/lib/python3.9/site-packages/pymc/sampling/mcmc.py:677\u001b[0m, in \u001b[0;36msample\u001b[0;34m(draws, tune, chains, cores, random_seed, progressbar, step, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, model, **kwargs)\u001b[0m\n\u001b[1;32m    675\u001b[0m _print_step_hierarchy(step)\n\u001b[1;32m    676\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 677\u001b[0m     _mp_sample(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msample_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparallel_args)\n\u001b[1;32m    678\u001b[0m \u001b[39mexcept\u001b[39;00m pickle\u001b[39m.\u001b[39mPickleError:\n\u001b[1;32m    679\u001b[0m     _log\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mCould not pickle model, sampling singlethreaded.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/idm_jupy/lib/python3.9/site-packages/pymc/sampling/mcmc.py:1052\u001b[0m, in \u001b[0;36m_mp_sample\u001b[0;34m(draws, tune, step, chains, cores, random_seed, start, progressbar, traces, model, callback, mp_ctx, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[39m# We did draws += tune in pm.sample\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m draws \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m tune\n\u001b[0;32m-> 1052\u001b[0m sampler \u001b[39m=\u001b[39m ps\u001b[39m.\u001b[39;49mParallelSampler(\n\u001b[1;32m   1053\u001b[0m     draws\u001b[39m=\u001b[39;49mdraws,\n\u001b[1;32m   1054\u001b[0m     tune\u001b[39m=\u001b[39;49mtune,\n\u001b[1;32m   1055\u001b[0m     chains\u001b[39m=\u001b[39;49mchains,\n\u001b[1;32m   1056\u001b[0m     cores\u001b[39m=\u001b[39;49mcores,\n\u001b[1;32m   1057\u001b[0m     seeds\u001b[39m=\u001b[39;49mrandom_seed,\n\u001b[1;32m   1058\u001b[0m     start_points\u001b[39m=\u001b[39;49mstart,\n\u001b[1;32m   1059\u001b[0m     step_method\u001b[39m=\u001b[39;49mstep,\n\u001b[1;32m   1060\u001b[0m     progressbar\u001b[39m=\u001b[39;49mprogressbar,\n\u001b[1;32m   1061\u001b[0m     mp_ctx\u001b[39m=\u001b[39;49mmp_ctx,\n\u001b[1;32m   1062\u001b[0m )\n\u001b[1;32m   1063\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1064\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/idm_jupy/lib/python3.9/site-packages/pymc/sampling/parallel.py:402\u001b[0m, in \u001b[0;36mParallelSampler.__init__\u001b[0;34m(self, draws, tune, chains, cores, seeds, start_points, step_method, progressbar, mp_ctx)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m mp_ctx\u001b[39m.\u001b[39mget_start_method() \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfork\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    400\u001b[0m     step_method_pickled \u001b[39m=\u001b[39m cloudpickle\u001b[39m.\u001b[39mdumps(step_method, protocol\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 402\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_samplers \u001b[39m=\u001b[39m [\n\u001b[1;32m    403\u001b[0m     ProcessAdapter(\n\u001b[1;32m    404\u001b[0m         draws,\n\u001b[1;32m    405\u001b[0m         tune,\n\u001b[1;32m    406\u001b[0m         step_method,\n\u001b[1;32m    407\u001b[0m         step_method_pickled,\n\u001b[1;32m    408\u001b[0m         chain,\n\u001b[1;32m    409\u001b[0m         seed,\n\u001b[1;32m    410\u001b[0m         start,\n\u001b[1;32m    411\u001b[0m         mp_ctx,\n\u001b[1;32m    412\u001b[0m     )\n\u001b[1;32m    413\u001b[0m     \u001b[39mfor\u001b[39;00m chain, seed, start \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mrange\u001b[39m(chains), seeds, start_points)\n\u001b[1;32m    414\u001b[0m ]\n\u001b[1;32m    416\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inactive \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_samplers\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    417\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finished: List[ProcessAdapter] \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/idm_jupy/lib/python3.9/site-packages/pymc/sampling/parallel.py:403\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m mp_ctx\u001b[39m.\u001b[39mget_start_method() \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfork\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    400\u001b[0m     step_method_pickled \u001b[39m=\u001b[39m cloudpickle\u001b[39m.\u001b[39mdumps(step_method, protocol\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    402\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_samplers \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 403\u001b[0m     ProcessAdapter(\n\u001b[1;32m    404\u001b[0m         draws,\n\u001b[1;32m    405\u001b[0m         tune,\n\u001b[1;32m    406\u001b[0m         step_method,\n\u001b[1;32m    407\u001b[0m         step_method_pickled,\n\u001b[1;32m    408\u001b[0m         chain,\n\u001b[1;32m    409\u001b[0m         seed,\n\u001b[1;32m    410\u001b[0m         start,\n\u001b[1;32m    411\u001b[0m         mp_ctx,\n\u001b[1;32m    412\u001b[0m     )\n\u001b[1;32m    413\u001b[0m     \u001b[39mfor\u001b[39;00m chain, seed, start \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mrange\u001b[39m(chains), seeds, start_points)\n\u001b[1;32m    414\u001b[0m ]\n\u001b[1;32m    416\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inactive \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_samplers\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    417\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finished: List[ProcessAdapter] \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/idm_jupy/lib/python3.9/site-packages/pymc/sampling/parallel.py:259\u001b[0m, in \u001b[0;36mProcessAdapter.__init__\u001b[0;34m(self, draws, tune, step_method, step_method_pickled, chain, seed, start, mp_ctx)\u001b[0m\n\u001b[1;32m    242\u001b[0m     step_method_send \u001b[39m=\u001b[39m step_method\n\u001b[1;32m    244\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process \u001b[39m=\u001b[39m mp_ctx\u001b[39m.\u001b[39mProcess(\n\u001b[1;32m    245\u001b[0m     daemon\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    246\u001b[0m     name\u001b[39m=\u001b[39mprocess_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     ),\n\u001b[1;32m    258\u001b[0m )\n\u001b[0;32m--> 259\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m    260\u001b[0m \u001b[39m# Close the remote pipe, so that we get notified if the other\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[39m# end is closed.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m remote_conn\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/idm_jupy/lib/python3.9/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/idm_jupy/lib/python3.9/multiprocessing/context.py:291\u001b[0m, in \u001b[0;36mForkServerProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    289\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    290\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_forkserver\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/idm_jupy/lib/python3.9/multiprocessing/popen_forkserver.py:35\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     34\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 35\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/idm_jupy/lib/python3.9/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/idm_jupy/lib/python3.9/multiprocessing/popen_forkserver.py:58\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mFinalize(\u001b[39mself\u001b[39m, util\u001b[39m.\u001b[39mclose_fds,\n\u001b[1;32m     56\u001b[0m                                (_parent_w, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentinel))\n\u001b[1;32m     57\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(w, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m, closefd\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 58\u001b[0m     f\u001b[39m.\u001b[39;49mwrite(buf\u001b[39m.\u001b[39;49mgetbuffer())\n\u001b[1;32m     59\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpid \u001b[39m=\u001b[39m forkserver\u001b[39m.\u001b[39mread_signed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentinel)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We will fit a model for each subject\n",
    "with pm.Model() as model_simple:\n",
    "\n",
    "    # Hyperparameters for k\n",
    "    # mu_kappa_hyper = pm.Beta('mu_kappa_hyper',mu=mu_kappa_hat,sigma=0.1)\n",
    "    sd_kappa_hyper = pm.Beta('sd_kappa_hyper',mu=std_kappa_hat,sigma=0.01)\n",
    "\n",
    "    # use above mean and stdev to define kappa and gamma, the posterior from the pooled is now our prior\n",
    "    # kappa = pm.Beta('kappa',mu=mu_kappa_hyper,sigma=std_kappa_hat,shape=np.size(np.unique(subj_id)))\n",
    "    kappa = pm.HalfNormal('kappa',sigma=sd_kappa_hyper,shape=np.size(np.unique(subj_id)))\n",
    "    gamma = pm.HalfNormal('gamma',sigma=0.02,shape=np.size(np.unique(subj_id)))\n",
    "    \n",
    "    prob = pm.Deterministic('prob', 1 / (1 + pm.math.exp(-gamma[subj_id] * ( delay_amt/(1+(kappa[subj_id]*delay_wait)) \n",
    "                                                                            - immed_amt/(1+(kappa[subj_id]*immed_wait)) ))))\n",
    "\n",
    "    y_1 = pm.Bernoulli('y_1',p=prob,observed=choices)\n",
    "\n",
    "    trace_prior = pm.sample(10000, tune=10000, cores=5,target_accept=0.95)\n",
    "\n",
    "# increase number of subplots\n",
    "az.rcParams[\"plot.max_subplots\"] = 600\n",
    "# Call the trace whatever you like. This just saves it. You don't want to run a whole model and then accidentally x-out your window or refresh or something and lose it all!\n",
    "az.plot_trace(trace_prior, var_names=[\"kappa\",\"gamma\"],compact=False)\n",
    "# This is how you get a nice array. Note that this returns a pandas DataFrame, not a numpy array. Indexing is totally different.\n",
    "summary= az.summary(trace_prior,round_to=10)\n",
    "fn = os.path.join(save_dir,\"simple_BH_model.csv\")\n",
    "# Again, call it what you want (yeah - call it what you want tooooo)\n",
    "print('Saving to : {}'.format(fn))\n",
    "summary.to_csv(fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working set\n",
    "The sets of parameters below seem to be generating posterior for kappa and gamma that have some variability... is it corect? Not sure, we have to plot against MLE and find out, or we can use the estimates to generate a softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# We will fit a model for each subject\n",
    "with pm.Model() as model_simple:\n",
    "\n",
    "    # Hyperparameters for k\n",
    "    mu_kappa_hyper = pm.Beta('mu_kappa_hyper',mu=mu_kappa_hat,sigma=0.1)\n",
    "    sd_kappa_hyper = pm.Beta('sd_kappa_hyper',mu=std_kappa_hat,sigma=0.01)\n",
    "\n",
    "    # use above mean and stdev to define kappa and gamma, the posterior from the pooled is now our prior\n",
    "    kappa = pm.Beta('kappa',mu=mu_kappa_hyper,sigma=sd_kappa_hyper,shape=np.size(np.unique(subj_id)))\n",
    "    # kappa = pm.HalfNormal('kappa',sigma=sd_kappa_hyper,shape=np.size(np.unique(subj_id)))\n",
    "    gamma = pm.HalfNormal('gamma',sigma=0.05,shape=np.size(np.unique(subj_id)))\n",
    "    \n",
    "    prob = pm.Deterministic('prob', 1 / (1 + pm.math.exp(-gamma[subj_id] * ( delay_amt/(1+(kappa[subj_id]*delay_wait)) \n",
    "                                                                            - immed_amt/(1+(kappa[subj_id]*immed_wait)) ))))\n",
    "\n",
    "    y_1 = pm.Bernoulli('y_1',p=prob,observed=choices)\n",
    "\n",
    "    trace_prior = pm.sample(10000, tune=10000, cores=2,target_accept=0.98)\n",
    "\n",
    "\n",
    "# increase number of subplots\n",
    "az.rcParams[\"plot.max_subplots\"] = 600\n",
    "# Call the trace whatever you like. This just saves it. You don't want to run a whole model and then accidentally x-out your window or refresh or something and lose it all!\n",
    "az.plot_trace(trace_prior, var_names=[\"kappa\",\"gamma\"],compact=False)\n",
    "# This is how you get a nice array. Note that this returns a pandas DataFrame, not a numpy array. Indexing is totally different.\n",
    "summary= az.summary(trace_prior,round_to=10)\n",
    "fn = os.path.join(save_dir,\"simple_BH_model.csv\")\n",
    "# Again, call it what you want (yeah - call it what you want tooooo)\n",
    "print('Saving to : {}'.format(fn))\n",
    "summary.to_csv(fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory\n",
    "\n",
    "Let's find out when the hyperparameters breakdown by increasing each on slowly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigma (SD) for mu_kappa_hyper worked for 0.12, broke at 0.15\n",
    "muk_sigma_prior = [0.12, 0.15, 0.17, 0.2]\n",
    "# sigma (SD) for sd_kappa_hyper worked for 0.02, broke at 0.03\n",
    "sdk_sigma_prior = [0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "# sigma (SD) for gamma HalfNormal worked for 15.0, broke at 20.0\n",
    "g_sigma_prior = [15.0, 20.0, 30.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigma (SD) for gamma HalfNormal worked for 10.0, broke at 50.0\n",
    "g_sigma_prior = [15.0, 20.0, 30.0]\n",
    "\n",
    "for igs in g_sigma_prior:\n",
    "\n",
    "    imks = 0.10 # worked for 0.12 but we will keep it just below\n",
    "    print('We are setting SD (sigma) on the prior of the mean of kappa to : {}'.format(imks))\n",
    "    isks = 0.02 # worked for 0.02 we will try to keep it here, could move down to 0.01\n",
    "    print('We are setting SD (sigma) on the prior of the SD of kappa to : {}'.format(isks))\n",
    "    print('We are setting SD (sigma) on the HalfNormal of gamma to : {}'.format(igs))\n",
    "\n",
    "    try:\n",
    "        with pm.Model() as model_simple:\n",
    "\n",
    "            # Hyperparameters for k\n",
    "            mu_kappa_hyper = pm.Beta('mu_kappa_hyper',mu=mu_kappa_hat,sigma=imks)\n",
    "            sd_kappa_hyper = pm.Beta('sd_kappa_hyper',mu=std_kappa_hat,sigma=isks)\n",
    "\n",
    "            # use above mean and stdev to define kappa and gamma, the posterior from the pooled is now our prior\n",
    "            kappa = pm.Beta('kappa',mu=mu_kappa_hyper,sigma=sd_kappa_hyper,shape=np.size(np.unique(subj_id)))\n",
    "            # kappa = pm.HalfNormal('kappa',sigma=sd_kappa_hyper,shape=np.size(np.unique(subj_id)))\n",
    "            gamma = pm.HalfNormal('gamma',sigma=igs,shape=np.size(np.unique(subj_id)))\n",
    "            \n",
    "            prob = pm.Deterministic('prob', 1 / (1 + pm.math.exp(-gamma[subj_id] * ( delay_amt/(1+(kappa[subj_id]*delay_wait)) \n",
    "                                                                                    - immed_amt/(1+(kappa[subj_id]*immed_wait)) ))))\n",
    "\n",
    "            y_1 = pm.Bernoulli('y_1',p=prob,observed=choices)\n",
    "\n",
    "            trace_prior = pm.sample(10000, tune=10000, cores=5,target_accept=0.95)\n",
    "            # trace_prior = pm.sample(100, tune=100, cores=1,target_accept=0.5)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(model_simple.debug(verbose=True))\n",
    "        print(\"SamplingError : lets continue to the next parameter\")\n",
    "        break\n",
    "    except:\n",
    "        print(\"Something else went wrong\")\n",
    "\n",
    "\n",
    "\n",
    "print('we are ready to continue!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09d5eda77765311109b2c027e144dcf58f89dd96008cdf29c2e6b03e99df71a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
